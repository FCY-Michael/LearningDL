{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData():\n",
    "    def __init__(self, data_path, nsub=3, length=5):\n",
    "        self.data = torch.load(data_path)\n",
    "        self.nsub = nsub\n",
    "        self.length = length\n",
    "\n",
    "    def getData(self,):\n",
    "        rho = torch.clone(self.data['rho'])\n",
    "        ans = torch.clone(self.data['ans'])\n",
    "        output = torch.clone(self.data['output_a'])\n",
    "        \n",
    "        return (rho, ans, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __int__(self, ):\n",
    "        super(CustomLoss,self).__init__()\n",
    "    def forward(self, a, b):\n",
    "        loss = torch.norm(a-b,p = 'fro')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Predictor,self).__init__()\n",
    "        self.input = nn.Linear(input_dim,output_dim)\n",
    "        self.hidden =  nn.ModuleList([ nn.Sequential( nn.Linear(output_dim, output_dim) ) for i in range(4) ])\n",
    "        self.pred = nn.Linear(output_dim,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x0 = self.input(x)\n",
    "        x = x0\n",
    "        x = x / torch.norm(x)\n",
    "        for layer in self.hidden:\n",
    "            x = self.tanh(layer(x) + x0)\n",
    "            x = x / torch.norm(x)\n",
    "        x = self.pred(x)\n",
    "        x = self.pred(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__() # necessary\n",
    "        self.conv = nn.Conv2d(cin, cout, (3, 3), padding=1)\n",
    "        # self.bn = nn.BatchNorm2d(cout)\n",
    "        # self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # x = self.bn(x)\n",
    "        # x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,nsub,length):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            ConvBlock(1,1),\n",
    "        )\n",
    "\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            Predictor(49,length*nsub),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.view(1,-1)\n",
    "        # print(features.shape)\n",
    "        y = self.regression(features)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epochs=10000, lr=1e-2, nsub=3, length=10):\n",
    "        self.log_dir = './runs/'+datetime.now().strftime('%b.%d %H-%M-%S')\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.data = TrainData(r'C:\\workspace\\1_Michael\\Research\\NN_solveQuantumProblem\\DataSet\\rankone_R3_10_64_1.pt').getData()\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.nsub = nsub\n",
    "        self.length = length\n",
    "        self.model = Net(nsub,length).to(self.device)\n",
    "        self.criterion = CustomLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.95, patience=8,eps = 10e-32, verbose=True)\n",
    "        self.max_epoch = epochs\n",
    "\n",
    "    def run(self):\n",
    "        training_result_dir = self.log_dir + '/training_result'\n",
    "        os.makedirs(training_result_dir)\n",
    "        metrics = {'train_loss': []}\n",
    "        for self.epoch in range(self.max_epoch): # epochs\n",
    "            train_loss = self.train() # train 1 epoch\n",
    "            print('lr:',self.get_lr(self.optimizer))\n",
    "            print(f'Epoch {self.epoch:03d}:')\n",
    "            print('train loss:', train_loss)\n",
    "            metrics['train_loss'].append(train_loss)\n",
    "\n",
    "            if torch.tensor(metrics['train_loss']).argmin() == self.epoch:\n",
    "                torch.save(self.model.state_dict(), str(training_result_dir + '/model.pth'))\n",
    "        # fig, ax = plt.subplots(1, 1, figsize=(10, 10), dpi=100)\n",
    "        # ax.set_title('Loss')\n",
    "        # ax.plot(range(self.epoch + 1), metrics['train_loss'], label='Train')\n",
    "        # ax.plot(range(self.epoch + 1), metrics['valid_loss'], label='Valid')\n",
    "        # ax.legend()\n",
    "        # plt.show()\n",
    "        # fig.savefig(str(training_result_dir / 'metrics.jpg')) \n",
    "        # plt.close()\n",
    "    def process(self,output):\n",
    "        output = output.view(self.nsub, self.length)\n",
    "        for i in range(output.shape[0]):\n",
    "            a = torch.clone(output[i]).view(1,self.length)\n",
    "            A = torch.matmul(a.t(),a)\n",
    "            if i == 0:\n",
    "                kron = A\n",
    "            else:\n",
    "                kron = torch.kron(kron,A)\n",
    "\n",
    "        return kron\n",
    "    def train(self):\n",
    "        loss_steps = []\n",
    "        rho,ans,output_a = self.data\n",
    "        rho = rho.to(self.device)\n",
    "        ans = ans.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(ans)\n",
    "        out = self.process(out)\n",
    "        loss = self.criterion(out, ans)\n",
    "        loss.backward()\n",
    "        # print(loss.item())\n",
    "        self.optimizer.step()\n",
    "        loss_steps.append(loss.detach().item())\n",
    "\n",
    "        avg_loss = sum(loss_steps) / len(loss_steps)\n",
    "        self.scheduler.step(avg_loss) #加入scheduler\n",
    "        return avg_loss\n",
    "\n",
    "    def get_lr(self,optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.01\n",
      "Epoch 000:\n",
      "train loss: 1.0000037548563787\n",
      "lr: 0.01\n",
      "Epoch 001:\n",
      "train loss: 0.9999999790909107\n",
      "lr: 0.01\n",
      "Epoch 002:\n",
      "train loss: 0.9999997085402909\n",
      "lr: 0.01\n",
      "Epoch 003:\n",
      "train loss: 0.9999909874432307\n",
      "lr: 0.01\n",
      "Epoch 004:\n",
      "train loss: 0.9996039622085993\n",
      "lr: 0.01\n",
      "Epoch 005:\n",
      "train loss: 0.995645829753673\n",
      "lr: 0.01\n",
      "Epoch 006:\n",
      "train loss: 0.9758962914404367\n",
      "lr: 0.01\n",
      "Epoch 007:\n",
      "train loss: 0.9221115212354222\n",
      "lr: 0.01\n",
      "Epoch 008:\n",
      "train loss: 0.8079153323711712\n",
      "lr: 0.01\n",
      "Epoch 009:\n",
      "train loss: 0.6308362857956118\n",
      "lr: 0.01\n",
      "Epoch 010:\n",
      "train loss: 0.8676382014757089\n",
      "lr: 0.01\n",
      "Epoch 011:\n",
      "train loss: 0.49053164987010633\n",
      "lr: 0.01\n",
      "Epoch 012:\n",
      "train loss: 0.6374578027679032\n",
      "lr: 0.01\n",
      "Epoch 013:\n",
      "train loss: 0.6082957380123215\n",
      "lr: 0.01\n",
      "Epoch 014:\n",
      "train loss: 0.43851113360936067\n",
      "lr: 0.01\n",
      "Epoch 015:\n",
      "train loss: 0.47399830860172776\n",
      "lr: 0.01\n",
      "Epoch 016:\n",
      "train loss: 0.4116137436119799\n",
      "lr: 0.01\n",
      "Epoch 017:\n",
      "train loss: 0.27790886288276667\n",
      "lr: 0.01\n",
      "Epoch 018:\n",
      "train loss: 0.324967983988407\n",
      "lr: 0.01\n",
      "Epoch 019:\n",
      "train loss: 0.22525401235630882\n",
      "lr: 0.01\n",
      "Epoch 020:\n",
      "train loss: 0.31683157300087667\n",
      "lr: 0.01\n",
      "Epoch 021:\n",
      "train loss: 0.22884855167548415\n",
      "lr: 0.01\n",
      "Epoch 022:\n",
      "train loss: 0.23814856662255632\n",
      "lr: 0.01\n",
      "Epoch 023:\n",
      "train loss: 0.2721364052349783\n",
      "lr: 0.01\n",
      "Epoch 024:\n",
      "train loss: 0.17459466757295844\n",
      "lr: 0.01\n",
      "Epoch 025:\n",
      "train loss: 0.28887931086182916\n",
      "lr: 0.01\n",
      "Epoch 026:\n",
      "train loss: 0.19338866778212185\n",
      "lr: 0.01\n",
      "Epoch 027:\n",
      "train loss: 0.21052368528402507\n",
      "lr: 0.01\n",
      "Epoch 028:\n",
      "train loss: 0.25130995063887346\n",
      "lr: 0.01\n",
      "Epoch 029:\n",
      "train loss: 0.14378000860888132\n",
      "lr: 0.01\n",
      "Epoch 030:\n",
      "train loss: 0.26582916792603495\n",
      "lr: 0.01\n",
      "Epoch 031:\n",
      "train loss: 0.21174677239314776\n",
      "lr: 0.01\n",
      "Epoch 032:\n",
      "train loss: 0.1682664755374681\n",
      "lr: 0.01\n",
      "Epoch 033:\n",
      "train loss: 0.23102939373955572\n",
      "lr: 0.01\n",
      "Epoch 034:\n",
      "train loss: 0.1462937827154214\n",
      "lr: 0.01\n",
      "Epoch 035:\n",
      "train loss: 0.19777883201042779\n",
      "lr: 0.01\n",
      "Epoch 036:\n",
      "train loss: 0.17359145036357812\n",
      "lr: 0.01\n",
      "Epoch 037:\n",
      "train loss: 0.13547974351011052\n",
      "lr: 0.01\n",
      "Epoch 038:\n",
      "train loss: 0.17431419050558608\n",
      "lr: 0.01\n",
      "Epoch 039:\n",
      "train loss: 0.0740853725112855\n",
      "lr: 0.01\n",
      "Epoch 040:\n",
      "train loss: 0.21041568473474598\n",
      "lr: 0.01\n",
      "Epoch 041:\n",
      "train loss: 0.121570068052586\n",
      "lr: 0.01\n",
      "Epoch 042:\n",
      "train loss: 0.19433876062923222\n",
      "lr: 0.01\n",
      "Epoch 043:\n",
      "train loss: 0.26118843283211995\n",
      "lr: 0.01\n",
      "Epoch 044:\n",
      "train loss: 0.19130182176953048\n",
      "lr: 0.01\n",
      "Epoch 045:\n",
      "train loss: 0.0953132777396606\n",
      "lr: 0.01\n",
      "Epoch 046:\n",
      "train loss: 0.13679506307131736\n",
      "lr: 0.01\n",
      "Epoch 047:\n",
      "train loss: 0.07153773020405993\n",
      "lr: 0.01\n",
      "Epoch 048:\n",
      "train loss: 0.06501161812713258\n",
      "lr: 0.01\n",
      "Epoch 049:\n",
      "train loss: 0.13382094463196847\n",
      "lr: 0.01\n",
      "Epoch 050:\n",
      "train loss: 0.05726508651876105\n",
      "lr: 0.01\n",
      "Epoch 051:\n",
      "train loss: 0.1700274864604711\n",
      "lr: 0.01\n",
      "Epoch 052:\n",
      "train loss: 0.18184725436266408\n",
      "lr: 0.01\n",
      "Epoch 053:\n",
      "train loss: 0.06214295824578885\n",
      "lr: 0.01\n",
      "Epoch 054:\n",
      "train loss: 0.23692231762764757\n",
      "lr: 0.01\n",
      "Epoch 055:\n",
      "train loss: 0.18492290115149632\n",
      "lr: 0.01\n",
      "Epoch 056:\n",
      "train loss: 0.11512883405449996\n",
      "lr: 0.01\n",
      "Epoch 057:\n",
      "train loss: 0.17584773450610083\n",
      "lr: 0.01\n",
      "Epoch 058:\n",
      "train loss: 0.08926795493890285\n",
      "Epoch 00060: reducing learning rate of group 0 to 9.5000e-03.\n",
      "lr: 0.0095\n",
      "Epoch 059:\n",
      "train loss: 0.20148385312916112\n",
      "lr: 0.0095\n",
      "Epoch 060:\n",
      "train loss: 0.19603420680979394\n",
      "lr: 0.0095\n",
      "Epoch 061:\n",
      "train loss: 0.05760824317026023\n",
      "lr: 0.0095\n",
      "Epoch 062:\n",
      "train loss: 0.09816363252985028\n",
      "lr: 0.0095\n",
      "Epoch 063:\n",
      "train loss: 0.04221273014178587\n",
      "lr: 0.0095\n",
      "Epoch 064:\n",
      "train loss: 0.029072187598087385\n",
      "lr: 0.0095\n",
      "Epoch 065:\n",
      "train loss: 0.1262502157835837\n",
      "lr: 0.0095\n",
      "Epoch 066:\n",
      "train loss: 0.11374963780394313\n",
      "lr: 0.0095\n",
      "Epoch 067:\n",
      "train loss: 0.09284719333638189\n",
      "lr: 0.0095\n",
      "Epoch 068:\n",
      "train loss: 0.0766902157819146\n",
      "lr: 0.0095\n",
      "Epoch 069:\n",
      "train loss: 0.11176398485693827\n",
      "lr: 0.0095\n",
      "Epoch 070:\n",
      "train loss: 0.1175829722384039\n",
      "lr: 0.0095\n",
      "Epoch 071:\n",
      "train loss: 0.06998813907486595\n",
      "lr: 0.0095\n",
      "Epoch 072:\n",
      "train loss: 0.07237083357779092\n",
      "Epoch 00074: reducing learning rate of group 0 to 9.0250e-03.\n",
      "lr: 0.009025\n",
      "Epoch 073:\n",
      "train loss: 0.09463203831431047\n",
      "lr: 0.009025\n",
      "Epoch 074:\n",
      "train loss: 0.0839072534204013\n",
      "lr: 0.009025\n",
      "Epoch 075:\n",
      "train loss: 0.0927144342413457\n",
      "lr: 0.009025\n",
      "Epoch 076:\n",
      "train loss: 0.05698670136128971\n",
      "lr: 0.009025\n",
      "Epoch 077:\n",
      "train loss: 0.12890570395347187\n",
      "lr: 0.009025\n",
      "Epoch 078:\n",
      "train loss: 0.13996629557624335\n",
      "lr: 0.009025\n",
      "Epoch 079:\n",
      "train loss: 0.05007087256596103\n",
      "lr: 0.009025\n",
      "Epoch 080:\n",
      "train loss: 0.14458860493829423\n",
      "lr: 0.009025\n",
      "Epoch 081:\n",
      "train loss: 0.08141248902668519\n",
      "Epoch 00083: reducing learning rate of group 0 to 8.5737e-03.\n",
      "lr: 0.00857375\n",
      "Epoch 082:\n",
      "train loss: 0.14628452540099945\n",
      "lr: 0.00857375\n",
      "Epoch 083:\n",
      "train loss: 0.18458199812468787\n",
      "lr: 0.00857375\n",
      "Epoch 084:\n",
      "train loss: 0.10868781553824129\n",
      "lr: 0.00857375\n",
      "Epoch 085:\n",
      "train loss: 0.12588346518269736\n",
      "lr: 0.00857375\n",
      "Epoch 086:\n",
      "train loss: 0.1475018835387844\n",
      "lr: 0.00857375\n",
      "Epoch 087:\n",
      "train loss: 0.04063879551077504\n",
      "lr: 0.00857375\n",
      "Epoch 088:\n",
      "train loss: 0.07443924546057536\n",
      "lr: 0.00857375\n",
      "Epoch 089:\n",
      "train loss: 0.042899400487454255\n",
      "lr: 0.00857375\n",
      "Epoch 090:\n",
      "train loss: 0.024863222130734577\n",
      "lr: 0.00857375\n",
      "Epoch 091:\n",
      "train loss: 0.07665992709937303\n",
      "lr: 0.00857375\n",
      "Epoch 092:\n",
      "train loss: 0.04976788411267234\n",
      "lr: 0.00857375\n",
      "Epoch 093:\n",
      "train loss: 0.0853303108169439\n",
      "lr: 0.00857375\n",
      "Epoch 094:\n",
      "train loss: 0.026868568055834888\n",
      "lr: 0.00857375\n",
      "Epoch 095:\n",
      "train loss: 0.04796710735288441\n",
      "lr: 0.00857375\n",
      "Epoch 096:\n",
      "train loss: 0.06802552809094202\n",
      "lr: 0.00857375\n",
      "Epoch 097:\n",
      "train loss: 0.03083166608256675\n",
      "lr: 0.00857375\n",
      "Epoch 098:\n",
      "train loss: 0.037413764159865484\n",
      "Epoch 00100: reducing learning rate of group 0 to 8.1451e-03.\n",
      "lr: 0.0081450625\n",
      "Epoch 099:\n",
      "train loss: 0.08128582310784961\n",
      "lr: 0.0081450625\n",
      "Epoch 100:\n",
      "train loss: 0.041843662225404615\n",
      "lr: 0.0081450625\n",
      "Epoch 101:\n",
      "train loss: 0.07171556076747365\n",
      "lr: 0.0081450625\n",
      "Epoch 102:\n",
      "train loss: 0.03351948435044674\n",
      "lr: 0.0081450625\n",
      "Epoch 103:\n",
      "train loss: 0.0654334577710016\n",
      "lr: 0.0081450625\n",
      "Epoch 104:\n",
      "train loss: 0.057335910734184925\n",
      "lr: 0.0081450625\n",
      "Epoch 105:\n",
      "train loss: 0.04249187651358786\n",
      "lr: 0.0081450625\n",
      "Epoch 106:\n",
      "train loss: 0.08890006063814776\n",
      "lr: 0.0081450625\n",
      "Epoch 107:\n",
      "train loss: 0.04698795424944697\n",
      "Epoch 00109: reducing learning rate of group 0 to 7.7378e-03.\n",
      "lr: 0.007737809374999999\n",
      "Epoch 108:\n",
      "train loss: 0.1075833579339098\n",
      "lr: 0.007737809374999999\n",
      "Epoch 109:\n",
      "train loss: 0.1012519579973772\n",
      "lr: 0.007737809374999999\n",
      "Epoch 110:\n",
      "train loss: 0.06239202899401525\n",
      "lr: 0.007737809374999999\n",
      "Epoch 111:\n",
      "train loss: 0.06416445506256595\n",
      "lr: 0.007737809374999999\n",
      "Epoch 112:\n",
      "train loss: 0.07842070815945898\n",
      "lr: 0.007737809374999999\n",
      "Epoch 113:\n",
      "train loss: 0.08232802272298174\n",
      "lr: 0.007737809374999999\n",
      "Epoch 114:\n",
      "train loss: 0.04813692651750467\n",
      "lr: 0.007737809374999999\n",
      "Epoch 115:\n",
      "train loss: 0.03719689303333457\n",
      "lr: 0.007737809374999999\n",
      "Epoch 116:\n",
      "train loss: 0.06471986870529714\n",
      "Epoch 00118: reducing learning rate of group 0 to 7.3509e-03.\n",
      "lr: 0.007350918906249998\n",
      "Epoch 117:\n",
      "train loss: 0.0296427447265469\n",
      "lr: 0.007350918906249998\n",
      "Epoch 118:\n",
      "train loss: 0.06215176592534771\n",
      "lr: 0.007350918906249998\n",
      "Epoch 119:\n",
      "train loss: 0.03842830221713124\n",
      "lr: 0.007350918906249998\n",
      "Epoch 120:\n",
      "train loss: 0.01901906999486938\n",
      "lr: 0.007350918906249998\n",
      "Epoch 121:\n",
      "train loss: 0.06344587304739309\n",
      "lr: 0.007350918906249998\n",
      "Epoch 122:\n",
      "train loss: 0.04893346357925326\n",
      "lr: 0.007350918906249998\n",
      "Epoch 123:\n",
      "train loss: 0.03960157797830894\n",
      "lr: 0.007350918906249998\n",
      "Epoch 124:\n",
      "train loss: 0.07631679576572331\n",
      "lr: 0.007350918906249998\n",
      "Epoch 125:\n",
      "train loss: 0.050981108680877275\n",
      "lr: 0.007350918906249998\n",
      "Epoch 126:\n",
      "train loss: 0.08617453724592025\n",
      "lr: 0.007350918906249998\n",
      "Epoch 127:\n",
      "train loss: 0.06754619219226456\n",
      "lr: 0.007350918906249998\n",
      "Epoch 128:\n",
      "train loss: 0.0856764017039018\n",
      "Epoch 00130: reducing learning rate of group 0 to 6.9834e-03.\n",
      "lr: 0.006983372960937498\n",
      "Epoch 129:\n",
      "train loss: 0.07136237891380684\n",
      "lr: 0.006983372960937498\n",
      "Epoch 130:\n",
      "train loss: 0.0816713013561243\n",
      "lr: 0.006983372960937498\n",
      "Epoch 131:\n",
      "train loss: 0.09046525558740585\n",
      "lr: 0.006983372960937498\n",
      "Epoch 132:\n",
      "train loss: 0.03397202050894672\n",
      "lr: 0.006983372960937498\n",
      "Epoch 133:\n",
      "train loss: 0.051197969058576515\n",
      "lr: 0.006983372960937498\n",
      "Epoch 134:\n",
      "train loss: 0.06125530758181937\n",
      "lr: 0.006983372960937498\n",
      "Epoch 135:\n",
      "train loss: 0.048810998973725137\n",
      "lr: 0.006983372960937498\n",
      "Epoch 136:\n",
      "train loss: 0.07448585995007242\n",
      "lr: 0.006983372960937498\n",
      "Epoch 137:\n",
      "train loss: 0.04559266702916681\n",
      "Epoch 00139: reducing learning rate of group 0 to 6.6342e-03.\n",
      "lr: 0.006634204312890623\n",
      "Epoch 138:\n",
      "train loss: 0.09723762225053802\n",
      "lr: 0.006634204312890623\n",
      "Epoch 139:\n",
      "train loss: 0.10330965957207819\n",
      "lr: 0.006634204312890623\n",
      "Epoch 140:\n",
      "train loss: 0.03564433877660078\n",
      "lr: 0.006634204312890623\n",
      "Epoch 141:\n",
      "train loss: 0.09887325706561106\n",
      "lr: 0.006634204312890623\n",
      "Epoch 142:\n",
      "train loss: 0.06455079365195673\n",
      "lr: 0.006634204312890623\n",
      "Epoch 143:\n",
      "train loss: 0.0968656264971261\n",
      "lr: 0.006634204312890623\n",
      "Epoch 144:\n",
      "train loss: 0.11189780517702898\n",
      "lr: 0.006634204312890623\n",
      "Epoch 145:\n",
      "train loss: 0.0314035981115101\n",
      "lr: 0.006634204312890623\n",
      "Epoch 146:\n",
      "train loss: 0.1703769236772154\n",
      "Epoch 00148: reducing learning rate of group 0 to 6.3025e-03.\n",
      "lr: 0.006302494097246091\n",
      "Epoch 147:\n",
      "train loss: 0.20290852377074986\n",
      "lr: 0.006302494097246091\n",
      "Epoch 148:\n",
      "train loss: 0.07419106812223289\n",
      "lr: 0.006302494097246091\n",
      "Epoch 149:\n",
      "train loss: 0.13870083571418973\n",
      "lr: 0.006302494097246091\n",
      "Epoch 150:\n",
      "train loss: 0.21556146790230762\n",
      "lr: 0.006302494097246091\n",
      "Epoch 151:\n",
      "train loss: 0.209866480412636\n",
      "lr: 0.006302494097246091\n",
      "Epoch 152:\n",
      "train loss: 0.1287217782439071\n",
      "lr: 0.006302494097246091\n",
      "Epoch 153:\n",
      "train loss: 0.061321098891292725\n",
      "lr: 0.006302494097246091\n",
      "Epoch 154:\n",
      "train loss: 0.12043228718722082\n",
      "lr: 0.006302494097246091\n",
      "Epoch 155:\n",
      "train loss: 0.04758031877026122\n",
      "Epoch 00157: reducing learning rate of group 0 to 5.9874e-03.\n",
      "lr: 0.005987369392383786\n",
      "Epoch 156:\n",
      "train loss: 0.1227214488943189\n",
      "lr: 0.005987369392383786\n",
      "Epoch 157:\n",
      "train loss: 0.169514445561678\n",
      "lr: 0.005987369392383786\n",
      "Epoch 158:\n",
      "train loss: 0.13309625819587959\n",
      "lr: 0.005987369392383786\n",
      "Epoch 159:\n",
      "train loss: 0.02425504120341743\n",
      "lr: 0.005987369392383786\n",
      "Epoch 160:\n",
      "train loss: 0.15798416883899788\n",
      "lr: 0.005987369392383786\n",
      "Epoch 161:\n",
      "train loss: 0.18241560751243321\n",
      "lr: 0.005987369392383786\n",
      "Epoch 162:\n",
      "train loss: 0.08041436318988823\n",
      "lr: 0.005987369392383786\n",
      "Epoch 163:\n",
      "train loss: 0.12201196784695957\n",
      "lr: 0.005987369392383786\n",
      "Epoch 164:\n",
      "train loss: 0.17971609074334877\n",
      "Epoch 00166: reducing learning rate of group 0 to 5.6880e-03.\n",
      "lr: 0.005688000922764597\n",
      "Epoch 165:\n",
      "train loss: 0.16014856174457098\n",
      "lr: 0.005688000922764597\n",
      "Epoch 166:\n",
      "train loss: 0.06848614577677054\n",
      "lr: 0.005688000922764597\n",
      "Epoch 167:\n",
      "train loss: 0.12639888110354508\n",
      "lr: 0.005688000922764597\n",
      "Epoch 168:\n",
      "train loss: 0.17972193855853635\n",
      "lr: 0.005688000922764597\n",
      "Epoch 169:\n",
      "train loss: 0.09936677741794178\n",
      "lr: 0.005688000922764597\n",
      "Epoch 170:\n",
      "train loss: 0.0783760666559172\n",
      "lr: 0.005688000922764597\n",
      "Epoch 171:\n",
      "train loss: 0.13555853348748986\n",
      "lr: 0.005688000922764597\n",
      "Epoch 172:\n",
      "train loss: 0.11138392109872895\n",
      "lr: 0.005688000922764597\n",
      "Epoch 173:\n",
      "train loss: 0.02049561080322695\n",
      "Epoch 00175: reducing learning rate of group 0 to 5.4036e-03.\n",
      "lr: 0.005403600876626367\n",
      "Epoch 174:\n",
      "train loss: 0.11859688943393967\n",
      "lr: 0.005403600876626367\n",
      "Epoch 175:\n",
      "train loss: 0.11785809203334326\n",
      "lr: 0.005403600876626367\n",
      "Epoch 176:\n",
      "train loss: 0.04845613188557293\n",
      "lr: 0.005403600876626367\n",
      "Epoch 177:\n",
      "train loss: 0.09567152235257749\n",
      "lr: 0.005403600876626367\n",
      "Epoch 178:\n",
      "train loss: 0.09788864822107889\n",
      "lr: 0.005403600876626367\n",
      "Epoch 179:\n",
      "train loss: 0.0389783761279542\n",
      "lr: 0.005403600876626367\n",
      "Epoch 180:\n",
      "train loss: 0.10764618565290557\n",
      "lr: 0.005403600876626367\n",
      "Epoch 181:\n",
      "train loss: 0.11355965320886355\n",
      "lr: 0.005403600876626367\n",
      "Epoch 182:\n",
      "train loss: 0.02271896789993757\n",
      "Epoch 00184: reducing learning rate of group 0 to 5.1334e-03.\n",
      "lr: 0.005133420832795048\n",
      "Epoch 183:\n",
      "train loss: 0.11484150880231135\n",
      "lr: 0.005133420832795048\n",
      "Epoch 184:\n",
      "train loss: 0.14860857068225397\n",
      "lr: 0.005133420832795048\n",
      "Epoch 185:\n",
      "train loss: 0.11598022847651444\n",
      "lr: 0.005133420832795048\n",
      "Epoch 186:\n",
      "train loss: 0.05160701205581857\n",
      "lr: 0.005133420832795048\n",
      "Epoch 187:\n",
      "train loss: 0.10128937649502877\n",
      "lr: 0.005133420832795048\n",
      "Epoch 188:\n",
      "train loss: 0.09243521612267468\n",
      "lr: 0.005133420832795048\n",
      "Epoch 189:\n",
      "train loss: 0.055465851539476405\n",
      "lr: 0.005133420832795048\n",
      "Epoch 190:\n",
      "train loss: 0.08869108881605978\n",
      "lr: 0.005133420832795048\n",
      "Epoch 191:\n",
      "train loss: 0.06736268469462552\n",
      "Epoch 00193: reducing learning rate of group 0 to 4.8767e-03.\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 192:\n",
      "train loss: 0.03871110361265115\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 193:\n",
      "train loss: 0.05267484063793207\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 194:\n",
      "train loss: 0.026707580129358477\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 195:\n",
      "train loss: 0.026663579976338964\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 196:\n",
      "train loss: 0.044546364706278666\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 197:\n",
      "train loss: 0.024643555977878807\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 198:\n",
      "train loss: 0.05725142097640764\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 199:\n",
      "train loss: 0.04730655112115162\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 200:\n",
      "train loss: 0.04873291125924442\n",
      "Epoch 00202: reducing learning rate of group 0 to 4.6329e-03.\n",
      "lr: 0.00463291230159753\n",
      "Epoch 201:\n",
      "train loss: 0.04407193153070945\n",
      "lr: 0.00463291230159753\n",
      "Epoch 202:\n",
      "train loss: 0.04392730942319475\n",
      "lr: 0.00463291230159753\n",
      "Epoch 203:\n",
      "train loss: 0.04428919141528992\n",
      "lr: 0.00463291230159753\n",
      "Epoch 204:\n",
      "train loss: 0.034881173422989305\n",
      "lr: 0.00463291230159753\n",
      "Epoch 205:\n",
      "train loss: 0.024564325797145743\n",
      "lr: 0.00463291230159753\n",
      "Epoch 206:\n",
      "train loss: 0.058311991727718865\n",
      "lr: 0.00463291230159753\n",
      "Epoch 207:\n",
      "train loss: 0.059116244334774075\n",
      "lr: 0.00463291230159753\n",
      "Epoch 208:\n",
      "train loss: 0.02569194019266178\n",
      "lr: 0.00463291230159753\n",
      "Epoch 209:\n",
      "train loss: 0.03332442498131706\n",
      "Epoch 00211: reducing learning rate of group 0 to 4.4013e-03.\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 210:\n",
      "train loss: 0.03736602158821808\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 211:\n",
      "train loss: 0.03329485415184663\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 212:\n",
      "train loss: 0.03755335617485511\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 213:\n",
      "train loss: 0.018918564968835656\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 214:\n",
      "train loss: 0.06391990543975438\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 215:\n",
      "train loss: 0.06658907651918515\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 216:\n",
      "train loss: 0.015973454481998623\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 217:\n",
      "train loss: 0.06291885017606386\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 218:\n",
      "train loss: 0.049725190299803566\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 219:\n",
      "train loss: 0.052361821308531474\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 220:\n",
      "train loss: 0.05686492525791689\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 221:\n",
      "train loss: 0.013549239467581395\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 222:\n",
      "train loss: 0.04892417336668049\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 223:\n",
      "train loss: 0.027236348834441003\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 224:\n",
      "train loss: 0.0513453788861321\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 225:\n",
      "train loss: 0.03823496193113853\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 226:\n",
      "train loss: 0.050812421381363694\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 227:\n",
      "train loss: 0.05116115007882211\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 228:\n",
      "train loss: 0.026461008110774956\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 229:\n",
      "train loss: 0.023792874705243428\n",
      "Epoch 00231: reducing learning rate of group 0 to 4.1812e-03.\n",
      "lr: 0.004181203352191771\n",
      "Epoch 230:\n",
      "train loss: 0.04976596343820666\n",
      "lr: 0.004181203352191771\n",
      "Epoch 231:\n",
      "train loss: 0.035109213476389\n",
      "lr: 0.004181203352191771\n",
      "Epoch 232:\n",
      "train loss: 0.048871233093532127\n",
      "lr: 0.004181203352191771\n",
      "Epoch 233:\n",
      "train loss: 0.05483720334408731\n",
      "lr: 0.004181203352191771\n",
      "Epoch 234:\n",
      "train loss: 0.012046773111620574\n",
      "lr: 0.004181203352191771\n",
      "Epoch 235:\n",
      "train loss: 0.029784610117622345\n",
      "lr: 0.004181203352191771\n",
      "Epoch 236:\n",
      "train loss: 0.026618450216735593\n",
      "lr: 0.004181203352191771\n",
      "Epoch 237:\n",
      "train loss: 0.016091995702882448\n",
      "lr: 0.004181203352191771\n",
      "Epoch 238:\n",
      "train loss: 0.048602190139735636\n",
      "lr: 0.004181203352191771\n",
      "Epoch 239:\n",
      "train loss: 0.03472694969924227\n",
      "lr: 0.004181203352191771\n",
      "Epoch 240:\n",
      "train loss: 0.0508808300127118\n",
      "lr: 0.004181203352191771\n",
      "Epoch 241:\n",
      "train loss: 0.0517216625121066\n",
      "lr: 0.004181203352191771\n",
      "Epoch 242:\n",
      "train loss: 0.013493699680224019\n",
      "Epoch 00244: reducing learning rate of group 0 to 3.9721e-03.\n",
      "lr: 0.003972143184582182\n",
      "Epoch 243:\n",
      "train loss: 0.012555932261471428\n",
      "lr: 0.003972143184582182\n",
      "Epoch 244:\n",
      "train loss: 0.043915517187367824\n",
      "lr: 0.003972143184582182\n",
      "Epoch 245:\n",
      "train loss: 0.02840808770620526\n",
      "lr: 0.003972143184582182\n",
      "Epoch 246:\n",
      "train loss: 0.0521192574258473\n",
      "lr: 0.003972143184582182\n",
      "Epoch 247:\n",
      "train loss: 0.050263379563570756\n",
      "lr: 0.003972143184582182\n",
      "Epoch 248:\n",
      "train loss: 0.022705018972457473\n",
      "lr: 0.003972143184582182\n",
      "Epoch 249:\n",
      "train loss: 0.023288293708024918\n",
      "lr: 0.003972143184582182\n",
      "Epoch 250:\n",
      "train loss: 0.041141510930025715\n",
      "lr: 0.003972143184582182\n",
      "Epoch 251:\n",
      "train loss: 0.028146484548036848\n",
      "Epoch 00253: reducing learning rate of group 0 to 3.7735e-03.\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 252:\n",
      "train loss: 0.05048003234166221\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 253:\n",
      "train loss: 0.05743820323706534\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 254:\n",
      "train loss: 0.009731384683117926\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 255:\n",
      "train loss: 0.07863799006645646\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 256:\n",
      "train loss: 0.0860962620098365\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 257:\n",
      "train loss: 0.03331238697795738\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 258:\n",
      "train loss: 0.0722056806326212\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 259:\n",
      "train loss: 0.0930486176065158\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 260:\n",
      "train loss: 0.061948917126882094\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 261:\n",
      "train loss: 0.03979272805795713\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 262:\n",
      "train loss: 0.06425352194257963\n",
      "Epoch 00264: reducing learning rate of group 0 to 3.5849e-03.\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 263:\n",
      "train loss: 0.02513077842185718\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 264:\n",
      "train loss: 0.06995151832296924\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 265:\n",
      "train loss: 0.09395534248325534\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 266:\n",
      "train loss: 0.0669486940007031\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 267:\n",
      "train loss: 0.025694764331527904\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 268:\n",
      "train loss: 0.05169821781836278\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 269:\n",
      "train loss: 0.025415812996949765\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 270:\n",
      "train loss: 0.056449359819845266\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 271:\n",
      "train loss: 0.06651812130550075\n",
      "Epoch 00273: reducing learning rate of group 0 to 3.4056e-03.\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 272:\n",
      "train loss: 0.025444408298328308\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 273:\n",
      "train loss: 0.07155260027193394\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 274:\n",
      "train loss: 0.09111581912254682\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 275:\n",
      "train loss: 0.045859668790547314\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 276:\n",
      "train loss: 0.05465105074550609\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 277:\n",
      "train loss: 0.08480806291992206\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 278:\n",
      "train loss: 0.06466934905899317\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 279:\n",
      "train loss: 0.010020169716906375\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 280:\n",
      "train loss: 0.038551856623527274\n",
      "Epoch 00282: reducing learning rate of group 0 to 3.2353e-03.\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 281:\n",
      "train loss: 0.01665879622633339\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 282:\n",
      "train loss: 0.04470615068649726\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 283:\n",
      "train loss: 0.040529538141962765\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 284:\n",
      "train loss: 0.023036059562251725\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 285:\n",
      "train loss: 0.028079950742653424\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 286:\n",
      "train loss: 0.020174360717014703\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 287:\n",
      "train loss: 0.017424430170749395\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 288:\n",
      "train loss: 0.03034796297349003\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 289:\n",
      "train loss: 0.01889200798885781\n",
      "Epoch 00291: reducing learning rate of group 0 to 3.0736e-03.\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 290:\n",
      "train loss: 0.03976465759671132\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 291:\n",
      "train loss: 0.03681625557780876\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 292:\n",
      "train loss: 0.018518890245211233\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 293:\n",
      "train loss: 0.020142401878366474\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 294:\n",
      "train loss: 0.028437475199853606\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 295:\n",
      "train loss: 0.023909742817587006\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 296:\n",
      "train loss: 0.02843002423786951\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 297:\n",
      "train loss: 0.023117984784431743\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 298:\n",
      "train loss: 0.03285150185980016\n",
      "Epoch 00300: reducing learning rate of group 0 to 2.9199e-03.\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 299:\n",
      "train loss: 0.03204035356346234\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 300:\n",
      "train loss: 0.019254560364161708\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 301:\n",
      "train loss: 0.015487675539007376\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 302:\n",
      "train loss: 0.03483022910903464\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 303:\n",
      "train loss: 0.034786844610904304\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 304:\n",
      "train loss: 0.01454837264025285\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 305:\n",
      "train loss: 0.01465840749890538\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 306:\n",
      "train loss: 0.030237197327293457\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 307:\n",
      "train loss: 0.02244403382662119\n",
      "Epoch 00309: reducing learning rate of group 0 to 2.7739e-03.\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 308:\n",
      "train loss: 0.031088235318838454\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 309:\n",
      "train loss: 0.029997924725385504\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 310:\n",
      "train loss: 0.019378399770414046\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 311:\n",
      "train loss: 0.018314532966396062\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 312:\n",
      "train loss: 0.027683086704421843\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 313:\n",
      "train loss: 0.022001842220480967\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 314:\n",
      "train loss: 0.02858374407003605\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 315:\n",
      "train loss: 0.029768244414067592\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 316:\n",
      "train loss: 0.014974012634699049\n",
      "Epoch 00318: reducing learning rate of group 0 to 2.6352e-03.\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 317:\n",
      "train loss: 0.011137684256767732\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 318:\n",
      "train loss: 0.03492373492591939\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 319:\n",
      "train loss: 0.03223673323565026\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 320:\n",
      "train loss: 0.012450177314203231\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 321:\n",
      "train loss: 0.010464175269991906\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 322:\n",
      "train loss: 0.03297712867274221\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 323:\n",
      "train loss: 0.030394176990559945\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 324:\n",
      "train loss: 0.014248792688649746\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 325:\n",
      "train loss: 0.011538706657359944\n",
      "Epoch 00327: reducing learning rate of group 0 to 2.5034e-03.\n",
      "lr: 0.002503440897424549\n",
      "Epoch 326:\n",
      "train loss: 0.03294128980983141\n",
      "lr: 0.002503440897424549\n",
      "Epoch 327:\n",
      "train loss: 0.03108308749005094\n",
      "lr: 0.002503440897424549\n",
      "Epoch 328:\n",
      "train loss: 0.010156074830974275\n",
      "lr: 0.002503440897424549\n",
      "Epoch 329:\n",
      "train loss: 0.006918100009388267\n",
      "lr: 0.002503440897424549\n",
      "Epoch 330:\n",
      "train loss: 0.0354773293503926\n",
      "lr: 0.002503440897424549\n",
      "Epoch 331:\n",
      "train loss: 0.03461940666962416\n",
      "lr: 0.002503440897424549\n",
      "Epoch 332:\n",
      "train loss: 0.007439537717159324\n",
      "lr: 0.002503440897424549\n",
      "Epoch 333:\n",
      "train loss: 0.013753497299968179\n",
      "lr: 0.002503440897424549\n",
      "Epoch 334:\n",
      "train loss: 0.019239282032962785\n",
      "lr: 0.002503440897424549\n",
      "Epoch 335:\n",
      "train loss: 0.007958633631346327\n",
      "lr: 0.002503440897424549\n",
      "Epoch 336:\n",
      "train loss: 0.0394432585973467\n",
      "lr: 0.002503440897424549\n",
      "Epoch 337:\n",
      "train loss: 0.03883706645421334\n",
      "Epoch 00339: reducing learning rate of group 0 to 2.3783e-03.\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 338:\n",
      "train loss: 0.010547380744634464\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 339:\n",
      "train loss: 0.02378983745894236\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 340:\n",
      "train loss: 0.013324618509156921\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 341:\n",
      "train loss: 0.023215431455787562\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 342:\n",
      "train loss: 0.011985348102264202\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 343:\n",
      "train loss: 0.027839571052345836\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 344:\n",
      "train loss: 0.021229441349384693\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 345:\n",
      "train loss: 0.024320187353681126\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 346:\n",
      "train loss: 0.024548231815495748\n",
      "Epoch 00348: reducing learning rate of group 0 to 2.2594e-03.\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 347:\n",
      "train loss: 0.015550233118743578\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 348:\n",
      "train loss: 0.014191362435084186\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 349:\n",
      "train loss: 0.022089604600286868\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 350:\n",
      "train loss: 0.017584042893248872\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 351:\n",
      "train loss: 0.023474128900420287\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 352:\n",
      "train loss: 0.02277154857576443\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 353:\n",
      "train loss: 0.014668867300404953\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 354:\n",
      "train loss: 0.012143027176431766\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 355:\n",
      "train loss: 0.025886361478179347\n",
      "Epoch 00357: reducing learning rate of group 0 to 2.1464e-03.\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 356:\n",
      "train loss: 0.02480269231792834\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 357:\n",
      "train loss: 0.01342456553748864\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 358:\n",
      "train loss: 0.0112409668745584\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 359:\n",
      "train loss: 0.02495518280525836\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 360:\n",
      "train loss: 0.023494015941530274\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 361:\n",
      "train loss: 0.011878404860586037\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 362:\n",
      "train loss: 0.009088378549995557\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 363:\n",
      "train loss: 0.026538798067590807\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 364:\n",
      "train loss: 0.02419664285852407\n",
      "Epoch 00366: reducing learning rate of group 0 to 2.0391e-03.\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 365:\n",
      "train loss: 0.012423272922705094\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 366:\n",
      "train loss: 0.010701617801749335\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 367:\n",
      "train loss: 0.02347734112973548\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 368:\n",
      "train loss: 0.021863576183196707\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 369:\n",
      "train loss: 0.012145684636576858\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 370:\n",
      "train loss: 0.009645839691373424\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 371:\n",
      "train loss: 0.025123060499976337\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 372:\n",
      "train loss: 0.02426688884194725\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 373:\n",
      "train loss: 0.00952473336256873\n",
      "Epoch 00375: reducing learning rate of group 0 to 1.9371e-03.\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 374:\n",
      "train loss: 0.007955214575381103\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 375:\n",
      "train loss: 0.02556017581130034\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 376:\n",
      "train loss: 0.0238178362951423\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 377:\n",
      "train loss: 0.009020687835275085\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 378:\n",
      "train loss: 0.008120594641152683\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 379:\n",
      "train loss: 0.023339325742188465\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 380:\n",
      "train loss: 0.021469978532997605\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 381:\n",
      "train loss: 0.011951247717939343\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 382:\n",
      "train loss: 0.010778742601382571\n",
      "Epoch 00384: reducing learning rate of group 0 to 1.8403e-03.\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 383:\n",
      "train loss: 0.021032649913462712\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 384:\n",
      "train loss: 0.01963432695166108\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 385:\n",
      "train loss: 0.012055586045775982\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 386:\n",
      "train loss: 0.01081637568570704\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 387:\n",
      "train loss: 0.019168358013797432\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 388:\n",
      "train loss: 0.017698098348238933\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 389:\n",
      "train loss: 0.01415235121106575\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 390:\n",
      "train loss: 0.012464503470313781\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 391:\n",
      "train loss: 0.017904841568270324\n",
      "Epoch 00393: reducing learning rate of group 0 to 1.7482e-03.\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 392:\n",
      "train loss: 0.01658848803549548\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 393:\n",
      "train loss: 0.01495707952304069\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 394:\n",
      "train loss: 0.013057076859091063\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 395:\n",
      "train loss: 0.016133742590669343\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 396:\n",
      "train loss: 0.01485912561818359\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 397:\n",
      "train loss: 0.014940545740525061\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 398:\n",
      "train loss: 0.013019474881324442\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 399:\n",
      "train loss: 0.016339144315791698\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 400:\n",
      "train loss: 0.015048229284952388\n",
      "Epoch 00402: reducing learning rate of group 0 to 1.6608e-03.\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 401:\n",
      "train loss: 0.014569553274451585\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 402:\n",
      "train loss: 0.012692140489920293\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 403:\n",
      "train loss: 0.015315596795453424\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 404:\n",
      "train loss: 0.014100519560843806\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 405:\n",
      "train loss: 0.013847584344469888\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 406:\n",
      "train loss: 0.011996204602748535\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 407:\n",
      "train loss: 0.016132413363224744\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 408:\n",
      "train loss: 0.015054588259275588\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 409:\n",
      "train loss: 0.01263560979312101\n",
      "Epoch 00411: reducing learning rate of group 0 to 1.5778e-03.\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 410:\n",
      "train loss: 0.010724904026189372\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 411:\n",
      "train loss: 0.01737463601141614\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 412:\n",
      "train loss: 0.016447468486140936\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 413:\n",
      "train loss: 0.00977930061982543\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 414:\n",
      "train loss: 0.008232389935158967\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 415:\n",
      "train loss: 0.017978892527615874\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 416:\n",
      "train loss: 0.016849559212824685\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 417:\n",
      "train loss: 0.009738170554463384\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 418:\n",
      "train loss: 0.008552875344462035\n",
      "Epoch 00420: reducing learning rate of group 0 to 1.4989e-03.\n",
      "lr: 0.001498902540488154\n",
      "Epoch 419:\n",
      "train loss: 0.01727246186827429\n",
      "lr: 0.001498902540488154\n",
      "Epoch 420:\n",
      "train loss: 0.015964532135264227\n",
      "lr: 0.001498902540488154\n",
      "Epoch 421:\n",
      "train loss: 0.009701625769888892\n",
      "lr: 0.001498902540488154\n",
      "Epoch 422:\n",
      "train loss: 0.008874299514179118\n",
      "lr: 0.001498902540488154\n",
      "Epoch 423:\n",
      "train loss: 0.01527675778417055\n",
      "lr: 0.001498902540488154\n",
      "Epoch 424:\n",
      "train loss: 0.013820753857152602\n",
      "lr: 0.001498902540488154\n",
      "Epoch 425:\n",
      "train loss: 0.011936807162331156\n",
      "lr: 0.001498902540488154\n",
      "Epoch 426:\n",
      "train loss: 0.01059061020723967\n",
      "lr: 0.001498902540488154\n",
      "Epoch 427:\n",
      "train loss: 0.014167186150209786\n",
      "Epoch 00429: reducing learning rate of group 0 to 1.4240e-03.\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 428:\n",
      "train loss: 0.013104189284640603\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 429:\n",
      "train loss: 0.012165856604801241\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 430:\n",
      "train loss: 0.01052898365302295\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 431:\n",
      "train loss: 0.0133379610429906\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 432:\n",
      "train loss: 0.012493583780072317\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 433:\n",
      "train loss: 0.011319878210510075\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 434:\n",
      "train loss: 0.009693745536279554\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 435:\n",
      "train loss: 0.014134960395615388\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 436:\n",
      "train loss: 0.01324118262300145\n",
      "Epoch 00438: reducing learning rate of group 0 to 1.3528e-03.\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 437:\n",
      "train loss: 0.01058280779509294\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 438:\n",
      "train loss: 0.009138905770820965\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 439:\n",
      "train loss: 0.01331594659052331\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 440:\n",
      "train loss: 0.012305819163595718\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 441:\n",
      "train loss: 0.0104313803144429\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 442:\n",
      "train loss: 0.009113137758967415\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 443:\n",
      "train loss: 0.013326406367458151\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 444:\n",
      "train loss: 0.012313002838456336\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 445:\n",
      "train loss: 0.010365181703551687\n",
      "Epoch 00447: reducing learning rate of group 0 to 1.2851e-03.\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 446:\n",
      "train loss: 0.009015904883035874\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 447:\n",
      "train loss: 0.013468489530631449\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 448:\n",
      "train loss: 0.012555604246036856\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 449:\n",
      "train loss: 0.008955496739817513\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 450:\n",
      "train loss: 0.0078080416231283084\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 451:\n",
      "train loss: 0.013340783387436174\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 452:\n",
      "train loss: 0.012315307961196986\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 453:\n",
      "train loss: 0.009303768530662067\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 454:\n",
      "train loss: 0.008176127830689339\n",
      "Epoch 00456: reducing learning rate of group 0 to 1.2209e-03.\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 455:\n",
      "train loss: 0.012965869510370454\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 456:\n",
      "train loss: 0.011963314435388348\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 457:\n",
      "train loss: 0.008600262911801161\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 458:\n",
      "train loss: 0.007596562499270496\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 459:\n",
      "train loss: 0.012365108687719121\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 460:\n",
      "train loss: 0.011358278488454604\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 461:\n",
      "train loss: 0.009231198097289577\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 462:\n",
      "train loss: 0.008131735916030359\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 463:\n",
      "train loss: 0.011946426078910156\n",
      "Epoch 00465: reducing learning rate of group 0 to 1.1598e-03.\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 464:\n",
      "train loss: 0.01102581292227896\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 465:\n",
      "train loss: 0.009430804864318494\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 466:\n",
      "train loss: 0.008262260383832735\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 467:\n",
      "train loss: 0.01093703595340464\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 468:\n",
      "train loss: 0.010120959213627908\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 469:\n",
      "train loss: 0.009218625035548442\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 470:\n",
      "train loss: 0.007985398818887152\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 471:\n",
      "train loss: 0.011289709763003003\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 472:\n",
      "train loss: 0.010527610341099074\n",
      "Epoch 00474: reducing learning rate of group 0 to 1.1018e-03.\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 473:\n",
      "train loss: 0.008727290321596148\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 474:\n",
      "train loss: 0.0075154377924461805\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 475:\n",
      "train loss: 0.010768463646543767\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 476:\n",
      "train loss: 0.010016862255842255\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 477:\n",
      "train loss: 0.008281364243414465\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 478:\n",
      "train loss: 0.007155953183534164\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 479:\n",
      "train loss: 0.011093086786370224\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 480:\n",
      "train loss: 0.010335494054210295\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 481:\n",
      "train loss: 0.007957220103471208\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 482:\n",
      "train loss: 0.006874454214085014\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 483:\n",
      "train loss: 0.01131150514577245\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 484:\n",
      "train loss: 0.01053678622848423\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 485:\n",
      "train loss: 0.007780390209193611\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 486:\n",
      "train loss: 0.006742403289295991\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 487:\n",
      "train loss: 0.01137494510103733\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 488:\n",
      "train loss: 0.010581197088367135\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 489:\n",
      "train loss: 0.007769755268995988\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 490:\n",
      "train loss: 0.0067612224912979605\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 491:\n",
      "train loss: 0.011303875165920498\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 492:\n",
      "train loss: 0.010499538823732484\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 493:\n",
      "train loss: 0.007874184001370086\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 494:\n",
      "train loss: 0.006867123033189123\n",
      "Epoch 00496: reducing learning rate of group 0 to 1.0467e-03.\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 495:\n",
      "train loss: 0.011175773494527398\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 496:\n",
      "train loss: 0.010376749449444696\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 497:\n",
      "train loss: 0.007146920459020053\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 498:\n",
      "train loss: 0.006319456458089702\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 499:\n",
      "train loss: 0.010632574873817\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 500:\n",
      "train loss: 0.009769492676635896\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 501:\n",
      "train loss: 0.00783259198539294\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 502:\n",
      "train loss: 0.006929731544137575\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 503:\n",
      "train loss: 0.010112866045268799\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 504:\n",
      "train loss: 0.009313753659327715\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 505:\n",
      "train loss: 0.008168709544303118\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 506:\n",
      "train loss: 0.007121436531108127\n",
      "Epoch 00508: reducing learning rate of group 0 to 9.9440e-04.\n",
      "lr: 0.000994402569870922\n",
      "Epoch 507:\n",
      "train loss: 0.01007973817434573\n",
      "lr: 0.000994402569870922\n",
      "Epoch 508:\n",
      "train loss: 0.009387140930106373\n",
      "lr: 0.000994402569870922\n",
      "Epoch 509:\n",
      "train loss: 0.00716521904467592\n",
      "lr: 0.000994402569870922\n",
      "Epoch 510:\n",
      "train loss: 0.006246630736668512\n",
      "lr: 0.000994402569870922\n",
      "Epoch 511:\n",
      "train loss: 0.009963490434042085\n",
      "lr: 0.000994402569870922\n",
      "Epoch 512:\n",
      "train loss: 0.009225453653658008\n",
      "lr: 0.000994402569870922\n",
      "Epoch 513:\n",
      "train loss: 0.007391604283316867\n",
      "lr: 0.000994402569870922\n",
      "Epoch 514:\n",
      "train loss: 0.006494344758212549\n",
      "lr: 0.000994402569870922\n",
      "Epoch 515:\n",
      "train loss: 0.009702882522488986\n",
      "lr: 0.000994402569870922\n",
      "Epoch 516:\n",
      "train loss: 0.00895180353139149\n",
      "lr: 0.000994402569870922\n",
      "Epoch 517:\n",
      "train loss: 0.007634118347680118\n",
      "lr: 0.000994402569870922\n",
      "Epoch 518:\n",
      "train loss: 0.006679134900318453\n",
      "Epoch 00520: reducing learning rate of group 0 to 9.4468e-04.\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 519:\n",
      "train loss: 0.009593230367316548\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 520:\n",
      "train loss: 0.008890900526970929\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 521:\n",
      "train loss: 0.006847430134922041\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 522:\n",
      "train loss: 0.006004650968394648\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 523:\n",
      "train loss: 0.00935441802374896\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 524:\n",
      "train loss: 0.008629783131147154\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 525:\n",
      "train loss: 0.007136131258983568\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 526:\n",
      "train loss: 0.0062696190277278544\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 527:\n",
      "train loss: 0.009129698148581649\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 528:\n",
      "train loss: 0.008431129642531935\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 529:\n",
      "train loss: 0.007268335330887521\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 530:\n",
      "train loss: 0.006336482785028358\n",
      "Epoch 00532: reducing learning rate of group 0 to 8.9745e-04.\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 531:\n",
      "train loss: 0.00914114489853634\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 532:\n",
      "train loss: 0.008497542377974255\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 533:\n",
      "train loss: 0.006400470352343343\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 534:\n",
      "train loss: 0.00559202411707274\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 535:\n",
      "train loss: 0.008996008746555975\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 536:\n",
      "train loss: 0.00831899518159782\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 537:\n",
      "train loss: 0.006630014600273796\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 538:\n",
      "train loss: 0.005825299809545615\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 539:\n",
      "train loss: 0.008766951890022743\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 540:\n",
      "train loss: 0.008095852180902833\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 541:\n",
      "train loss: 0.006814404334708851\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 542:\n",
      "train loss: 0.0059515283951974566\n",
      "Epoch 00544: reducing learning rate of group 0 to 8.5258e-04.\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 543:\n",
      "train loss: 0.008708649983063376\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 544:\n",
      "train loss: 0.008087464785328685\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 545:\n",
      "train loss: 0.006065337733210805\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 546:\n",
      "train loss: 0.005308619755675908\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 547:\n",
      "train loss: 0.008521034770105799\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 548:\n",
      "train loss: 0.00787842035862592\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 549:\n",
      "train loss: 0.006310828935543408\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 550:\n",
      "train loss: 0.005541429061417109\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 551:\n",
      "train loss: 0.00830754077913015\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 552:\n",
      "train loss: 0.007681020502197145\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 553:\n",
      "train loss: 0.006461653296321195\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 554:\n",
      "train loss: 0.005634102827128789\n",
      "Epoch 00556: reducing learning rate of group 0 to 8.0995e-04.\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 555:\n",
      "train loss: 0.008280530743983962\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 556:\n",
      "train loss: 0.007701469553795753\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 557:\n",
      "train loss: 0.0057286755425757395\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 558:\n",
      "train loss: 0.005012079664534741\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 559:\n",
      "train loss: 0.008103687890785492\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 560:\n",
      "train loss: 0.0074960340862563445\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 561:\n",
      "train loss: 0.0059759130944962964\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 562:\n",
      "train loss: 0.005249603697135778\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 563:\n",
      "train loss: 0.007882266913132105\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 564:\n",
      "train loss: 0.007288196257970446\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 565:\n",
      "train loss: 0.006138994425282858\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 566:\n",
      "train loss: 0.005352669900882818\n",
      "Epoch 00568: reducing learning rate of group 0 to 7.6945e-04.\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 567:\n",
      "train loss: 0.007847296450155479\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 568:\n",
      "train loss: 0.00730110135003706\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 569:\n",
      "train loss: 0.005447928051693263\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 570:\n",
      "train loss: 0.0047656883946115055\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 571:\n",
      "train loss: 0.007679322250723017\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 572:\n",
      "train loss: 0.0071072995639265574\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 573:\n",
      "train loss: 0.005678600937769105\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 574:\n",
      "train loss: 0.004985078064153067\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 575:\n",
      "train loss: 0.007477253953415864\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 576:\n",
      "train loss: 0.006918838156544741\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 577:\n",
      "train loss: 0.0058231637844500615\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 578:\n",
      "train loss: 0.005073636432549583\n",
      "Epoch 00580: reducing learning rate of group 0 to 7.3098e-04.\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 579:\n",
      "train loss: 0.007452716731730691\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 580:\n",
      "train loss: 0.006938587484241732\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 581:\n",
      "train loss: 0.005163518587350313\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 582:\n",
      "train loss: 0.004516915419251903\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 583:\n",
      "train loss: 0.007289765864191873\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 584:\n",
      "train loss: 0.006748346217368623\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 585:\n",
      "train loss: 0.005389590008101572\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 586:\n",
      "train loss: 0.00473096798329562\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 587:\n",
      "train loss: 0.0070941188996116\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 588:\n",
      "train loss: 0.006566991328914898\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 589:\n",
      "train loss: 0.00552703046070179\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 590:\n",
      "train loss: 0.004812917905649802\n",
      "Epoch 00592: reducing learning rate of group 0 to 6.9443e-04.\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 591:\n",
      "train loss: 0.0070754521911430505\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 592:\n",
      "train loss: 0.006591483676054218\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 593:\n",
      "train loss: 0.004895935282030654\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 594:\n",
      "train loss: 0.004281633263161029\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 595:\n",
      "train loss: 0.006922615204920386\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 596:\n",
      "train loss: 0.0064117142300629095\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 597:\n",
      "train loss: 0.005110243916798895\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 598:\n",
      "train loss: 0.0044842360788583035\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 599:\n",
      "train loss: 0.006737444101672907\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 600:\n",
      "train loss: 0.006240043742352501\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 601:\n",
      "train loss: 0.005239685783615498\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 602:\n",
      "train loss: 0.00456048610784576\n",
      "Epoch 00604: reducing learning rate of group 0 to 6.5971e-04.\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 603:\n",
      "train loss: 0.006722010364884221\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 604:\n",
      "train loss: 0.006265835671231374\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 605:\n",
      "train loss: 0.0046400422065598825\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 606:\n",
      "train loss: 0.004057632387135292\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 607:\n",
      "train loss: 0.006574042537393312\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 608:\n",
      "train loss: 0.006090972456171182\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 609:\n",
      "train loss: 0.0048477081624734\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 610:\n",
      "train loss: 0.00425274776426054\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 611:\n",
      "train loss: 0.0063971487821903456\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 612:\n",
      "train loss: 0.005928202867845591\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 613:\n",
      "train loss: 0.004968813085732145\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 614:\n",
      "train loss: 0.004321957340665014\n",
      "Epoch 00616: reducing learning rate of group 0 to 6.2672e-04.\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 615:\n",
      "train loss: 0.006386889341122867\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 616:\n",
      "train loss: 0.0059574580293631505\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 617:\n",
      "train loss: 0.004396513810144309\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 618:\n",
      "train loss: 0.0038442694109657462\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 619:\n",
      "train loss: 0.006244774057747878\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 620:\n",
      "train loss: 0.005788237464128255\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 621:\n",
      "train loss: 0.004597584968597422\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 622:\n",
      "train loss: 0.004032367542061535\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 623:\n",
      "train loss: 0.00607485995301429\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 624:\n",
      "train loss: 0.0056323643814582235\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 625:\n",
      "train loss: 0.004712499665681795\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 626:\n",
      "train loss: 0.004096615016039318\n",
      "Epoch 00628: reducing learning rate of group 0 to 5.9539e-04.\n",
      "lr: 0.000595385551055294\n",
      "Epoch 627:\n",
      "train loss: 0.006068223846520539\n",
      "lr: 0.000595385551055294\n",
      "Epoch 628:\n",
      "train loss: 0.005664099840722471\n",
      "lr: 0.000595385551055294\n",
      "Epoch 629:\n",
      "train loss: 0.004166598489878951\n",
      "lr: 0.000595385551055294\n",
      "Epoch 630:\n",
      "train loss: 0.003642777652662781\n",
      "lr: 0.000595385551055294\n",
      "Epoch 631:\n",
      "train loss: 0.005931767640742506\n",
      "lr: 0.000595385551055294\n",
      "Epoch 632:\n",
      "train loss: 0.0055002558547520945\n",
      "lr: 0.000595385551055294\n",
      "Epoch 633:\n",
      "train loss: 0.004361134988672375\n",
      "lr: 0.000595385551055294\n",
      "Epoch 634:\n",
      "train loss: 0.0038239729374521907\n",
      "lr: 0.000595385551055294\n",
      "Epoch 635:\n",
      "train loss: 0.005769076739774638\n",
      "lr: 0.000595385551055294\n",
      "Epoch 636:\n",
      "train loss: 0.00535188610091979\n",
      "lr: 0.000595385551055294\n",
      "Epoch 637:\n",
      "train loss: 0.004469233108851256\n",
      "lr: 0.000595385551055294\n",
      "Epoch 638:\n",
      "train loss: 0.003882647429971796\n",
      "Epoch 00640: reducing learning rate of group 0 to 5.6562e-04.\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 639:\n",
      "train loss: 0.005766310175412949\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 640:\n",
      "train loss: 0.0053859400196941375\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 641:\n",
      "train loss: 0.003948577566821704\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 642:\n",
      "train loss: 0.003451975359104881\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 643:\n",
      "train loss: 0.005634580588014974\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 644:\n",
      "train loss: 0.005226654665050458\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 645:\n",
      "train loss: 0.004137482027610597\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 646:\n",
      "train loss: 0.003626941119069341\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 647:\n",
      "train loss: 0.005478410913314557\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 648:\n",
      "train loss: 0.005084936398616194\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 649:\n",
      "train loss: 0.004239399952038271\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 650:\n",
      "train loss: 0.003680547999323282\n",
      "Epoch 00652: reducing learning rate of group 0 to 5.3734e-04.\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 651:\n",
      "train loss: 0.005479505287819311\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 652:\n",
      "train loss: 0.0051218206341237275\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 653:\n",
      "train loss: 0.003741819944137216\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 654:\n",
      "train loss: 0.00327089064692551\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 655:\n",
      "train loss: 0.005352931278028777\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 656:\n",
      "train loss: 0.004967198469199002\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 657:\n",
      "train loss: 0.0039252273432977694\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 658:\n",
      "train loss: 0.0034400550922059074\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 659:\n",
      "train loss: 0.005202873495318519\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 660:\n",
      "train loss: 0.004831876703380734\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 661:\n",
      "train loss: 0.004021256530444473\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 662:\n",
      "train loss: 0.0034888209090932075\n",
      "Epoch 00664: reducing learning rate of group 0 to 5.1047e-04.\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 663:\n",
      "train loss: 0.00520738631738736\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 664:\n",
      "train loss: 0.004870874950377821\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 665:\n",
      "train loss: 0.0035461942326830277\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 666:\n",
      "train loss: 0.0030997535259100254\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 667:\n",
      "train loss: 0.005085233539883064\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 668:\n",
      "train loss: 0.004720496086597013\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 669:\n",
      "train loss: 0.0037243745549897663\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 670:\n",
      "train loss: 0.0032631949924489586\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 671:\n",
      "train loss: 0.004941166350303995\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 672:\n",
      "train loss: 0.004591286768424478\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 673:\n",
      "train loss: 0.0038147761929698673\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 674:\n",
      "train loss: 0.003307406426707903\n",
      "Epoch 00676: reducing learning rate of group 0 to 4.8495e-04.\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 675:\n",
      "train loss: 0.00494905836482606\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 676:\n",
      "train loss: 0.004632697003594951\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 677:\n",
      "train loss: 0.003360518438051311\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 678:\n",
      "train loss: 0.0029372930918104023\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 679:\n",
      "train loss: 0.004831402966573706\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 680:\n",
      "train loss: 0.004486330359613342\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 681:\n",
      "train loss: 0.003533958625450774\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 682:\n",
      "train loss: 0.003095660134388862\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 683:\n",
      "train loss: 0.0046927727968688986\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 684:\n",
      "train loss: 0.004362878368499617\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 685:\n",
      "train loss: 0.0036189897479211217\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 686:\n",
      "train loss: 0.0031354725384914804\n",
      "Epoch 00688: reducing learning rate of group 0 to 4.6070e-04.\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 687:\n",
      "train loss: 0.0047037720813478765\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 688:\n",
      "train loss: 0.004406219878967114\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 689:\n",
      "train loss: 0.0031848934495586857\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 690:\n",
      "train loss: 0.002783764945178991\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 691:\n",
      "train loss: 0.004590122306611841\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 692:\n",
      "train loss: 0.0042636524187081686\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 693:\n",
      "train loss: 0.0033536537305414033\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 694:\n",
      "train loss: 0.0029370305959184485\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 695:\n",
      "train loss: 0.004456861421919866\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 696:\n",
      "train loss: 0.004145691551333953\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 697:\n",
      "train loss: 0.003433662107126268\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 698:\n",
      "train loss: 0.0029728411624873282\n",
      "Epoch 00700: reducing learning rate of group 0 to 4.3766e-04.\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 699:\n",
      "train loss: 0.004470732295185876\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 700:\n",
      "train loss: 0.004190991956110609\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 701:\n",
      "train loss: 0.0030183827504871243\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 702:\n",
      "train loss: 0.002638198188836438\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 703:\n",
      "train loss: 0.004361156629123761\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 704:\n",
      "train loss: 0.004052090786332919\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 705:\n",
      "train loss: 0.003182806554007456\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 706:\n",
      "train loss: 0.002786845373012317\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 707:\n",
      "train loss: 0.004232827239914436\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 708:\n",
      "train loss: 0.003939325164002845\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 709:\n",
      "train loss: 0.0032580028995396245\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 710:\n",
      "train loss: 0.0028188216809162917\n",
      "Epoch 00712: reducing learning rate of group 0 to 4.1578e-04.\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 711:\n",
      "train loss: 0.0042492930751687995\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 712:\n",
      "train loss: 0.00398610846145198\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 713:\n",
      "train loss: 0.00286101531544695\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 714:\n",
      "train loss: 0.002500738424188096\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 715:\n",
      "train loss: 0.0041434019057692605\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 716:\n",
      "train loss: 0.0038507551395532682\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 717:\n",
      "train loss: 0.003021050657153123\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 718:\n",
      "train loss: 0.0026446841616124773\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 719:\n",
      "train loss: 0.004020000441573181\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 720:\n",
      "train loss: 0.003742991213403947\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 721:\n",
      "train loss: 0.0030917521753827852\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 722:\n",
      "train loss: 0.0026732245970212468\n",
      "Epoch 00724: reducing learning rate of group 0 to 3.9499e-04.\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 723:\n",
      "train loss: 0.0040387261241244836\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 724:\n",
      "train loss: 0.003791099436399221\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 725:\n",
      "train loss: 0.0027120051327224727\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 726:\n",
      "train loss: 0.0023705674533454502\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 727:\n",
      "train loss: 0.003936664854124021\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 728:\n",
      "train loss: 0.003659357245823556\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 729:\n",
      "train loss: 0.002867750291486876\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 730:\n",
      "train loss: 0.0025100912141381767\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 731:\n",
      "train loss: 0.0038178346557399662\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 732:\n",
      "train loss: 0.0035562691066068166\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 733:\n",
      "train loss: 0.002934280622449758\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 734:\n",
      "train loss: 0.0025355302444352154\n",
      "Epoch 00736: reducing learning rate of group 0 to 3.7524e-04.\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 735:\n",
      "train loss: 0.0038383951060443748\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 736:\n",
      "train loss: 0.0036051098999355996\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 737:\n",
      "train loss: 0.002571396534084852\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 738:\n",
      "train loss: 0.002247801311284163\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 739:\n",
      "train loss: 0.0037399657662522305\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 740:\n",
      "train loss: 0.003477082827515891\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 741:\n",
      "train loss: 0.002722577735825565\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 742:\n",
      "train loss: 0.002382689889795146\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 743:\n",
      "train loss: 0.0036257994227375657\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 744:\n",
      "train loss: 0.0033785591766169417\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 745:\n",
      "train loss: 0.00278522206975994\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 746:\n",
      "train loss: 0.002405456113598417\n",
      "Epoch 00748: reducing learning rate of group 0 to 3.5648e-04.\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 747:\n",
      "train loss: 0.0036476971161313056\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 748:\n",
      "train loss: 0.00342770264384776\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 749:\n",
      "train loss: 0.002438530179559037\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 750:\n",
      "train loss: 0.002131762509403056\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 751:\n",
      "train loss: 0.003553134102581814\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 752:\n",
      "train loss: 0.003303712266574057\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 753:\n",
      "train loss: 0.002584897474376939\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 754:\n",
      "train loss: 0.002262020955865\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 755:\n",
      "train loss: 0.003443388274306349\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 756:\n",
      "train loss: 0.003209394787275684\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 757:\n",
      "train loss: 0.002644152521485195\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 758:\n",
      "train loss: 0.0022826847183172244\n",
      "Epoch 00760: reducing learning rate of group 0 to 3.3866e-04.\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 759:\n",
      "train loss: 0.0034659725805497575\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 760:\n",
      "train loss: 0.003258066108541129\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 761:\n",
      "train loss: 0.002313426691864376\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 762:\n",
      "train loss: 0.0020225024519589093\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 763:\n",
      "train loss: 0.0033753287403493195\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 764:\n",
      "train loss: 0.0031385168398219924\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 765:\n",
      "train loss: 0.002454434906388776\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 766:\n",
      "train loss: 0.002147764373195499\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 767:\n",
      "train loss: 0.0032701813281098085\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 768:\n",
      "train loss: 0.003048353298626907\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 769:\n",
      "train loss: 0.002510572379923432\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 770:\n",
      "train loss: 0.0021668115124843295\n",
      "Epoch 00772: reducing learning rate of group 0 to 3.2172e-04.\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 771:\n",
      "train loss: 0.0032927562193755893\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 772:\n",
      "train loss: 0.0030958077741305634\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 773:\n",
      "train loss: 0.002195565022929938\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 774:\n",
      "train loss: 0.0019194985868183265\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 775:\n",
      "train loss: 0.0032063662472815407\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 776:\n",
      "train loss: 0.002981325861739604\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 777:\n",
      "train loss: 0.002330537772223656\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 778:\n",
      "train loss: 0.0020394324115058683\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 779:\n",
      "train loss: 0.003105729355205349\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 780:\n",
      "train loss: 0.00289494766326768\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 781:\n",
      "train loss: 0.002384249870064497\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 782:\n",
      "train loss: 0.0020577002514831533\n",
      "Epoch 00784: reducing learning rate of group 0 to 3.0564e-04.\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 783:\n",
      "train loss: 0.0031273753740262687\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 784:\n",
      "train loss: 0.002940193206379207\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 785:\n",
      "train loss: 0.002084890479566884\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 786:\n",
      "train loss: 0.0018227137712712256\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 787:\n",
      "train loss: 0.003045522419369724\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 788:\n",
      "train loss: 0.0028314590516886513\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 789:\n",
      "train loss: 0.002213051962873199\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 790:\n",
      "train loss: 0.0019368137223164433\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 791:\n",
      "train loss: 0.0029496691961854163\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 792:\n",
      "train loss: 0.0027489132386500355\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 793:\n",
      "train loss: 0.002264524401667688\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 794:\n",
      "train loss: 0.001954773488558934\n",
      "Epoch 00796: reducing learning rate of group 0 to 2.9035e-04.\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 795:\n",
      "train loss: 0.002969559464525766\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 796:\n",
      "train loss: 0.0027909511577143024\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 797:\n",
      "train loss: 0.0019809720806620115\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 798:\n",
      "train loss: 0.0017317611110870953\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 799:\n",
      "train loss: 0.0028925658480209483\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 800:\n",
      "train loss: 0.0026887588830108327\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 801:\n",
      "train loss: 0.002101332010791035\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 802:\n",
      "train loss: 0.0018394001240374701\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 803:\n",
      "train loss: 0.0028016037916067213\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 804:\n",
      "train loss: 0.002609762208399012\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 805:\n",
      "train loss: 0.0021513353574712556\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 806:\n",
      "train loss: 0.0018580323642245391\n",
      "Epoch 00808: reducing learning rate of group 0 to 2.7584e-04.\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 807:\n",
      "train loss: 0.002818667544110032\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 808:\n",
      "train loss: 0.002647505772128682\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 809:\n",
      "train loss: 0.0018835955608135545\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 810:\n",
      "train loss: 0.0016464725524784664\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 811:\n",
      "train loss: 0.002746869463736986\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 812:\n",
      "train loss: 0.002552555651194486\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 813:\n",
      "train loss: 0.0019954205004248495\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 814:\n",
      "train loss: 0.0017471492673061225\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 815:\n",
      "train loss: 0.002661160538998665\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 816:\n",
      "train loss: 0.0024773620614098753\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 817:\n",
      "train loss: 0.0020438452631906987\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 818:\n",
      "train loss: 0.0017666682845997102\n",
      "Epoch 00820: reducing learning rate of group 0 to 2.6205e-04.\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 819:\n",
      "train loss: 0.0026747076670033524\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 820:\n",
      "train loss: 0.0025098316925953237\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 821:\n",
      "train loss: 0.0017923211792840624\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 822:\n",
      "train loss: 0.0015665379613002133\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 823:\n",
      "train loss: 0.002608138382746512\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 824:\n",
      "train loss: 0.0024226787333004912\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 825:\n",
      "train loss: 0.0018947209418773547\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 826:\n",
      "train loss: 0.0016595636639359533\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 827:\n",
      "train loss: 0.002527961576548343\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 828:\n",
      "train loss: 0.002351222832763325\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 829:\n",
      "train loss: 0.001942156347006006\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 830:\n",
      "train loss: 0.0016807749147917084\n",
      "Epoch 00832: reducing learning rate of group 0 to 2.4894e-04.\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 831:\n",
      "train loss: 0.0025371246928905626\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 832:\n",
      "train loss: 0.0023776116800565735\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 833:\n",
      "train loss: 0.0017066724272532265\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 834:\n",
      "train loss: 0.0014915754669027697\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 835:\n",
      "train loss: 0.0024758849831025553\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 836:\n",
      "train loss: 0.002298511986141935\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 837:\n",
      "train loss: 0.001799474115461958\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 838:\n",
      "train loss: 0.001576788680573855\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 839:\n",
      "train loss: 0.002401553068265916\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 840:\n",
      "train loss: 0.002231279612739074\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 841:\n",
      "train loss: 0.0018453134581055135\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 842:\n",
      "train loss: 0.0015993258714151958\n",
      "Epoch 00844: reducing learning rate of group 0 to 2.3650e-04.\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 843:\n",
      "train loss: 0.002406224276139449\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 844:\n",
      "train loss: 0.0022511377356004363\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 845:\n",
      "train loss: 0.0016261159983944414\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 846:\n",
      "train loss: 0.0014212486312152428\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 847:\n",
      "train loss: 0.0023498334063288356\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 848:\n",
      "train loss: 0.0021799690655226497\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 849:\n",
      "train loss: 0.0017090616993957256\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 850:\n",
      "train loss: 0.0014982938859921186\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 851:\n",
      "train loss: 0.0022815736508306245\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 852:\n",
      "train loss: 0.0021170331262899858\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 853:\n",
      "train loss: 0.0017536119535318379\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 854:\n",
      "train loss: 0.0015225340734668202\n",
      "Epoch 00856: reducing learning rate of group 0 to 2.2467e-04.\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 855:\n",
      "train loss: 0.002281479942928975\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 856:\n",
      "train loss: 0.002130304107334573\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 857:\n",
      "train loss: 0.0015499388632987537\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 858:\n",
      "train loss: 0.001354912243984844\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 859:\n",
      "train loss: 0.002229753057803521\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 860:\n",
      "train loss: 0.0020666679257110823\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 861:\n",
      "train loss: 0.0016237369365972416\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 862:\n",
      "train loss: 0.0014242760991843174\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 863:\n",
      "train loss: 0.0021675244277016004\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 864:\n",
      "train loss: 0.0020084637348766875\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 865:\n",
      "train loss: 0.0016660693225722138\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 866:\n",
      "train loss: 0.0014492664244455257\n",
      "Epoch 00868: reducing learning rate of group 0 to 2.1344e-04.\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 867:\n",
      "train loss: 0.0021633574277954253\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 868:\n",
      "train loss: 0.002015582010685111\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 869:\n",
      "train loss: 0.0014775993798914213\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 870:\n",
      "train loss: 0.001292183186541516\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 871:\n",
      "train loss: 0.00211548698987172\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 872:\n",
      "train loss: 0.001958754772051144\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 873:\n",
      "train loss: 0.0015426826927441226\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 874:\n",
      "train loss: 0.0013540104513185417\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 875:\n",
      "train loss: 0.0020592003798204682\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 876:\n",
      "train loss: 0.0019051749500602643\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 877:\n",
      "train loss: 0.0015831015939057093\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 878:\n",
      "train loss: 0.0013798666136515298\n",
      "Epoch 00880: reducing learning rate of group 0 to 2.0277e-04.\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 879:\n",
      "train loss: 0.0020512287471190767\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 880:\n",
      "train loss: 0.0019068488147252888\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 881:\n",
      "train loss: 0.0014083816189067475\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 882:\n",
      "train loss: 0.001232288774951163\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 883:\n",
      "train loss: 0.0020070492815258455\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 884:\n",
      "train loss: 0.001856136895990644\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 885:\n",
      "train loss: 0.001466004066055925\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 886:\n",
      "train loss: 0.001287573489140641\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 887:\n",
      "train loss: 0.0019562137143425504\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 888:\n",
      "train loss: 0.0018072809402674994\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 889:\n",
      "train loss: 0.001503666134416544\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 890:\n",
      "train loss: 0.0013131444890910739\n",
      "Epoch 00892: reducing learning rate of group 0 to 1.9263e-04.\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 891:\n",
      "train loss: 0.0019456090606325557\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 892:\n",
      "train loss: 0.0018045668971250666\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 893:\n",
      "train loss: 0.001341877901494614\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 894:\n",
      "train loss: 0.0011748983746344656\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 895:\n",
      "train loss: 0.0019043479692546778\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 896:\n",
      "train loss: 0.0017591066834802803\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 897:\n",
      "train loss: 0.001392709905259528\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 898:\n",
      "train loss: 0.0012240288981455458\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 899:\n",
      "train loss: 0.0018586232369368098\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 900:\n",
      "train loss: 0.00171459483032764\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 901:\n",
      "train loss: 0.0014281325065363336\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 902:\n",
      "train loss: 0.0012494863529968151\n",
      "Epoch 00904: reducing learning rate of group 0 to 1.8300e-04.\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 903:\n",
      "train loss: 0.0018457745708392258\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 904:\n",
      "train loss: 0.0017084540611204508\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 905:\n",
      "train loss: 0.001277550238175792\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 906:\n",
      "train loss: 0.0011193251365785224\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 907:\n",
      "train loss: 0.0018074586448268272\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 908:\n",
      "train loss: 0.0016676358954235621\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 909:\n",
      "train loss: 0.0013228698964467386\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 910:\n",
      "train loss: 0.001163412291344427\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 911:\n",
      "train loss: 0.0017661334457242757\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 912:\n",
      "train loss: 0.0016272820634161513\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 913:\n",
      "train loss: 0.0013554497757812445\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 914:\n",
      "train loss: 0.0011877317434220334\n",
      "Epoch 00916: reducing learning rate of group 0 to 1.7385e-04.\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 915:\n",
      "train loss: 0.0017522403059376166\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 916:\n",
      "train loss: 0.0016188607520100388\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 917:\n",
      "train loss: 0.0012151538063034063\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 918:\n",
      "train loss: 0.0010653764245283361\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 919:\n",
      "train loss: 0.001716244494528757\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 920:\n",
      "train loss: 0.0015819319303957836\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 921:\n",
      "train loss: 0.0012555400640189584\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 922:\n",
      "train loss: 0.001104802143789024\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 923:\n",
      "train loss: 0.0016789245732994039\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 924:\n",
      "train loss: 0.0015452110719224486\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 925:\n",
      "train loss: 0.0012859741365972258\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 926:\n",
      "train loss: 0.0011283552752990873\n",
      "Epoch 00928: reducing learning rate of group 0 to 1.6515e-04.\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 927:\n",
      "train loss: 0.001664165158974612\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 928:\n",
      "train loss: 0.0015352325229811268\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 929:\n",
      "train loss: 0.0011544575363002072\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 930:\n",
      "train loss: 0.001012674384466621\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 931:\n",
      "train loss: 0.0016306008506996822\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 932:\n",
      "train loss: 0.0015017165166332984\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 933:\n",
      "train loss: 0.0011910090094153824\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 934:\n",
      "train loss: 0.0010484671222530705\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 935:\n",
      "train loss: 0.0015965447048083595\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 936:\n",
      "train loss: 0.0014682870451226135\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 937:\n",
      "train loss: 0.0012189252264242338\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 938:\n",
      "train loss: 0.0010705039098585082\n",
      "Epoch 00940: reducing learning rate of group 0 to 1.5690e-04.\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 939:\n",
      "train loss: 0.0015818666584070554\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 940:\n",
      "train loss: 0.0014576258815764496\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 941:\n",
      "train loss: 0.001095474486287272\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 942:\n",
      "train loss: 0.0009613045398679092\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 943:\n",
      "train loss: 0.0015501920422455553\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 944:\n",
      "train loss: 0.0014268905899757222\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 945:\n",
      "train loss: 0.0011286216477365753\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 946:\n",
      "train loss: 0.0009937790621264875\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 947:\n",
      "train loss: 0.001519040804022819\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 948:\n",
      "train loss: 0.0013961917974692473\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 949:\n",
      "train loss: 0.0011548008405429485\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 950:\n",
      "train loss: 0.001014869004381553\n",
      "Epoch 00952: reducing learning rate of group 0 to 1.4905e-04.\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 951:\n",
      "train loss: 0.0015043063357136313\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 952:\n",
      "train loss: 0.0013851407815870635\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 953:\n",
      "train loss: 0.0010383624169156196\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 954:\n",
      "train loss: 0.0009113356559066478\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 955:\n",
      "train loss: 0.00147456898998359\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 956:\n",
      "train loss: 0.0013567630164517215\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 957:\n",
      "train loss: 0.0010690332327900841\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 958:\n",
      "train loss: 0.0009414066255977589\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 959:\n",
      "train loss: 0.001445620844096672\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 960:\n",
      "train loss: 0.001328389598684666\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 961:\n",
      "train loss: 0.0010932775477194086\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 962:\n",
      "train loss: 0.0009610714608044638\n",
      "Epoch 00964: reducing learning rate of group 0 to 1.4160e-04.\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 963:\n",
      "train loss: 0.001431469731147029\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 964:\n",
      "train loss: 0.0013174817288978985\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 965:\n",
      "train loss: 0.0009834058716317344\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 966:\n",
      "train loss: 0.000863133462608206\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 967:\n",
      "train loss: 0.0014031889397279212\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 968:\n",
      "train loss: 0.00129095734442986\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 969:\n",
      "train loss: 0.0010118704582516838\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 970:\n",
      "train loss: 0.0008910054509187099\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 971:\n",
      "train loss: 0.0013761930283162462\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 972:\n",
      "train loss: 0.0012644383011074481\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 973:\n",
      "train loss: 0.0010348968062993472\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 974:\n",
      "train loss: 0.0009098633165467022\n",
      "Epoch 00976: reducing learning rate of group 0 to 1.3452e-04.\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 975:\n",
      "train loss: 0.0013623022110274166\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 976:\n",
      "train loss: 0.0012536660813582661\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 977:\n",
      "train loss: 0.0009308962356499698\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 978:\n",
      "train loss: 0.0008169490280116104\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 979:\n",
      "train loss: 0.0013354898003832174\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 980:\n",
      "train loss: 0.0012286629856650612\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 981:\n",
      "train loss: 0.000957875203603049\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 982:\n",
      "train loss: 0.0008433524231828496\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 983:\n",
      "train loss: 0.0013098777616939107\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 984:\n",
      "train loss: 0.001203662764421311\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 985:\n",
      "train loss: 0.0009795244567919654\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 986:\n",
      "train loss: 0.0008610517993230963\n",
      "Epoch 00988: reducing learning rate of group 0 to 1.2779e-04.\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 987:\n",
      "train loss: 0.001296708478759585\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 988:\n",
      "train loss: 0.0011933640722114832\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 989:\n",
      "train loss: 0.0008810897081364858\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 990:\n",
      "train loss: 0.0007731056351039216\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 991:\n",
      "train loss: 0.0012710102842960823\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 992:\n",
      "train loss: 0.0011695681595948995\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 993:\n",
      "train loss: 0.0009066498845057509\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 994:\n",
      "train loss: 0.0007980660458065467\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 995:\n",
      "train loss: 0.0012467148208060693\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 996:\n",
      "train loss: 0.0011458156743784549\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 997:\n",
      "train loss: 0.0009274448939586558\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 998:\n",
      "train loss: 0.0008151077324917259\n",
      "Epoch 01000: reducing learning rate of group 0 to 1.2140e-04.\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 999:\n",
      "train loss: 0.0012339464233768638\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1000:\n",
      "train loss: 0.0011358993901104885\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1001:\n",
      "train loss: 0.0008340402896462981\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1002:\n",
      "train loss: 0.0007316423516076381\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1003:\n",
      "train loss: 0.0012094300666606326\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1004:\n",
      "train loss: 0.0011131360244360524\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1005:\n",
      "train loss: 0.000858660475109018\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1006:\n",
      "train loss: 0.0007556588170224749\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1007:\n",
      "train loss: 0.0011860683965697848\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1008:\n",
      "train loss: 0.0010904379682753033\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1009:\n",
      "train loss: 0.0008784000026044776\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 1010:\n",
      "train loss: 0.0007717167261098075\n",
      "Epoch 01012: reducing learning rate of group 0 to 1.1533e-04.\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1011:\n",
      "train loss: 0.0011740838087972159\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1012:\n",
      "train loss: 0.0010811573844996183\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1013:\n",
      "train loss: 0.0007897776385929994\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1014:\n",
      "train loss: 0.0006926482952893873\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1015:\n",
      "train loss: 0.0011505127485143735\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1016:\n",
      "train loss: 0.0010592683054872175\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1017:\n",
      "train loss: 0.0008133806811239619\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1018:\n",
      "train loss: 0.0007156161978317252\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1019:\n",
      "train loss: 0.0011281480801554882\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1020:\n",
      "train loss: 0.0010375083413299295\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1021:\n",
      "train loss: 0.0008324269753661799\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 1022:\n",
      "train loss: 0.0007310740259901377\n",
      "Epoch 01024: reducing learning rate of group 0 to 1.0957e-04.\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1023:\n",
      "train loss: 0.00111666211823345\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1024:\n",
      "train loss: 0.0010287476474412477\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1025:\n",
      "train loss: 0.0007481525961403193\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1026:\n",
      "train loss: 0.0006559692427664113\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1027:\n",
      "train loss: 0.0010941436371993652\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1028:\n",
      "train loss: 0.0010076709484585474\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1029:\n",
      "train loss: 0.0007710441806169719\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1030:\n",
      "train loss: 0.0006782193311372113\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1031:\n",
      "train loss: 0.0010725243694610746\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1032:\n",
      "train loss: 0.000986752449666746\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1033:\n",
      "train loss: 0.0007891837006449747\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 1034:\n",
      "train loss: 0.0006927869939327316\n",
      "Epoch 01036: reducing learning rate of group 0 to 1.0409e-04.\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1035:\n",
      "train loss: 0.0010618428523658237\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1036:\n",
      "train loss: 0.000978687439324091\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1037:\n",
      "train loss: 0.0007090720698990018\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1038:\n",
      "train loss: 0.000621567942740491\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1039:\n",
      "train loss: 0.0010401940055263462\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1040:\n",
      "train loss: 0.0009583358813963207\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1041:\n",
      "train loss: 0.0007311274549227428\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1042:\n",
      "train loss: 0.0006429573091198699\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1043:\n",
      "train loss: 0.0010194216493783436\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1044:\n",
      "train loss: 0.000938203461466843\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1045:\n",
      "train loss: 0.000748645861443381\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 1046:\n",
      "train loss: 0.0006569644407059573\n",
      "Epoch 01048: reducing learning rate of group 0 to 9.8884e-05.\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1047:\n",
      "train loss: 0.001009261384176872\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1048:\n",
      "train loss: 0.0009306772791436974\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1049:\n",
      "train loss: 0.0006723694843968644\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1050:\n",
      "train loss: 0.0005892696048703507\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1051:\n",
      "train loss: 0.0009885918259793453\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1052:\n",
      "train loss: 0.0009110477945520796\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1053:\n",
      "train loss: 0.000693783713790462\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1054:\n",
      "train loss: 0.0006100202100343255\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1055:\n",
      "train loss: 0.0009684972893757502\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1056:\n",
      "train loss: 0.0008916573098985719\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1057:\n",
      "train loss: 0.0007104842271732114\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 1058:\n",
      "train loss: 0.000623230132533218\n",
      "Epoch 01060: reducing learning rate of group 0 to 9.3939e-05.\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1059:\n",
      "train loss: 0.0009590812828372326\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1060:\n",
      "train loss: 0.0008847654930443321\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1061:\n",
      "train loss: 0.0006379241243426118\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1062:\n",
      "train loss: 0.0005590045587434466\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1063:\n",
      "train loss: 0.0009392347571082102\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1064:\n",
      "train loss: 0.0008658059094798049\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1065:\n",
      "train loss: 0.0006585617946703133\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1066:\n",
      "train loss: 0.0005789714285108734\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1067:\n",
      "train loss: 0.0009199218849806116\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1068:\n",
      "train loss: 0.000847123188688203\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1069:\n",
      "train loss: 0.0006746892562313395\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 1070:\n",
      "train loss: 0.0005916879592814485\n",
      "Epoch 01072: reducing learning rate of group 0 to 8.9242e-05.\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1071:\n",
      "train loss: 0.000910956472202559\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1072:\n",
      "train loss: 0.0008406634193783518\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1073:\n",
      "train loss: 0.0006056194065913574\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1074:\n",
      "train loss: 0.0005306414708725909\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1075:\n",
      "train loss: 0.0008920374648050211\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1076:\n",
      "train loss: 0.0008224142002694361\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1077:\n",
      "train loss: 0.0006255841416303914\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1078:\n",
      "train loss: 0.000549958024477881\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1079:\n",
      "train loss: 0.000873404520690009\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1080:\n",
      "train loss: 0.0008044338766578053\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1081:\n",
      "train loss: 0.0006409645578766314\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 1082:\n",
      "train loss: 0.000561999193847558\n",
      "Epoch 01084: reducing learning rate of group 0 to 8.4780e-05.\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1083:\n",
      "train loss: 0.0008650338645263376\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1084:\n",
      "train loss: 0.000798426635276564\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1085:\n",
      "train loss: 0.0005753260533381266\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1086:\n",
      "train loss: 0.0005040993488624055\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1087:\n",
      "train loss: 0.0008469101934486587\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1088:\n",
      "train loss: 0.0007808661278001968\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1089:\n",
      "train loss: 0.0005944746918164075\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1090:\n",
      "train loss: 0.0005226210979790504\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1091:\n",
      "train loss: 0.0008290605496329439\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1092:\n",
      "train loss: 0.0007635708098948408\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1093:\n",
      "train loss: 0.0006093163694406573\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 1094:\n",
      "train loss: 0.0005342665033031349\n",
      "Epoch 01096: reducing learning rate of group 0 to 8.0541e-05.\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1095:\n",
      "train loss: 0.0008209861413367501\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1096:\n",
      "train loss: 0.0007577792158453366\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1097:\n",
      "train loss: 0.0005469695848390151\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1098:\n",
      "train loss: 0.00047928542596521116\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1099:\n",
      "train loss: 0.0008037569090520616\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1100:\n",
      "train loss: 0.0007409855843535208\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1101:\n",
      "train loss: 0.000565325405227678\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1102:\n",
      "train loss: 0.0004970682036950054\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1103:\n",
      "train loss: 0.0007866482093038136\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1104:\n",
      "train loss: 0.0007244045354847081\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1105:\n",
      "train loss: 0.0005794751829110417\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1106:\n",
      "train loss: 0.0005081818374810497\n",
      "Epoch 01108: reducing learning rate of group 0 to 7.6514e-05.\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1107:\n",
      "train loss: 0.0007789498457838166\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1108:\n",
      "train loss: 0.0007187935086983949\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1109:\n",
      "train loss: 0.0005204037442600684\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1110:\n",
      "train loss: 0.0004561017489175424\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1111:\n",
      "train loss: 0.0007625047472054561\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1112:\n",
      "train loss: 0.0007027697441323604\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1113:\n",
      "train loss: 0.0005378230696555319\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1114:\n",
      "train loss: 0.00047300421997439684\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1115:\n",
      "train loss: 0.0007462401110440878\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1116:\n",
      "train loss: 0.0006869072271280763\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1117:\n",
      "train loss: 0.0005514444621826396\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1118:\n",
      "train loss: 0.0004838249645322092\n",
      "Epoch 01120: reducing learning rate of group 0 to 7.2689e-05.\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1119:\n",
      "train loss: 0.0007386401074072279\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1120:\n",
      "train loss: 0.0006812403211394976\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1121:\n",
      "train loss: 0.0004955635076082333\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1122:\n",
      "train loss: 0.0004344672355792414\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1123:\n",
      "train loss: 0.0007230618979255032\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1124:\n",
      "train loss: 0.0006660781034417698\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1125:\n",
      "train loss: 0.0005120243181218769\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1126:\n",
      "train loss: 0.00045049558412851056\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1127:\n",
      "train loss: 0.0007076381047677859\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1128:\n",
      "train loss: 0.0006509892225626978\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1129:\n",
      "train loss: 0.0005249796708142358\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1130:\n",
      "train loss: 0.00046090973723494173\n",
      "Epoch 01132: reducing learning rate of group 0 to 6.9054e-05.\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1131:\n",
      "train loss: 0.0007001912454655745\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1132:\n",
      "train loss: 0.0006452285017096329\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1133:\n",
      "train loss: 0.0004722699846659437\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1134:\n",
      "train loss: 0.00041425023405605683\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1135:\n",
      "train loss: 0.0006853838949676944\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1136:\n",
      "train loss: 0.0006309328456774288\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1137:\n",
      "train loss: 0.0004876595255593041\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1138:\n",
      "train loss: 0.0004292877424879163\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1139:\n",
      "train loss: 0.0006708898853921621\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1140:\n",
      "train loss: 0.0006166402783314282\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1141:\n",
      "train loss: 0.0005000587108037086\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1142:\n",
      "train loss: 0.0004394640329992492\n",
      "Epoch 01144: reducing learning rate of group 0 to 6.5601e-05.\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1143:\n",
      "train loss: 0.0006633883978915864\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1144:\n",
      "train loss: 0.0006106153622796516\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1145:\n",
      "train loss: 0.00045043245124962774\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1146:\n",
      "train loss: 0.00039534884482804895\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1147:\n",
      "train loss: 0.0006494012797950417\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1148:\n",
      "train loss: 0.0005972484860828516\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1149:\n",
      "train loss: 0.00046474447882286337\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1150:\n",
      "train loss: 0.0004094017817252755\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1151:\n",
      "train loss: 0.0006358344581835847\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1152:\n",
      "train loss: 0.0005838092682872244\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1153:\n",
      "train loss: 0.00047646575631182754\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1154:\n",
      "train loss: 0.0004192199907752636\n",
      "Epoch 01156: reducing learning rate of group 0 to 6.2321e-05.\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1155:\n",
      "train loss: 0.0006283636997572582\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1156:\n",
      "train loss: 0.0005775531927686803\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1157:\n",
      "train loss: 0.0004298339759687147\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1158:\n",
      "train loss: 0.00037758189904192087\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1159:\n",
      "train loss: 0.0006151156238318642\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1160:\n",
      "train loss: 0.0005650994459976845\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1161:\n",
      "train loss: 0.00044302174201297996\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1162:\n",
      "train loss: 0.00039058652275849063\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1163:\n",
      "train loss: 0.0006025269592202511\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1164:\n",
      "train loss: 0.0005525355385602843\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1165:\n",
      "train loss: 0.0004541267583755027\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1166:\n",
      "train loss: 0.00040012705268088517\n",
      "Epoch 01168: reducing learning rate of group 0 to 5.9205e-05.\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1167:\n",
      "train loss: 0.0005949920144292517\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1168:\n",
      "train loss: 0.0005460025333654158\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1169:\n",
      "train loss: 0.00041033956853838544\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1170:\n",
      "train loss: 0.00036080245854305646\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1171:\n",
      "train loss: 0.0005825015802585031\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1172:\n",
      "train loss: 0.0005344715173520261\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1173:\n",
      "train loss: 0.00042245175798516596\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1174:\n",
      "train loss: 0.000372802432930035\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1175:\n",
      "train loss: 0.0005708553103360277\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1176:\n",
      "train loss: 0.0005228092406788975\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1177:\n",
      "train loss: 0.0004328499629016922\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1178:\n",
      "train loss: 0.0003819360027931099\n",
      "Epoch 01180: reducing learning rate of group 0 to 5.6245e-05.\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1179:\n",
      "train loss: 0.0005633939346892056\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1180:\n",
      "train loss: 0.0005161309592746426\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1181:\n",
      "train loss: 0.00039171706606369285\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1182:\n",
      "train loss: 0.00034478998231183996\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1183:\n",
      "train loss: 0.0005516121331455915\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1184:\n",
      "train loss: 0.0005054928867629978\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1185:\n",
      "train loss: 0.00040276533198808993\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1186:\n",
      "train loss: 0.00035577216635266056\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1187:\n",
      "train loss: 0.0005409194711448232\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1188:\n",
      "train loss: 0.0004947381300772656\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1189:\n",
      "train loss: 0.00041248619906781333\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1190:\n",
      "train loss: 0.00036450582200852613\n",
      "Epoch 01192: reducing learning rate of group 0 to 5.3433e-05.\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1191:\n",
      "train loss: 0.0005335433353363445\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1192:\n",
      "train loss: 0.0004879846270783544\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1193:\n",
      "train loss: 0.0003737880387066965\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1194:\n",
      "train loss: 0.00032934972495848565\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1195:\n",
      "train loss: 0.0005224824910417974\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1196:\n",
      "train loss: 0.00047821530446358844\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1197:\n",
      "train loss: 0.00038385618486497973\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1198:\n",
      "train loss: 0.0003393868450591328\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1199:\n",
      "train loss: 0.0005126783335101034\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1200:\n",
      "train loss: 0.00046835167731501244\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1201:\n",
      "train loss: 0.0003928539282900237\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1202:\n",
      "train loss: 0.00034761164565698486\n",
      "Epoch 01204: reducing learning rate of group 0 to 5.0761e-05.\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1203:\n",
      "train loss: 0.0005055391372894606\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1204:\n",
      "train loss: 0.00046169476135111444\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1205:\n",
      "train loss: 0.00035634803814046966\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1206:\n",
      "train loss: 0.0003142721830392892\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1207:\n",
      "train loss: 0.0004951824709020161\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1208:\n",
      "train loss: 0.00045276171323801654\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1209:\n",
      "train loss: 0.000365474297954941\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1210:\n",
      "train loss: 0.00032338068686446446\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1211:\n",
      "train loss: 0.0004862546108897086\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1212:\n",
      "train loss: 0.000443774590434048\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1213:\n",
      "train loss: 0.0003737663245784819\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1214:\n",
      "train loss: 0.00033107581791455353\n",
      "Epoch 01216: reducing learning rate of group 0 to 4.8223e-05.\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1215:\n",
      "train loss: 0.0004794113688904281\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1216:\n",
      "train loss: 0.0004373223935011314\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1217:\n",
      "train loss: 0.00033922067592387047\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1218:\n",
      "train loss: 0.0002993696288030042\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1219:\n",
      "train loss: 0.0004697693551420563\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1220:\n",
      "train loss: 0.0004291859956480678\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1221:\n",
      "train loss: 0.00034749293202091826\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1222:\n",
      "train loss: 0.0003076324773009508\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1223:\n",
      "train loss: 0.00046163991169224824\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1224:\n",
      "train loss: 0.0004210207888957776\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1225:\n",
      "train loss: 0.0003550845861921938\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1226:\n",
      "train loss: 0.0003147483802287321\n",
      "Epoch 01228: reducing learning rate of group 0 to 4.5812e-05.\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1227:\n",
      "train loss: 0.00045519187045155044\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1228:\n",
      "train loss: 0.0004148909270313706\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1229:\n",
      "train loss: 0.00032229921328406577\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1230:\n",
      "train loss: 0.00028453892399441736\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1231:\n",
      "train loss: 0.00044623395866818995\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1232:\n",
      "train loss: 0.0004074904542264647\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1233:\n",
      "train loss: 0.0003297892526747317\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1234:\n",
      "train loss: 0.00029201691091606066\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1235:\n",
      "train loss: 0.000438848777010769\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1236:\n",
      "train loss: 0.00040008381552110586\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1237:\n",
      "train loss: 0.0003367337358524525\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1238:\n",
      "train loss: 0.0002985777918441569\n",
      "Epoch 01240: reducing learning rate of group 0 to 4.3521e-05.\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1239:\n",
      "train loss: 0.0004328094045430346\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1240:\n",
      "train loss: 0.0003943183869247427\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1241:\n",
      "train loss: 0.00030555704566851197\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1242:\n",
      "train loss: 0.00026975845908810357\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1243:\n",
      "train loss: 0.0004244960928399334\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1244:\n",
      "train loss: 0.0003875695602589812\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1245:\n",
      "train loss: 0.00031237964051907306\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1246:\n",
      "train loss: 0.00027656960745780704\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1247:\n",
      "train loss: 0.0004177408963826826\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1248:\n",
      "train loss: 0.0003808112233725911\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1249:\n",
      "train loss: 0.0003187554918209181\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1250:\n",
      "train loss: 0.0002826217961324322\n",
      "Epoch 01252: reducing learning rate of group 0 to 4.1345e-05.\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1251:\n",
      "train loss: 0.0004121020174006476\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1252:\n",
      "train loss: 0.00037540967102794864\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1253:\n",
      "train loss: 0.0002891006653244973\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1254:\n",
      "train loss: 0.0002551533525830462\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1255:\n",
      "train loss: 0.00040433774562524064\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1256:\n",
      "train loss: 0.0003691908392840243\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1257:\n",
      "train loss: 0.0002953862445638216\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1258:\n",
      "train loss: 0.0002614258974210564\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1259:\n",
      "train loss: 0.00039809061759270986\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1260:\n",
      "train loss: 0.00036294771046915906\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1261:\n",
      "train loss: 0.00030131523088318266\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1262:\n",
      "train loss: 0.00026707585631393385\n",
      "Epoch 01264: reducing learning rate of group 0 to 3.9278e-05.\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1263:\n",
      "train loss: 0.00039277337720767093\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1264:\n",
      "train loss: 0.0003578451551366166\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1265:\n",
      "train loss: 0.00027314765078191843\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1266:\n",
      "train loss: 0.00024095395706495262\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1267:\n",
      "train loss: 0.0003854470815223375\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1268:\n",
      "train loss: 0.0003520181526297538\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1269:\n",
      "train loss: 0.0002790480359118753\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1270:\n",
      "train loss: 0.0002468433918564078\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1271:\n",
      "train loss: 0.00037955812760261966\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1272:\n",
      "train loss: 0.0003461402217594326\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1273:\n",
      "train loss: 0.0002846597788111475\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1274:\n",
      "train loss: 0.00025220245520245557\n",
      "Epoch 01276: reducing learning rate of group 0 to 3.7314e-05.\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1275:\n",
      "train loss: 0.00037447789612203965\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1276:\n",
      "train loss: 0.00034125661724266905\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1277:\n",
      "train loss: 0.00025796971108056366\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1278:\n",
      "train loss: 0.0002274438543340505\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1279:\n",
      "train loss: 0.00036746952874603566\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1280:\n",
      "train loss: 0.0003356901974555253\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1281:\n",
      "train loss: 0.00026361716878048905\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1282:\n",
      "train loss: 0.0002330791363153836\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1283:\n",
      "train loss: 0.00036181880994088186\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1284:\n",
      "train loss: 0.00033005310487072687\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1285:\n",
      "train loss: 0.00026902287745445886\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1286:\n",
      "train loss: 0.00023824799757471584\n",
      "Epoch 01288: reducing learning rate of group 0 to 3.5448e-05.\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1287:\n",
      "train loss: 0.0003568957886114486\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1288:\n",
      "train loss: 0.0003253188225394108\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1289:\n",
      "train loss: 0.00024378652260104446\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1290:\n",
      "train loss: 0.00021484166558436463\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1291:\n",
      "train loss: 0.0003501278695207331\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1292:\n",
      "train loss: 0.00031992719469117265\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1293:\n",
      "train loss: 0.00024926584069381055\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1294:\n",
      "train loss: 0.00022030591811971376\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1295:\n",
      "train loss: 0.0003446416673478726\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1296:\n",
      "train loss: 0.0003144606643006433\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1297:\n",
      "train loss: 0.0002545202647120666\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1298:\n",
      "train loss: 0.00022532676370371542\n",
      "Epoch 01300: reducing learning rate of group 0 to 3.3676e-05.\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1299:\n",
      "train loss: 0.00033985247535815797\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1300:\n",
      "train loss: 0.00030985753408965805\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1301:\n",
      "train loss: 0.00023066817398333\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1302:\n",
      "train loss: 0.0002032140201539805\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1303:\n",
      "train loss: 0.00033330284964629023\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1304:\n",
      "train loss: 0.00030461876957972193\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1305:\n",
      "train loss: 0.00023599676521185954\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1306:\n",
      "train loss: 0.00020852003040845848\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1307:\n",
      "train loss: 0.00032797701292243484\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1308:\n",
      "train loss: 0.0002993187332266766\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1309:\n",
      "train loss: 0.00024109300666132034\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1310:\n",
      "train loss: 0.0002133807864038632\n",
      "Epoch 01312: reducing learning rate of group 0 to 3.1992e-05.\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1311:\n",
      "train loss: 0.00032334543111954713\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1312:\n",
      "train loss: 0.00029487625781285295\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1313:\n",
      "train loss: 0.00021851421257193667\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1314:\n",
      "train loss: 0.00019245683960280945\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1315:\n",
      "train loss: 0.00031704309217699095\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1316:\n",
      "train loss: 0.0002898182874373899\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1317:\n",
      "train loss: 0.0002236606621569645\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1318:\n",
      "train loss: 0.00019757366083331733\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1319:\n",
      "train loss: 0.00031191264082497513\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1320:\n",
      "train loss: 0.0002847209217594997\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1321:\n",
      "train loss: 0.00022855767765715733\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1322:\n",
      "train loss: 0.00020223034052580544\n",
      "Epoch 01324: reducing learning rate of group 0 to 3.0393e-05.\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1323:\n",
      "train loss: 0.00030748801595529624\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1324:\n",
      "train loss: 0.0002804884408143769\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1325:\n",
      "train loss: 0.0002071288853125109\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1326:\n",
      "train loss: 0.00018238155626943849\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1327:\n",
      "train loss: 0.00030147137191954105\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1328:\n",
      "train loss: 0.0002756496615287951\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1329:\n",
      "train loss: 0.00021205671077754846\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1330:\n",
      "train loss: 0.00018727415602986578\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1331:\n",
      "train loss: 0.0002965706765247328\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1332:\n",
      "train loss: 0.00027078643877257314\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1333:\n",
      "train loss: 0.00021672152976964017\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1334:\n",
      "train loss: 0.00019169614623584753\n",
      "Epoch 01336: reducing learning rate of group 0 to 2.8873e-05.\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1335:\n",
      "train loss: 0.0002923835877594785\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1336:\n",
      "train loss: 0.00026679393031738365\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1337:\n",
      "train loss: 0.00019634749474465988\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1338:\n",
      "train loss: 0.0001728353485772007\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1339:\n",
      "train loss: 0.0002866665434241954\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1340:\n",
      "train loss: 0.0002621866985244388\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1341:\n",
      "train loss: 0.0002010459435288753\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1342:\n",
      "train loss: 0.0001774966406128296\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1343:\n",
      "train loss: 0.0002820003864722882\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1344:\n",
      "train loss: 0.00025756080978194353\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1345:\n",
      "train loss: 0.00020547779903517824\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1346:\n",
      "train loss: 0.00018168339943789777\n",
      "Epoch 01348: reducing learning rate of group 0 to 2.7429e-05.\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1347:\n",
      "train loss: 0.00027805073332941993\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1348:\n",
      "train loss: 0.00025380696274919404\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1349:\n",
      "train loss: 0.00018609527785009683\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1350:\n",
      "train loss: 0.00016375541001716534\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1351:\n",
      "train loss: 0.000272619417744423\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1352:\n",
      "train loss: 0.0002494179348588616\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1353:\n",
      "train loss: 0.00019058176702484376\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1354:\n",
      "train loss: 0.00016820340128429436\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1355:\n",
      "train loss: 0.00026816833456537833\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1356:\n",
      "train loss: 0.00024500875356238926\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1357:\n",
      "train loss: 0.00019479934668350783\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1358:\n",
      "train loss: 0.0001721733072780121\n",
      "Epoch 01360: reducing learning rate of group 0 to 2.6058e-05.\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1359:\n",
      "train loss: 0.0002644388631769754\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1360:\n",
      "train loss: 0.00024147744421255227\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1361:\n",
      "train loss: 0.0001763644402274875\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1362:\n",
      "train loss: 0.00015513865704971174\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1363:\n",
      "train loss: 0.00025926881866802643\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1364:\n",
      "train loss: 0.0002372836786207478\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1365:\n",
      "train loss: 0.0001806607487946286\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1366:\n",
      "train loss: 0.00015939512476926317\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1367:\n",
      "train loss: 0.0002550118072790781\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1368:\n",
      "train loss: 0.00023307108603010294\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1369:\n",
      "train loss: 0.00018468407810753334\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1370:\n",
      "train loss: 0.00016316540257418723\n",
      "Epoch 01372: reducing learning rate of group 0 to 2.4755e-05.\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1371:\n",
      "train loss: 0.0002514875558943676\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1372:\n",
      "train loss: 0.00022974895906963718\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1373:\n",
      "train loss: 0.00016714882807078804\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1374:\n",
      "train loss: 0.00014698076183351796\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1375:\n",
      "train loss: 0.0002465625049939405\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1376:\n",
      "train loss: 0.00022573637792590035\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1377:\n",
      "train loss: 0.00017127094228555569\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1378:\n",
      "train loss: 0.00015106046697608715\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1379:\n",
      "train loss: 0.00024248534433378245\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1380:\n",
      "train loss: 0.00022170668035351746\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1381:\n",
      "train loss: 0.00017510999092167374\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1382:\n",
      "train loss: 0.00015463970419005865\n",
      "Epoch 01384: reducing learning rate of group 0 to 2.3517e-05.\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1383:\n",
      "train loss: 0.0002391599808973913\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1384:\n",
      "train loss: 0.00021858889055077172\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1385:\n",
      "train loss: 0.00015842488853249408\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1386:\n",
      "train loss: 0.00013925944390505643\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1387:\n",
      "train loss: 0.00023446817046796612\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1388:\n",
      "train loss: 0.00021474736488884006\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1389:\n",
      "train loss: 0.00016238135494945805\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1390:\n",
      "train loss: 0.0001431710775694694\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1391:\n",
      "train loss: 0.0002305628190874546\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1392:\n",
      "train loss: 0.00021089292172104992\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1393:\n",
      "train loss: 0.0001660444008838484\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1394:\n",
      "train loss: 0.00014656581133319844\n",
      "Epoch 01396: reducing learning rate of group 0 to 2.2341e-05.\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1395:\n",
      "train loss: 0.00022743105006258667\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1396:\n",
      "train loss: 0.0002079751764818728\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1397:\n",
      "train loss: 0.00015015907737772667\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1398:\n",
      "train loss: 0.00013194465084669858\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1399:\n",
      "train loss: 0.000222963091774209\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1400:\n",
      "train loss: 0.0002042963931443465\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1401:\n",
      "train loss: 0.00015396110139337448\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1402:\n",
      "train loss: 0.00013569911669671902\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1403:\n",
      "train loss: 0.000219218237408347\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1404:\n",
      "train loss: 0.0002006059829267894\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1405:\n",
      "train loss: 0.00015745644664142032\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1406:\n",
      "train loss: 0.00013891680858513374\n",
      "Epoch 01408: reducing learning rate of group 0 to 2.1224e-05.\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1407:\n",
      "train loss: 0.00021627412762680238\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1408:\n",
      "train loss: 0.0001978832126564384\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1409:\n",
      "train loss: 0.00014232786383196308\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1410:\n",
      "train loss: 0.00012501583718733383\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1411:\n",
      "train loss: 0.0002120164481585922\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1412:\n",
      "train loss: 0.0001943548053050717\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1413:\n",
      "train loss: 0.000145986549556248\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1414:\n",
      "train loss: 0.00012862427606610613\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1415:\n",
      "train loss: 0.00020842158117352028\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1416:\n",
      "train loss: 0.0001908182941916962\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1417:\n",
      "train loss: 0.00014932497918902364\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1418:\n",
      "train loss: 0.00013167379526119984\n",
      "Epoch 01420: reducing learning rate of group 0 to 2.0163e-05.\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1419:\n",
      "train loss: 0.0002056576428184808\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1420:\n",
      "train loss: 0.00018828432655345343\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1421:\n",
      "train loss: 0.0001349097434913701\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1422:\n",
      "train loss: 0.00011845422382601254\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1423:\n",
      "train loss: 0.00020159966333933857\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1424:\n",
      "train loss: 0.00018489653633373674\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1425:\n",
      "train loss: 0.0001384376156467565\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1426:\n",
      "train loss: 0.0001219285586536614\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1427:\n",
      "train loss: 0.0001981427862354024\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1428:\n",
      "train loss: 0.0001815024281337205\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1429:\n",
      "train loss: 0.00014162734800796163\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1430:\n",
      "train loss: 0.00012481684475919443\n",
      "Epoch 01432: reducing learning rate of group 0 to 1.9155e-05.\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1431:\n",
      "train loss: 0.0001955539957794143\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1432:\n",
      "train loss: 0.00017915322442781984\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1433:\n",
      "train loss: 0.0001278862626624589\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1434:\n",
      "train loss: 0.00011224345408406123\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1435:\n",
      "train loss: 0.00019168331155380364\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1436:\n",
      "train loss: 0.00017589463167723334\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1437:\n",
      "train loss: 0.00013129341562593755\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1438:\n",
      "train loss: 0.00011559356643785726\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1439:\n",
      "train loss: 0.00018835542180850617\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1440:\n",
      "train loss: 0.00017263434222144534\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1441:\n",
      "train loss: 0.00013434351476629883\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1442:\n",
      "train loss: 0.000118327728449149\n",
      "Epoch 01444: reducing learning rate of group 0 to 1.8197e-05.\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1443:\n",
      "train loss: 0.0001859361477171698\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1444:\n",
      "train loss: 0.0001704656518029436\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1445:\n",
      "train loss: 0.00012123613340833833\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1446:\n",
      "train loss: 0.00010636471080951812\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1447:\n",
      "train loss: 0.0001822435427586827\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1448:\n",
      "train loss: 0.00016732722954453328\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1449:\n",
      "train loss: 0.00012453453962260432\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1450:\n",
      "train loss: 0.00010960201678179465\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1451:\n",
      "train loss: 0.0001790331836552561\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1452:\n",
      "train loss: 0.00016418987927014844\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1453:\n",
      "train loss: 0.00012745253666809035\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1454:\n",
      "train loss: 0.00011218819041403483\n",
      "Epoch 01456: reducing learning rate of group 0 to 1.7287e-05.\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1455:\n",
      "train loss: 0.00017677931563699994\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1456:\n",
      "train loss: 0.00016219844093024176\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1457:\n",
      "train loss: 0.00011494347401646948\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1458:\n",
      "train loss: 0.00010080421668785994\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1459:\n",
      "train loss: 0.0001732526661446832\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1460:\n",
      "train loss: 0.00015916866739726696\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1461:\n",
      "train loss: 0.00011814334056896878\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1462:\n",
      "train loss: 0.00010393845747426198\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1463:\n",
      "train loss: 0.0001701512725685891\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1464:\n",
      "train loss: 0.00015614616914828706\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1465:\n",
      "train loss: 0.00012093751418954114\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1466:\n",
      "train loss: 0.00010638285068837665\n",
      "Epoch 01468: reducing learning rate of group 0 to 1.6423e-05.\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1467:\n",
      "train loss: 0.00016805828765966866\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1468:\n",
      "train loss: 0.00015432890207066417\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1469:\n",
      "train loss: 0.00010898944580028052\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1470:\n",
      "train loss: 9.554520896159164e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1471:\n",
      "train loss: 0.00016468926294919379\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1472:\n",
      "train loss: 0.0001513990879409779\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1473:\n",
      "train loss: 0.00011210242132539896\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1474:\n",
      "train loss: 9.858738244444067e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1475:\n",
      "train loss: 0.00016168601655494347\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1476:\n",
      "train loss: 0.000148481327805817\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1477:\n",
      "train loss: 0.00011477933059759072\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1478:\n",
      "train loss: 0.00010089507469034506\n",
      "Epoch 01480: reducing learning rate of group 0 to 1.5602e-05.\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1479:\n",
      "train loss: 0.00015975098792924005\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1480:\n",
      "train loss: 0.00014683594462070671\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1481:\n",
      "train loss: 0.00010335997663289437\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1482:\n",
      "train loss: 9.057556285723313e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1483:\n",
      "train loss: 0.00015652816526232148\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1484:\n",
      "train loss: 0.00014399492433289187\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1485:\n",
      "train loss: 0.00010639580688891263\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1486:\n",
      "train loss: 9.353469942968576e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1487:\n",
      "train loss: 0.00015361556525888878\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1488:\n",
      "train loss: 0.00014117489496872962\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1489:\n",
      "train loss: 0.00010896238598030022\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1490:\n",
      "train loss: 9.571063797975816e-05\n",
      "Epoch 01492: reducing learning rate of group 0 to 1.4822e-05.\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1491:\n",
      "train loss: 0.0001518352704102182\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1492:\n",
      "train loss: 0.00013969944867287638\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1493:\n",
      "train loss: 9.803719494650395e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1494:\n",
      "train loss: 8.587928236452477e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1495:\n",
      "train loss: 0.00014875118954498827\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1496:\n",
      "train loss: 0.000136938879993603\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1497:\n",
      "train loss: 0.00010100713177377707\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1498:\n",
      "train loss: 8.87659187247132e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1499:\n",
      "train loss: 0.00014591911199364116\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1500:\n",
      "train loss: 0.00013420720570002593\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1501:\n",
      "train loss: 0.00010346904358295549\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1502:\n",
      "train loss: 9.081442909533082e-05\n",
      "Epoch 01504: reducing learning rate of group 0 to 1.4081e-05.\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1503:\n",
      "train loss: 0.0001442913205055403\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1504:\n",
      "train loss: 0.0001328996866708361\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1505:\n",
      "train loss: 9.300904952224702e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1506:\n",
      "train loss: 8.144611497547394e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1507:\n",
      "train loss: 0.00014133517867876994\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1508:\n",
      "train loss: 0.00013020885559019674\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1509:\n",
      "train loss: 9.592205896281291e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1510:\n",
      "train loss: 8.426824558541779e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1511:\n",
      "train loss: 0.00013857758970998765\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1512:\n",
      "train loss: 0.00012755996606170585\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1513:\n",
      "train loss: 9.828460809858113e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1514:\n",
      "train loss: 8.619305935217788e-05\n",
      "Epoch 01516: reducing learning rate of group 0 to 1.3377e-05.\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1515:\n",
      "train loss: 0.00013710002908838586\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1516:\n",
      "train loss: 0.0001264188973917463\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1517:\n",
      "train loss: 8.825824417929482e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1518:\n",
      "train loss: 7.726046247169235e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1519:\n",
      "train loss: 0.00013426528222268362\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1520:\n",
      "train loss: 0.0001237900773507738\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1521:\n",
      "train loss: 9.112470038868955e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1522:\n",
      "train loss: 8.00277754002922e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1523:\n",
      "train loss: 0.00013157309786381718\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1524:\n",
      "train loss: 0.00012121552215781454\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1525:\n",
      "train loss: 9.339266088958305e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1526:\n",
      "train loss: 8.183290801585069e-05\n",
      "Epoch 01528: reducing learning rate of group 0 to 1.2708e-05.\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1527:\n",
      "train loss: 0.00013024339946021075\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1528:\n",
      "train loss: 0.0001202380175998408\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1529:\n",
      "train loss: 8.377483317042469e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1530:\n",
      "train loss: 7.331396404185105e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1531:\n",
      "train loss: 0.00012752014431589238\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1532:\n",
      "train loss: 0.00011766159371734534\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1533:\n",
      "train loss: 8.660197117406708e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1534:\n",
      "train loss: 7.603264401721946e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1535:\n",
      "train loss: 0.00012488948541722268\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1536:\n",
      "train loss: 0.00011515774833423702\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1537:\n",
      "train loss: 8.877881383254024e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1538:\n",
      "train loss: 7.772105768261371e-05\n",
      "Epoch 01540: reducing learning rate of group 0 to 1.2072e-05.\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1539:\n",
      "train loss: 0.00012370523434795884\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1540:\n",
      "train loss: 0.00011434120212082043\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1541:\n",
      "train loss: 7.954199014885667e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1542:\n",
      "train loss: 6.959137165795798e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1543:\n",
      "train loss: 0.00012108800609940683\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1544:\n",
      "train loss: 0.0001118107553818928\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1545:\n",
      "train loss: 8.233822781391759e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1546:\n",
      "train loss: 7.226948962800597e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1547:\n",
      "train loss: 0.00011851134696224743\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1548:\n",
      "train loss: 0.0001093702726501104\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1549:\n",
      "train loss: 8.442814301056297e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1550:\n",
      "train loss: 7.38459150788654e-05\n",
      "Epoch 01552: reducing learning rate of group 0 to 1.1469e-05.\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1551:\n",
      "train loss: 0.00011746833451973149\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1552:\n",
      "train loss: 0.00010870882642364943\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1553:\n",
      "train loss: 7.55522846425124e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1554:\n",
      "train loss: 6.60864013211122e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1555:\n",
      "train loss: 0.00011494907712896463\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1556:\n",
      "train loss: 0.0001062174929418395\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1557:\n",
      "train loss: 7.832110225724748e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1558:\n",
      "train loss: 6.872691693842001e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1559:\n",
      "train loss: 0.00011242561849643035\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1560:\n",
      "train loss: 0.00010383918201359626\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1561:\n",
      "train loss: 8.032593512032422e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1562:\n",
      "train loss: 7.019480086778012e-05\n",
      "Epoch 01564: reducing learning rate of group 0 to 1.0895e-05.\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1563:\n",
      "train loss: 0.00011151894330629888\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1564:\n",
      "train loss: 0.00010332607263588318\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1565:\n",
      "train loss: 7.178972628043294e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1566:\n",
      "train loss: 6.278456545286059e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1567:\n",
      "train loss: 0.00010909397311296227\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1568:\n",
      "train loss: 0.00010087050754052065\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1569:\n",
      "train loss: 7.453521525032478e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1570:\n",
      "train loss: 6.539235569193913e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1571:\n",
      "train loss: 0.00010661858562052874\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1572:\n",
      "train loss: 9.854827326348892e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1573:\n",
      "train loss: 7.645941856295259e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1574:\n",
      "train loss: 6.675906521763903e-05\n",
      "Epoch 01576: reducing learning rate of group 0 to 1.0351e-05.\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1575:\n",
      "train loss: 0.00010583922003555915\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1576:\n",
      "train loss: 9.817130060674346e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1577:\n",
      "train loss: 6.824982267296095e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1578:\n",
      "train loss: 5.968174561439528e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1579:\n",
      "train loss: 0.00010350432857115053\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1580:\n",
      "train loss: 9.575054694741806e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1581:\n",
      "train loss: 7.096826381607118e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1582:\n",
      "train loss: 6.22545199232607e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1583:\n",
      "train loss: 0.00010108002088805767\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1584:\n",
      "train loss: 9.348550700536304e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1585:\n",
      "train loss: 7.281342034327123e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1586:\n",
      "train loss: 6.352666606944712e-05\n",
      "Epoch 01588: reducing learning rate of group 0 to 9.8330e-06.\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1587:\n",
      "train loss: 0.00010041642586993317\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1588:\n",
      "train loss: 9.322890132696221e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1589:\n",
      "train loss: 6.49188500375017e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1590:\n",
      "train loss: 5.6765677917569035e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1591:\n",
      "train loss: 9.817125464809858e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1592:\n",
      "train loss: 9.084602486802977e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1593:\n",
      "train loss: 6.76059163068425e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1594:\n",
      "train loss: 5.9302546029466144e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1595:\n",
      "train loss: 9.579629785437443e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1596:\n",
      "train loss: 8.863314130841689e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1597:\n",
      "train loss: 6.937862941901962e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1598:\n",
      "train loss: 6.0493377051791934e-05\n",
      "Epoch 01600: reducing learning rate of group 0 to 9.3414e-06.\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1599:\n",
      "train loss: 9.522995867591401e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1600:\n",
      "train loss: 8.847344499309264e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1601:\n",
      "train loss: 6.179611636507727e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1602:\n",
      "train loss: 5.403517769821101e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1603:\n",
      "train loss: 9.307639476402514e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1604:\n",
      "train loss: 8.613705185342309e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1605:\n",
      "train loss: 6.44372558655319e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1606:\n",
      "train loss: 5.652716814637651e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1607:\n",
      "train loss: 9.075682899061717e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1608:\n",
      "train loss: 8.39777804840875e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1609:\n",
      "train loss: 6.614231829967699e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1610:\n",
      "train loss: 5.765097318042944e-05\n",
      "Epoch 01612: reducing learning rate of group 0 to 8.8743e-06.\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1611:\n",
      "train loss: 9.02636325140762e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1612:\n",
      "train loss: 8.388377191997321e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1613:\n",
      "train loss: 5.887494009715506e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1614:\n",
      "train loss: 5.1485334687231284e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1615:\n",
      "train loss: 8.820492643787571e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1616:\n",
      "train loss: 8.1605350753264e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1617:\n",
      "train loss: 6.14547243476044e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1618:\n",
      "train loss: 5.392477875824505e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1619:\n",
      "train loss: 8.594134164072795e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1620:\n",
      "train loss: 7.949301442577808e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1621:\n",
      "train loss: 6.310537336497944e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1622:\n",
      "train loss: 5.500641068339968e-05\n",
      "Epoch 01624: reducing learning rate of group 0 to 8.4306e-06.\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1623:\n",
      "train loss: 8.548611185854439e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1624:\n",
      "train loss: 7.942257127193674e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1625:\n",
      "train loss: 5.61660779121696e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1626:\n",
      "train loss: 4.912647704382614e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1627:\n",
      "train loss: 8.352675555651979e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1628:\n",
      "train loss: 7.721628913412761e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1629:\n",
      "train loss: 5.866356408639368e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1630:\n",
      "train loss: 5.1503732442733944e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1631:\n",
      "train loss: 8.132049741200353e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1632:\n",
      "train loss: 7.514394256032906e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1633:\n",
      "train loss: 6.027640609779356e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1634:\n",
      "train loss: 5.257367054180994e-05\n",
      "Epoch 01636: reducing learning rate of group 0 to 8.0091e-06.\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1635:\n",
      "train loss: 8.085750768039789e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1636:\n",
      "train loss: 7.504044049646806e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1637:\n",
      "train loss: 5.36921894461671e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1638:\n",
      "train loss: 4.698523648657499e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1639:\n",
      "train loss: 7.89917090132671e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1640:\n",
      "train loss: 7.29103079799572e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1641:\n",
      "train loss: 5.609685131169742e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1642:\n",
      "train loss: 4.930185841075366e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1643:\n",
      "train loss: 7.682961329411292e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1644:\n",
      "train loss: 7.085199491942975e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1645:\n",
      "train loss: 5.770810001703099e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1646:\n",
      "train loss: 5.041323623117105e-05\n",
      "Epoch 01648: reducing learning rate of group 0 to 7.6086e-06.\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1647:\n",
      "train loss: 7.629051383133707e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1648:\n",
      "train loss: 7.063706606337166e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1649:\n",
      "train loss: 5.152486098383975e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1650:\n",
      "train loss: 4.513619782917035e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1651:\n",
      "train loss: 7.449681760694008e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1652:\n",
      "train loss: 6.856390620081751e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1653:\n",
      "train loss: 5.385180709821257e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1654:\n",
      "train loss: 4.7425037688192994e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1655:\n",
      "train loss: 7.233517137183414e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1656:\n",
      "train loss: 6.64638562555675e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1657:\n",
      "train loss: 5.552561719342829e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1658:\n",
      "train loss: 4.8657731587624274e-05\n",
      "Epoch 01660: reducing learning rate of group 0 to 7.2282e-06.\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1659:\n",
      "train loss: 7.162518032961531e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1660:\n",
      "train loss: 6.602819477354782e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1661:\n",
      "train loss: 4.982412528139315e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1662:\n",
      "train loss: 4.375188484034553e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1663:\n",
      "train loss: 6.983948017796082e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1664:\n",
      "train loss: 6.394431971440825e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1665:\n",
      "train loss: 5.2143405626655317e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1666:\n",
      "train loss: 4.610043586174214e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1667:\n",
      "train loss: 6.758690464482005e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1668:\n",
      "train loss: 6.169904526731715e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1669:\n",
      "train loss: 5.3995503572756066e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1670:\n",
      "train loss: 4.758540837483121e-05\n",
      "Epoch 01672: reducing learning rate of group 0 to 6.8668e-06.\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1671:\n",
      "train loss: 6.657788896585198e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1672:\n",
      "train loss: 6.091819661792607e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1673:\n",
      "train loss: 4.887607726089187e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1674:\n",
      "train loss: 4.312551255926301e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1675:\n",
      "train loss: 6.473660080869571e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1676:\n",
      "train loss: 5.8760175117380076e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1677:\n",
      "train loss: 5.127890974862656e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1678:\n",
      "train loss: 4.564362225969968e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1679:\n",
      "train loss: 6.230867834717444e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1680:\n",
      "train loss: 5.6312882776292006e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1681:\n",
      "train loss: 5.3376070863334764e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1682:\n",
      "train loss: 4.7428582334361966e-05\n",
      "Epoch 01684: reducing learning rate of group 0 to 6.5234e-06.\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1683:\n",
      "train loss: 6.100397113087298e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1684:\n",
      "train loss: 5.524651635855217e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1685:\n",
      "train loss: 4.877683931518427e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1686:\n",
      "train loss: 4.33230808573245e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1687:\n",
      "train loss: 5.925143071323957e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1688:\n",
      "train loss: 5.32224153127438e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1689:\n",
      "train loss: 5.10903587788704e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1690:\n",
      "train loss: 4.581074328544984e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1691:\n",
      "train loss: 5.689577596818852e-05\n",
      "Epoch 01693: reducing learning rate of group 0 to 6.1973e-06.\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1692:\n",
      "train loss: 5.090116994418656e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1693:\n",
      "train loss: 5.310890599394472e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1694:\n",
      "train loss: 4.767514292998912e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1695:\n",
      "train loss: 5.0741298447503115e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1696:\n",
      "train loss: 4.577425261093268e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1697:\n",
      "train loss: 5.2112917889359136e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1698:\n",
      "train loss: 4.587817681470326e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1699:\n",
      "train loss: 5.319198078725889e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1700:\n",
      "train loss: 4.8480583849184667e-05\n",
      "Epoch 01702: reducing learning rate of group 0 to 5.8874e-06.\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1701:\n",
      "train loss: 4.952003470154421e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1702:\n",
      "train loss: 4.364814508825452e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1703:\n",
      "train loss: 5.008542834504459e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1704:\n",
      "train loss: 4.51665636615184e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1705:\n",
      "train loss: 4.825796726024936e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1706:\n",
      "train loss: 4.287998478993153e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1707:\n",
      "train loss: 5.0768515610737834e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1708:\n",
      "train loss: 4.563372136899596e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1709:\n",
      "train loss: 4.8094980952943506e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1710:\n",
      "train loss: 4.299444949350399e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1711:\n",
      "train loss: 5.0497042266758915e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1712:\n",
      "train loss: 4.516101605300436e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1713:\n",
      "train loss: 4.876853015467527e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1714:\n",
      "train loss: 4.378161504266867e-05\n",
      "Epoch 01716: reducing learning rate of group 0 to 5.5930e-06.\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1715:\n",
      "train loss: 4.977034214721279e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1716:\n",
      "train loss: 4.446994622353443e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1717:\n",
      "train loss: 4.4841698483682304e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1718:\n",
      "train loss: 4.0254286649667115e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1719:\n",
      "train loss: 4.8388440740088236e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1720:\n",
      "train loss: 4.30351857974956e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1721:\n",
      "train loss: 4.6569752017047505e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1722:\n",
      "train loss: 4.2113494922246586e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1723:\n",
      "train loss: 4.666576561925293e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1724:\n",
      "train loss: 4.150135775972237e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1725:\n",
      "train loss: 4.7877459391175785e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1726:\n",
      "train loss: 4.3146917954903045e-05\n",
      "Epoch 01728: reducing learning rate of group 0 to 5.3134e-06.\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1727:\n",
      "train loss: 4.600760209582745e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1728:\n",
      "train loss: 4.119199394880576e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1729:\n",
      "train loss: 4.35264589522017e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1730:\n",
      "train loss: 3.894186479927953e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1731:\n",
      "train loss: 4.573196507473251e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1732:\n",
      "train loss: 4.103681289344926e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1733:\n",
      "train loss: 4.3872603019493764e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1734:\n",
      "train loss: 3.9442366624125786e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1735:\n",
      "train loss: 4.517622529708719e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1736:\n",
      "train loss: 4.045670480296306e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1737:\n",
      "train loss: 4.4479205531529986e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1738:\n",
      "train loss: 4.0055900747524307e-05\n",
      "Epoch 01740: reducing learning rate of group 0 to 5.0477e-06.\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1739:\n",
      "train loss: 4.462286252751319e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1740:\n",
      "train loss: 3.998053889379136e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1741:\n",
      "train loss: 4.068889823368219e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1742:\n",
      "train loss: 3.6518540621383664e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1743:\n",
      "train loss: 4.385297860030572e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1744:\n",
      "train loss: 3.934911382357672e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1745:\n",
      "train loss: 4.14237645584855e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1746:\n",
      "train loss: 3.733063103052359e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1747:\n",
      "train loss: 4.3029525376779554e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1748:\n",
      "train loss: 3.854677352714412e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1749:\n",
      "train loss: 4.2197206413727056e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1750:\n",
      "train loss: 3.807216181117141e-05\n",
      "Epoch 01752: reducing learning rate of group 0 to 4.7953e-06.\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1751:\n",
      "train loss: 4.2351516715629914e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1752:\n",
      "train loss: 3.794905837403702e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1753:\n",
      "train loss: 3.8703936240564505e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1754:\n",
      "train loss: 3.4765840663638904e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1755:\n",
      "train loss: 4.1630758195801715e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1756:\n",
      "train loss: 3.743753362281085e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1757:\n",
      "train loss: 3.923069826786259e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1758:\n",
      "train loss: 3.530711754800329e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1759:\n",
      "train loss: 4.10957149259156e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1760:\n",
      "train loss: 3.6928070441181665e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1761:\n",
      "train loss: 3.9709992092138756e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1762:\n",
      "train loss: 3.576473611351867e-05\n",
      "Epoch 01764: reducing learning rate of group 0 to 4.5555e-06.\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1763:\n",
      "train loss: 4.066631962656924e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1764:\n",
      "train loss: 3.6540313356811594e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1765:\n",
      "train loss: 3.623184477254085e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1766:\n",
      "train loss: 3.247283554404787e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1767:\n",
      "train loss: 4.013145475990633e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1768:\n",
      "train loss: 3.6211949358159884e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1769:\n",
      "train loss: 3.655616105634823e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1770:\n",
      "train loss: 3.2799647432189256e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1771:\n",
      "train loss: 3.980547766563661e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1772:\n",
      "train loss: 3.590150503340558e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1773:\n",
      "train loss: 3.684375212068517e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1774:\n",
      "train loss: 3.307456570622321e-05\n",
      "Epoch 01776: reducing learning rate of group 0 to 4.3278e-06.\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1775:\n",
      "train loss: 3.95405236693939e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1776:\n",
      "train loss: 3.5658589996868096e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1777:\n",
      "train loss: 3.342450791453922e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1778:\n",
      "train loss: 2.9834298822528827e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1779:\n",
      "train loss: 3.914850079574942e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1780:\n",
      "train loss: 3.54682560021983e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1781:\n",
      "train loss: 3.3599861498278776e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1782:\n",
      "train loss: 3.0003519091007812e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1783:\n",
      "train loss: 3.898110909453552e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1784:\n",
      "train loss: 3.531398945190759e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1785:\n",
      "train loss: 3.373301890992306e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1786:\n",
      "train loss: 3.0125691474798058e-05\n",
      "Epoch 01788: reducing learning rate of group 0 to 4.1114e-06.\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1787:\n",
      "train loss: 3.886318430634869e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1788:\n",
      "train loss: 3.521035998041217e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1789:\n",
      "train loss: 3.036379845432914e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1790:\n",
      "train loss: 2.692743705039516e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1791:\n",
      "train loss: 3.8612730780968144e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1792:\n",
      "train loss: 3.515211588596321e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1793:\n",
      "train loss: 3.0405041012536135e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1794:\n",
      "train loss: 2.6960786298368575e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1795:\n",
      "train loss: 3.857995342449993e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1796:\n",
      "train loss: 3.512863140214429e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1797:\n",
      "train loss: 3.0411963591054507e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1798:\n",
      "train loss: 2.696043799638589e-05\n",
      "Epoch 01800: reducing learning rate of group 0 to 3.9058e-06.\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1799:\n",
      "train loss: 3.85795140459603e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1800:\n",
      "train loss: 3.5135633335751005e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1801:\n",
      "train loss: 2.7114036935899375e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1802:\n",
      "train loss: 2.3830229652415188e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1803:\n",
      "train loss: 3.842929166520227e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1804:\n",
      "train loss: 3.516185898688402e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1805:\n",
      "train loss: 2.7076891164756924e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1806:\n",
      "train loss: 2.3790867440946243e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1807:\n",
      "train loss: 3.846239024030552e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1808:\n",
      "train loss: 3.519620075026668e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1809:\n",
      "train loss: 2.7034627802034448e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1810:\n",
      "train loss: 2.3749387029470188e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1811:\n",
      "train loss: 3.849420090410739e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1812:\n",
      "train loss: 3.522585709847074e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1813:\n",
      "train loss: 2.7000356418496173e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1814:\n",
      "train loss: 2.3719208611295536e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1815:\n",
      "train loss: 3.8510974140590305e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1816:\n",
      "train loss: 3.523681704319037e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1817:\n",
      "train loss: 2.698829413787068e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1818:\n",
      "train loss: 2.3714774586471854e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1819:\n",
      "train loss: 3.849800193281125e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1820:\n",
      "train loss: 3.521414791839732e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1821:\n",
      "train loss: 2.7013550352032648e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1822:\n",
      "train loss: 2.37513586719978e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1823:\n",
      "train loss: 3.843984432976114e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1824:\n",
      "train loss: 3.514225793201364e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1825:\n",
      "train loss: 2.7091823946097974e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1826:\n",
      "train loss: 2.3844684794849607e-05\n",
      "Epoch 01828: reducing learning rate of group 0 to 3.7105e-06.\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1827:\n",
      "train loss: 3.8320854655051055e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1828:\n",
      "train loss: 3.500555119176928e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1829:\n",
      "train loss: 2.412837302487747e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1830:\n",
      "train loss: 2.1072422058147597e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1831:\n",
      "train loss: 3.793206220925967e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1832:\n",
      "train loss: 3.473966312598208e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1833:\n",
      "train loss: 2.4426967509024878e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1834:\n",
      "train loss: 2.1413591344375485e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1835:\n",
      "train loss: 3.753372826225082e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1836:\n",
      "train loss: 3.429573020443248e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1837:\n",
      "train loss: 2.4905330901452597e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1838:\n",
      "train loss: 2.193609045030261e-05\n",
      "Epoch 01840: reducing learning rate of group 0 to 3.5250e-06.\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1839:\n",
      "train loss: 3.6952142689788114e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1840:\n",
      "train loss: 3.366598124585645e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1841:\n",
      "train loss: 2.261494985038587e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1842:\n",
      "train loss: 1.9866053194587386e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1843:\n",
      "train loss: 3.5957744622648055e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1844:\n",
      "train loss: 3.2731320419686087e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1845:\n",
      "train loss: 2.3634842873150413e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1846:\n",
      "train loss: 2.0973679961523658e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1847:\n",
      "train loss: 3.475382258816122e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1848:\n",
      "train loss: 3.144618614029876e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1849:\n",
      "train loss: 2.4981467344987925e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1850:\n",
      "train loss: 2.2375739520027855e-05\n",
      "Epoch 01852: reducing learning rate of group 0 to 3.3487e-06.\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1851:\n",
      "train loss: 3.3300391840491596e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1852:\n",
      "train loss: 2.994967798254704e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1853:\n",
      "train loss: 2.369133486112847e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1854:\n",
      "train loss: 2.126846646646202e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1855:\n",
      "train loss: 3.15398087969228e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1856:\n",
      "train loss: 2.827639571290422e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1857:\n",
      "train loss: 2.5421671538130706e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1858:\n",
      "train loss: 2.3024314234261396e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1859:\n",
      "train loss: 2.9797435648924832e-05\n",
      "Epoch 01861: reducing learning rate of group 0 to 3.1813e-06.\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1860:\n",
      "train loss: 2.6553701445965265e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1861:\n",
      "train loss: 2.7103713684632775e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1862:\n",
      "train loss: 2.4750046517328243e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1863:\n",
      "train loss: 2.5569258629397174e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1864:\n",
      "train loss: 2.263801268578708e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1865:\n",
      "train loss: 2.815019555593187e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1866:\n",
      "train loss: 2.5599897179030937e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1867:\n",
      "train loss: 2.4916138948547573e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1868:\n",
      "train loss: 2.2171413846768515e-05\n",
      "Epoch 01870: reducing learning rate of group 0 to 3.0222e-06.\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1869:\n",
      "train loss: 2.8440799219915e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1870:\n",
      "train loss: 2.5727619965594432e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1871:\n",
      "train loss: 2.24197035354884e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1872:\n",
      "train loss: 1.9982557487580035e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1873:\n",
      "train loss: 2.7906590191822917e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1874:\n",
      "train loss: 2.5145695021427852e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1875:\n",
      "train loss: 2.3162455152394472e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1876:\n",
      "train loss: 2.0850211226868247e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1877:\n",
      "train loss: 2.6961582469014405e-05\n",
      "Epoch 01879: reducing learning rate of group 0 to 2.8711e-06.\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1878:\n",
      "train loss: 2.4142297039340807e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1879:\n",
      "train loss: 2.420304030647583e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1880:\n",
      "train loss: 2.200594852689525e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1881:\n",
      "train loss: 2.3475226259723337e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1882:\n",
      "train loss: 2.087330849064122e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1883:\n",
      "train loss: 2.495274047038482e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1884:\n",
      "train loss: 2.2645488113626396e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1885:\n",
      "train loss: 2.294934066158359e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1886:\n",
      "train loss: 2.0457516832034632e-05\n",
      "Epoch 01888: reducing learning rate of group 0 to 2.7276e-06.\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1887:\n",
      "train loss: 2.5268275025245612e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1888:\n",
      "train loss: 2.2871446191204327e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1889:\n",
      "train loss: 2.053278731683373e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1890:\n",
      "train loss: 1.826613918353748e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1891:\n",
      "train loss: 2.506009144425779e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1892:\n",
      "train loss: 2.2676554555068266e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1893:\n",
      "train loss: 2.0823317863207977e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1894:\n",
      "train loss: 1.86367772371292e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1895:\n",
      "train loss: 2.463266186070613e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1896:\n",
      "train loss: 2.220620619713452e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1897:\n",
      "train loss: 2.1325742765637797e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1898:\n",
      "train loss: 1.9161605718524564e-05\n",
      "Epoch 01900: reducing learning rate of group 0 to 2.5912e-06.\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1899:\n",
      "train loss: 2.410153874727129e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1900:\n",
      "train loss: 2.167655299497153e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1901:\n",
      "train loss: 1.967778810166292e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1902:\n",
      "train loss: 1.7628017563542378e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1903:\n",
      "train loss: 2.345896349963741e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1904:\n",
      "train loss: 2.114328611266543e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1905:\n",
      "train loss: 2.0218963791874747e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1906:\n",
      "train loss: 1.8171238354157815e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1907:\n",
      "train loss: 2.2924659416672802e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1908:\n",
      "train loss: 2.062321829014635e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1909:\n",
      "train loss: 2.072140779438851e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1910:\n",
      "train loss: 1.8655036284829616e-05\n",
      "Epoch 01912: reducing learning rate of group 0 to 2.4616e-06.\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1911:\n",
      "train loss: 2.2464582359107952e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1912:\n",
      "train loss: 2.0189451232992746e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1913:\n",
      "train loss: 1.9065478373790502e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1914:\n",
      "train loss: 1.708761529337445e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1915:\n",
      "train loss: 2.1983544701212205e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1916:\n",
      "train loss: 1.9828939863494134e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1917:\n",
      "train loss: 1.941851720186571e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1918:\n",
      "train loss: 1.743293260507245e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1919:\n",
      "train loss: 2.1649288246651075e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1920:\n",
      "train loss: 1.9508778513711817e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1921:\n",
      "train loss: 1.9723070550079827e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1922:\n",
      "train loss: 1.772349734471533e-05\n",
      "Epoch 01924: reducing learning rate of group 0 to 2.3386e-06.\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1923:\n",
      "train loss: 2.1373583381218052e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1924:\n",
      "train loss: 1.924992136227914e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1925:\n",
      "train loss: 1.8005709887437104e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1926:\n",
      "train loss: 1.6097755022182387e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1927:\n",
      "train loss: 2.104678622848568e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1928:\n",
      "train loss: 1.903199168399261e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1929:\n",
      "train loss: 1.8219881993460698e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1930:\n",
      "train loss: 1.63090837315975e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1931:\n",
      "train loss: 2.083925407967505e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1932:\n",
      "train loss: 1.8831034320692762e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1933:\n",
      "train loss: 1.8412510210975427e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1934:\n",
      "train loss: 1.6495168179460583e-05\n",
      "Epoch 01936: reducing learning rate of group 0 to 2.2216e-06.\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1935:\n",
      "train loss: 2.0659449792498105e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1936:\n",
      "train loss: 1.8659679965478988e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1937:\n",
      "train loss: 1.6713682879757485e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1938:\n",
      "train loss: 1.4889201374350017e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1939:\n",
      "train loss: 2.0405700717648783e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1940:\n",
      "train loss: 1.8505237612971887e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1941:\n",
      "train loss: 1.6867088549197603e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1942:\n",
      "train loss: 1.504270162856004e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1943:\n",
      "train loss: 2.0252102248205967e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1944:\n",
      "train loss: 1.8354197478586805e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1945:\n",
      "train loss: 1.7013550860256483e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1946:\n",
      "train loss: 1.518626819951981e-05\n",
      "Epoch 01948: reducing learning rate of group 0 to 2.1105e-06.\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1947:\n",
      "train loss: 2.011068376732492e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1948:\n",
      "train loss: 1.8217124927960946e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1949:\n",
      "train loss: 1.5377418321465485e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1950:\n",
      "train loss: 1.3641062943454458e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1951:\n",
      "train loss: 1.988687894333603e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1952:\n",
      "train loss: 1.8085578387489403e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1953:\n",
      "train loss: 1.5509331854502635e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1954:\n",
      "train loss: 1.3774556356600166e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1955:\n",
      "train loss: 1.9751272805088142e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1956:\n",
      "train loss: 1.7950399533104764e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1957:\n",
      "train loss: 1.5641949479318924e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 1958:\n",
      "train loss: 1.3906228150950732e-05\n",
      "Epoch 01960: reducing learning rate of group 0 to 2.0050e-06.\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1959:\n",
      "train loss: 1.9619503499201936e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1960:\n",
      "train loss: 1.7820694062238857e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1961:\n",
      "train loss: 1.40890949842665e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1962:\n",
      "train loss: 1.2441532467709288e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1963:\n",
      "train loss: 1.9401731099822185e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1964:\n",
      "train loss: 1.7688677440411124e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1965:\n",
      "train loss: 1.4223009871584333e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1966:\n",
      "train loss: 1.2578591834387883e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1967:\n",
      "train loss: 1.9260654231617992e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1968:\n",
      "train loss: 1.754612691373753e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1969:\n",
      "train loss: 1.4364770359465548e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 1970:\n",
      "train loss: 1.272114329293252e-05\n",
      "Epoch 01972: reducing learning rate of group 0 to 1.9048e-06.\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1971:\n",
      "train loss: 1.9116038557748087e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1972:\n",
      "train loss: 1.7401638923870803e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1973:\n",
      "train loss: 1.2912494071306848e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1974:\n",
      "train loss: 1.1354650402576542e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1975:\n",
      "train loss: 1.888133422632318e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1976:\n",
      "train loss: 1.7245627372155777e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1977:\n",
      "train loss: 1.3072882629650626e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1978:\n",
      "train loss: 1.152067209449283e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1979:\n",
      "train loss: 1.870852317678892e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1980:\n",
      "train loss: 1.7068801758916602e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1981:\n",
      "train loss: 1.3251151828326662e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 1982:\n",
      "train loss: 1.170196153731865e-05\n",
      "Epoch 01984: reducing learning rate of group 0 to 1.8095e-06.\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1983:\n",
      "train loss: 1.8522696361356873e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1984:\n",
      "train loss: 1.6880823794248984e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1985:\n",
      "train loss: 1.1923990585283809e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1986:\n",
      "train loss: 1.0459178089112682e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1987:\n",
      "train loss: 1.8239676564452895e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1988:\n",
      "train loss: 1.6668187412491867e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1989:\n",
      "train loss: 1.2144973761407062e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1990:\n",
      "train loss: 1.068967179151696e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1991:\n",
      "train loss: 1.799834657946539e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1992:\n",
      "train loss: 1.641934412724322e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1993:\n",
      "train loss: 1.239811146676725e-05\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 1994:\n",
      "train loss: 1.0948562732340675e-05\n",
      "Epoch 01996: reducing learning rate of group 0 to 1.7191e-06.\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 1995:\n",
      "train loss: 1.7732094127595088e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 1996:\n",
      "train loss: 1.6148498404581914e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 1997:\n",
      "train loss: 1.1231127536825515e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 1998:\n",
      "train loss: 9.864939814462964e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 1999:\n",
      "train loss: 1.736048543459215e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2000:\n",
      "train loss: 1.5838428749101316e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2001:\n",
      "train loss: 1.1554340899299527e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2002:\n",
      "train loss: 1.0201766246041558e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2003:\n",
      "train loss: 1.7009367881333516e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2004:\n",
      "train loss: 1.5477008720562684e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2005:\n",
      "train loss: 1.192189170484746e-05\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2006:\n",
      "train loss: 1.0576104582155302e-05\n",
      "Epoch 02008: reducing learning rate of group 0 to 1.6331e-06.\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2007:\n",
      "train loss: 1.6627517390254055e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2008:\n",
      "train loss: 1.509064415950749e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2009:\n",
      "train loss: 1.094123393348446e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2010:\n",
      "train loss: 9.6743758637316e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2011:\n",
      "train loss: 1.6145649618796024e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2012:\n",
      "train loss: 1.46657263480831e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2013:\n",
      "train loss: 1.138063855028419e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2014:\n",
      "train loss: 1.0126849370408814e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2015:\n",
      "train loss: 1.5681828293495804e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2016:\n",
      "train loss: 1.419453718794015e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2017:\n",
      "train loss: 1.1854386833993432e-05\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2018:\n",
      "train loss: 1.0602102443318044e-05\n",
      "Epoch 02020: reducing learning rate of group 0 to 1.5514e-06.\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2019:\n",
      "train loss: 1.5206623373959473e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2020:\n",
      "train loss: 1.3721847192069667e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2021:\n",
      "train loss: 1.1021306737240917e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2022:\n",
      "train loss: 9.835780695470987e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2023:\n",
      "train loss: 1.466795413849067e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2024:\n",
      "train loss: 1.3244656263454164e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2025:\n",
      "train loss: 1.1506310211087101e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2026:\n",
      "train loss: 1.0325421037460859e-05\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2027:\n",
      "train loss: 1.4177895345630335e-05\n",
      "Epoch 02029: reducing learning rate of group 0 to 1.4739e-06.\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2028:\n",
      "train loss: 1.2757245862677836e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2029:\n",
      "train loss: 1.1986475919824639e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2030:\n",
      "train loss: 1.085231895483262e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2031:\n",
      "train loss: 1.2448432873609724e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2032:\n",
      "train loss: 1.1123760817186561e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2033:\n",
      "train loss: 1.2350891468161706e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2034:\n",
      "train loss: 1.1183185328691158e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2035:\n",
      "train loss: 1.2150935968605226e-05\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2036:\n",
      "train loss: 1.0859149088503715e-05\n",
      "Epoch 02038: reducing learning rate of group 0 to 1.4002e-06.\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2037:\n",
      "train loss: 1.2581887594336009e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2038:\n",
      "train loss: 1.1382737784771331e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2039:\n",
      "train loss: 1.0816276742813872e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2040:\n",
      "train loss: 9.626168499177382e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2041:\n",
      "train loss: 1.2595848204095818e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2042:\n",
      "train loss: 1.1410653737615986e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2043:\n",
      "train loss: 1.0831103285040175e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2044:\n",
      "train loss: 9.680450736476331e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2045:\n",
      "train loss: 1.2505484398138464e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2046:\n",
      "train loss: 1.1289358224505023e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2047:\n",
      "train loss: 1.0977818345186828e-05\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2048:\n",
      "train loss: 9.849345315173049e-06\n",
      "Epoch 02050: reducing learning rate of group 0 to 1.3302e-06.\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2049:\n",
      "train loss: 1.2318121648851416e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2050:\n",
      "train loss: 1.1087318599636716e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2051:\n",
      "train loss: 1.007969143239456e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2052:\n",
      "train loss: 9.026352639156616e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2053:\n",
      "train loss: 1.2005136925535136e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2054:\n",
      "train loss: 1.0809371294747195e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2055:\n",
      "train loss: 1.0379194951226232e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2056:\n",
      "train loss: 9.342108398662149e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2057:\n",
      "train loss: 1.1679967266838385e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2058:\n",
      "train loss: 1.0479031518693713e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2059:\n",
      "train loss: 1.0709111946047716e-05\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2060:\n",
      "train loss: 9.668151907508612e-06\n",
      "Epoch 02062: reducing learning rate of group 0 to 1.2637e-06.\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2061:\n",
      "train loss: 1.136261080551057e-05\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2062:\n",
      "train loss: 1.0172674739185779e-05\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2063:\n",
      "train loss: 9.945773527144251e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2064:\n",
      "train loss: 8.951837963585684e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2065:\n",
      "train loss: 1.1024215608260346e-05\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2066:\n",
      "train loss: 9.888910104877983e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2067:\n",
      "train loss: 1.0231813282736196e-05\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2068:\n",
      "train loss: 9.236465132098942e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2069:\n",
      "train loss: 1.0746224087591155e-05\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2070:\n",
      "train loss: 9.620166454690615e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2071:\n",
      "train loss: 1.048735974562518e-05\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2072:\n",
      "train loss: 9.477208119546373e-06\n",
      "Epoch 02074: reducing learning rate of group 0 to 1.2005e-06.\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2073:\n",
      "train loss: 1.0522959222745508e-05\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2074:\n",
      "train loss: 9.415434101065504e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2075:\n",
      "train loss: 9.670971632714848e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2076:\n",
      "train loss: 8.701684427779688e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2077:\n",
      "train loss: 1.029864848092951e-05\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2078:\n",
      "train loss: 9.243236382597821e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2079:\n",
      "train loss: 9.845925801206542e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2080:\n",
      "train loss: 8.876910203017824e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2081:\n",
      "train loss: 1.0126763482044687e-05\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2082:\n",
      "train loss: 9.076776674561217e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2083:\n",
      "train loss: 1.0004002135465791e-05\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2084:\n",
      "train loss: 9.02549929461381e-06\n",
      "Epoch 02086: reducing learning rate of group 0 to 1.1405e-06.\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2085:\n",
      "train loss: 9.989505965354133e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2086:\n",
      "train loss: 8.951704578067944e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2087:\n",
      "train loss: 9.166148454892747e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2088:\n",
      "train loss: 8.23422301987995e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2089:\n",
      "train loss: 9.822053294857536e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2090:\n",
      "train loss: 8.824777121594693e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2091:\n",
      "train loss: 9.30392431307071e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2092:\n",
      "train loss: 8.379834720376125e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2093:\n",
      "train loss: 9.672794042019041e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2094:\n",
      "train loss: 8.67455176330514e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2095:\n",
      "train loss: 9.451634488761616e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2096:\n",
      "train loss: 8.523220072915149e-06\n",
      "Epoch 02098: reducing learning rate of group 0 to 1.0834e-06.\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2097:\n",
      "train loss: 9.5364150177184e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2098:\n",
      "train loss: 8.546580612331511e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2099:\n",
      "train loss: 8.667645698719008e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2100:\n",
      "train loss: 7.786500355159743e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2101:\n",
      "train loss: 9.358989682995001e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2102:\n",
      "train loss: 8.404541849389282e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2103:\n",
      "train loss: 8.822534963962437e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2104:\n",
      "train loss: 7.95046844471624e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2105:\n",
      "train loss: 9.191195487544955e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2106:\n",
      "train loss: 8.236088169852487e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2107:\n",
      "train loss: 8.987538978495717e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2108:\n",
      "train loss: 8.1098031302442e-06\n",
      "Epoch 02110: reducing learning rate of group 0 to 1.0293e-06.\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2109:\n",
      "train loss: 9.040837022942776e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2110:\n",
      "train loss: 8.096319481922847e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2111:\n",
      "train loss: 8.25823337524184e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2112:\n",
      "train loss: 7.423030138493918e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2113:\n",
      "train loss: 8.861100926197325e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2114:\n",
      "train loss: 7.951687698325592e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2115:\n",
      "train loss: 8.41377253766564e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2116:\n",
      "train loss: 7.585672737750323e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2117:\n",
      "train loss: 8.696758188286211e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2118:\n",
      "train loss: 7.788793957939418e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2119:\n",
      "train loss: 8.571110073832251e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2120:\n",
      "train loss: 7.735398584437536e-06\n",
      "Epoch 02122: reducing learning rate of group 0 to 9.7780e-07.\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2121:\n",
      "train loss: 8.557800953725491e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2122:\n",
      "train loss: 7.66209766353341e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2123:\n",
      "train loss: 7.870605797069092e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2124:\n",
      "train loss: 7.074296991219933e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2125:\n",
      "train loss: 8.39588730123076e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2126:\n",
      "train loss: 7.533144165528249e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2127:\n",
      "train loss: 8.01046931543438e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2128:\n",
      "train loss: 7.221391659255862e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2129:\n",
      "train loss: 8.246849148457914e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2130:\n",
      "train loss: 7.385324371104972e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2131:\n",
      "train loss: 8.152990739405865e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2132:\n",
      "train loss: 7.356632729410398e-06\n",
      "Epoch 02134: reducing learning rate of group 0 to 9.2891e-07.\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2133:\n",
      "train loss: 8.121951855052866e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2134:\n",
      "train loss: 7.272272537766982e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2135:\n",
      "train loss: 7.480470846931865e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2136:\n",
      "train loss: 6.722325543968514e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2137:\n",
      "train loss: 7.972842848507624e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2138:\n",
      "train loss: 7.152639990582599e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2139:\n",
      "train loss: 7.612160288675456e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2140:\n",
      "train loss: 6.86226216107641e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2141:\n",
      "train loss: 7.830155922533255e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2142:\n",
      "train loss: 7.010565705238059e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2143:\n",
      "train loss: 7.749308679741471e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2144:\n",
      "train loss: 6.99239483608719e-06\n",
      "Epoch 02146: reducing learning rate of group 0 to 8.8246e-07.\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2145:\n",
      "train loss: 7.71025206883632e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2146:\n",
      "train loss: 6.902519031516362e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2147:\n",
      "train loss: 7.1106687316945625e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2148:\n",
      "train loss: 6.39010623760251e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2149:\n",
      "train loss: 7.567578198948289e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2150:\n",
      "train loss: 6.786856000999297e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2151:\n",
      "train loss: 7.238655527092841e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2152:\n",
      "train loss: 6.526477532895168e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2153:\n",
      "train loss: 7.428540154922034e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2154:\n",
      "train loss: 6.6486801004228784e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2155:\n",
      "train loss: 7.3714385969057485e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2156:\n",
      "train loss: 6.651744320584809e-06\n",
      "Epoch 02158: reducing learning rate of group 0 to 8.3834e-07.\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2157:\n",
      "train loss: 7.3140601182214216e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2158:\n",
      "train loss: 6.546671355695107e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2159:\n",
      "train loss: 6.763213991781394e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2160:\n",
      "train loss: 6.077678998762752e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2161:\n",
      "train loss: 7.180015023432853e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2162:\n",
      "train loss: 6.437507980445533e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2163:\n",
      "train loss: 6.8848257984047084e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2164:\n",
      "train loss: 6.207754875052531e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2165:\n",
      "train loss: 7.047295771425838e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2166:\n",
      "train loss: 6.305791655782558e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2167:\n",
      "train loss: 7.010849930922891e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2168:\n",
      "train loss: 6.325964451546798e-06\n",
      "Epoch 02170: reducing learning rate of group 0 to 7.9642e-07.\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2169:\n",
      "train loss: 6.940165522502403e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2170:\n",
      "train loss: 6.211455818966504e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2171:\n",
      "train loss: 6.429914230260771e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2172:\n",
      "train loss: 5.777472612103564e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2173:\n",
      "train loss: 6.815116303447404e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2174:\n",
      "train loss: 6.10882829835686e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2175:\n",
      "train loss: 6.54549625260679e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2176:\n",
      "train loss: 5.901926208567822e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2177:\n",
      "train loss: 6.6877782148928715e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2178:\n",
      "train loss: 5.98240998495682e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2179:\n",
      "train loss: 6.666082515408744e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2180:\n",
      "train loss: 6.0145006490464325e-06\n",
      "Epoch 02182: reducing learning rate of group 0 to 7.5660e-07.\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2181:\n",
      "train loss: 6.586539768774006e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2182:\n",
      "train loss: 5.894276991411745e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2183:\n",
      "train loss: 6.112128587400935e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2184:\n",
      "train loss: 5.491337269740558e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2185:\n",
      "train loss: 6.468998686749105e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2186:\n",
      "train loss: 5.7968011593019645e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2187:\n",
      "train loss: 6.223037932887055e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2188:\n",
      "train loss: 5.6114505219442545e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2189:\n",
      "train loss: 6.345897102679942e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2190:\n",
      "train loss: 5.674686051907733e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2191:\n",
      "train loss: 6.338995768129862e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2192:\n",
      "train loss: 5.7189945604235025e-06\n",
      "Epoch 02194: reducing learning rate of group 0 to 7.1877e-07.\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2193:\n",
      "train loss: 6.250164122089729e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2194:\n",
      "train loss: 5.59256653242145e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2195:\n",
      "train loss: 5.810399101131647e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2196:\n",
      "train loss: 5.2195075018830985e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2197:\n",
      "train loss: 6.140162648131985e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2198:\n",
      "train loss: 5.500346277558859e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2199:\n",
      "train loss: 5.9165734418508335e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2200:\n",
      "train loss: 5.335235162083764e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2201:\n",
      "train loss: 6.0213543237672705e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2202:\n",
      "train loss: 5.38259947225567e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2203:\n",
      "train loss: 6.027815753414736e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2204:\n",
      "train loss: 5.437616193540949e-06\n",
      "Epoch 02206: reducing learning rate of group 0 to 6.8283e-07.\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2205:\n",
      "train loss: 5.931317281968507e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2206:\n",
      "train loss: 5.3067154744097585e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2207:\n",
      "train loss: 5.522803816181759e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2208:\n",
      "train loss: 4.960185617556691e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2209:\n",
      "train loss: 5.82874235392703e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2210:\n",
      "train loss: 5.219524270184249e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2211:\n",
      "train loss: 5.6246652322869695e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2212:\n",
      "train loss: 5.072077667825444e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2213:\n",
      "train loss: 5.7136367183570805e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2214:\n",
      "train loss: 5.105533294631619e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2215:\n",
      "train loss: 5.731779821418669e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2216:\n",
      "train loss: 5.169803982048504e-06\n",
      "Epoch 02218: reducing learning rate of group 0 to 6.4869e-07.\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2217:\n",
      "train loss: 5.628918245937701e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2218:\n",
      "train loss: 5.035616126789247e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2219:\n",
      "train loss: 5.24906703338266e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2220:\n",
      "train loss: 4.713209552575907e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2221:\n",
      "train loss: 5.53341725449127e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2222:\n",
      "train loss: 4.953049805746123e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2223:\n",
      "train loss: 5.347161723562039e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2224:\n",
      "train loss: 4.821873979315435e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2225:\n",
      "train loss: 5.421451430627442e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2226:\n",
      "train loss: 4.842308218365313e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2227:\n",
      "train loss: 5.450559349481154e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2228:\n",
      "train loss: 4.915204817869803e-06\n",
      "Epoch 02230: reducing learning rate of group 0 to 6.1626e-07.\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2229:\n",
      "train loss: 5.341992056235169e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2230:\n",
      "train loss: 4.77845295384828e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2231:\n",
      "train loss: 4.988566870984486e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2232:\n",
      "train loss: 4.477940667907345e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2233:\n",
      "train loss: 5.25353575029113e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2234:\n",
      "train loss: 4.700370467662915e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2235:\n",
      "train loss: 5.083248952575361e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2236:\n",
      "train loss: 4.583848277382123e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2237:\n",
      "train loss: 5.14425514821836e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2238:\n",
      "train loss: 4.592449685765095e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2239:\n",
      "train loss: 5.183295091051914e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2240:\n",
      "train loss: 4.673013388378673e-06\n",
      "Epoch 02242: reducing learning rate of group 0 to 5.8544e-07.\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2241:\n",
      "train loss: 5.070027371791575e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2242:\n",
      "train loss: 4.53477238867736e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2243:\n",
      "train loss: 4.740527031373307e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2244:\n",
      "train loss: 4.253686988074439e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2245:\n",
      "train loss: 4.98850992771916e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2246:\n",
      "train loss: 4.4609135369406605e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2247:\n",
      "train loss: 4.8323137467490295e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2248:\n",
      "train loss: 4.357488320302966e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2249:\n",
      "train loss: 4.881308673919319e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2250:\n",
      "train loss: 4.3552625054915776e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2251:\n",
      "train loss: 4.929470802227437e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2252:\n",
      "train loss: 4.442774711445216e-06\n",
      "Epoch 02254: reducing learning rate of group 0 to 5.5617e-07.\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2253:\n",
      "train loss: 4.812284353234243e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2254:\n",
      "train loss: 4.303914400031868e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2255:\n",
      "train loss: 4.504458535975529e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2256:\n",
      "train loss: 4.040011656042952e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2257:\n",
      "train loss: 4.7376634153690535e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2258:\n",
      "train loss: 4.2340848832130484e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2259:\n",
      "train loss: 4.593817004206447e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2260:\n",
      "train loss: 4.142310382106665e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2261:\n",
      "train loss: 4.632008396712894e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2262:\n",
      "train loss: 4.1302414842203344e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2263:\n",
      "train loss: 4.688466934980912e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2264:\n",
      "train loss: 4.223928507267697e-06\n",
      "Epoch 02266: reducing learning rate of group 0 to 5.2836e-07.\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2265:\n",
      "train loss: 4.568240576400577e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2266:\n",
      "train loss: 4.085436711584649e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2267:\n",
      "train loss: 4.2797881261627595e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2268:\n",
      "train loss: 3.836430171390834e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2269:\n",
      "train loss: 4.500466460367864e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2270:\n",
      "train loss: 4.01944695503748e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2271:\n",
      "train loss: 4.367183873967571e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2272:\n",
      "train loss: 3.937828192993007e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2273:\n",
      "train loss: 4.39584800274667e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2274:\n",
      "train loss: 3.916983597697078e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2275:\n",
      "train loss: 4.4596997220385894e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2276:\n",
      "train loss: 4.016010748254283e-06\n",
      "Epoch 02278: reducing learning rate of group 0 to 5.0194e-07.\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2277:\n",
      "train loss: 4.337340573673265e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2278:\n",
      "train loss: 3.8788470782036554e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2279:\n",
      "train loss: 4.066122830853259e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2280:\n",
      "train loss: 3.642675041847715e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2281:\n",
      "train loss: 4.2763000775263685e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2282:\n",
      "train loss: 3.816546470672115e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2283:\n",
      "train loss: 4.151901227173537e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2284:\n",
      "train loss: 3.74360652547777e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2285:\n",
      "train loss: 4.172396526480287e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2286:\n",
      "train loss: 3.7152222897577572e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2287:\n",
      "train loss: 4.2425359406492744e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2288:\n",
      "train loss: 3.818565793343209e-06\n",
      "Epoch 02290: reducing learning rate of group 0 to 4.7685e-07.\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2289:\n",
      "train loss: 4.119056461331683e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2290:\n",
      "train loss: 3.683672798456796e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2291:\n",
      "train loss: 3.863152897482939e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2292:\n",
      "train loss: 3.4586131127048105e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2293:\n",
      "train loss: 4.064506473598162e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2294:\n",
      "train loss: 3.624984016274184e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2295:\n",
      "train loss: 3.947400541388292e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2296:\n",
      "train loss: 3.5591575599085797e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2297:\n",
      "train loss: 3.961337345601142e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2298:\n",
      "train loss: 3.5248548606672448e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2299:\n",
      "train loss: 4.036243391601977e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2300:\n",
      "train loss: 3.631141843778097e-06\n",
      "Epoch 02302: reducing learning rate of group 0 to 4.5301e-07.\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2301:\n",
      "train loss: 3.9127938511029415e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2302:\n",
      "train loss: 3.4993450640975347e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2303:\n",
      "train loss: 3.670712352986263e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2304:\n",
      "train loss: 3.284310923897578e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2305:\n",
      "train loss: 3.864268683949638e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2306:\n",
      "train loss: 3.444320559033585e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2307:\n",
      "train loss: 3.7530436975232054e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2308:\n",
      "train loss: 3.383901374396395e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2309:\n",
      "train loss: 3.762438695064062e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2310:\n",
      "train loss: 3.3459212869776517e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2311:\n",
      "train loss: 3.8399163755749576e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2312:\n",
      "train loss: 3.453208032598159e-06\n",
      "Epoch 02314: reducing learning rate of group 0 to 4.3035e-07.\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2313:\n",
      "train loss: 3.717880499007931e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2314:\n",
      "train loss: 3.3251831802695385e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2315:\n",
      "train loss: 3.488690845804257e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2316:\n",
      "train loss: 3.119935447241099e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2317:\n",
      "train loss: 3.6745902490782273e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2318:\n",
      "train loss: 3.2740201159373475e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2319:\n",
      "train loss: 3.568092501613549e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2320:\n",
      "train loss: 3.217127515707794e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2321:\n",
      "train loss: 3.575474966913443e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2322:\n",
      "train loss: 3.178496612226172e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2323:\n",
      "train loss: 3.6524563731716205e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2324:\n",
      "train loss: 3.2840729162836722e-06\n",
      "Epoch 02326: reducing learning rate of group 0 to 4.0884e-07.\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2325:\n",
      "train loss: 3.533585518546953e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2326:\n",
      "train loss: 3.1604209392515473e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2327:\n",
      "train loss: 3.31688476215006e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2328:\n",
      "train loss: 2.9655741381391027e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2329:\n",
      "train loss: 3.4943116677719107e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2330:\n",
      "train loss: 3.1133405249482466e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2331:\n",
      "train loss: 3.3918031269393252e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2332:\n",
      "train loss: 3.0580929772262923e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2333:\n",
      "train loss: 3.4000321491028778e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2334:\n",
      "train loss: 3.0224553977149387e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2335:\n",
      "train loss: 3.472662316081982e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2336:\n",
      "train loss: 3.1228147759799793e-06\n",
      "Epoch 02338: reducing learning rate of group 0 to 3.8840e-07.\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2337:\n",
      "train loss: 3.3592580987520535e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2338:\n",
      "train loss: 3.004396265997514e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2339:\n",
      "train loss: 3.1547153058740258e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2340:\n",
      "train loss: 2.8209001402576247e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2341:\n",
      "train loss: 3.32230526516936e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2342:\n",
      "train loss: 2.9613178104491815e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2343:\n",
      "train loss: 3.2235866310746404e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2344:\n",
      "train loss: 2.9062371138436292e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2345:\n",
      "train loss: 3.2352501702797484e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2346:\n",
      "train loss: 2.877137311316713e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2347:\n",
      "train loss: 3.299525998509899e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2348:\n",
      "train loss: 2.9683984105615348e-06\n",
      "Epoch 02350: reducing learning rate of group 0 to 3.6898e-07.\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2349:\n",
      "train loss: 3.19442624963053e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2350:\n",
      "train loss: 2.856741051578408e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2351:\n",
      "train loss: 3.0010186144178804e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2352:\n",
      "train loss: 2.6848607864858464e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2353:\n",
      "train loss: 3.157845626812453e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2354:\n",
      "train loss: 2.8170062470320926e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2355:\n",
      "train loss: 3.0630045841856428e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2356:\n",
      "train loss: 2.7612463823178693e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2357:\n",
      "train loss: 3.079803840029385e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2358:\n",
      "train loss: 2.741213367247029e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2359:\n",
      "train loss: 3.1325725210352684e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2360:\n",
      "train loss: 2.820007077560956e-06\n",
      "Epoch 02362: reducing learning rate of group 0 to 3.5053e-07.\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2361:\n",
      "train loss: 3.0386661253134975e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2362:\n",
      "train loss: 2.7172862997756054e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2363:\n",
      "train loss: 2.8542391255506685e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2364:\n",
      "train loss: 2.5557550502976376e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2365:\n",
      "train loss: 3.0008740480488546e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2366:\n",
      "train loss: 2.6798809669124867e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2367:\n",
      "train loss: 2.9094750053554338e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2368:\n",
      "train loss: 2.622727669576792e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2369:\n",
      "train loss: 2.932406190554472e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2370:\n",
      "train loss: 2.613135993584652e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2371:\n",
      "train loss: 2.9717840372248204e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2372:\n",
      "train loss: 2.677196741671709e-06\n",
      "Epoch 02374: reducing learning rate of group 0 to 3.3300e-07.\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2373:\n",
      "train loss: 2.8914399240460586e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2374:\n",
      "train loss: 2.585771752989942e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2375:\n",
      "train loss: 2.7129805991358833e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2376:\n",
      "train loss: 2.431816641793413e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2377:\n",
      "train loss: 2.851777493313583e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2378:\n",
      "train loss: 2.5499103924201613e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2379:\n",
      "train loss: 2.7621090944172704e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2380:\n",
      "train loss: 2.489892478329294e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2381:\n",
      "train loss: 2.792416425288121e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2382:\n",
      "train loss: 2.4918977413890045e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2383:\n",
      "train loss: 2.8170225096810756e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2384:\n",
      "train loss: 2.5395171519453964e-06\n",
      "Epoch 02386: reducing learning rate of group 0 to 3.1635e-07.\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2385:\n",
      "train loss: 2.752374279115012e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2386:\n",
      "train loss: 2.4619733512845585e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2387:\n",
      "train loss: 2.576153177558671e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2388:\n",
      "train loss: 2.3115648533945423e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2389:\n",
      "train loss: 2.711091333167706e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2390:\n",
      "train loss: 2.4273812182138823e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2391:\n",
      "train loss: 2.6198213563076134e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2392:\n",
      "train loss: 2.361610556246142e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2393:\n",
      "train loss: 2.659951450245152e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2394:\n",
      "train loss: 2.3773394690487027e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2395:\n",
      "train loss: 2.667631008406901e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2396:\n",
      "train loss: 2.406110096415473e-06\n",
      "Epoch 02398: reducing learning rate of group 0 to 3.0053e-07.\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2397:\n",
      "train loss: 2.621662255048225e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2398:\n",
      "train loss: 2.3461299061642966e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2399:\n",
      "train loss: 2.4425818990847696e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2400:\n",
      "train loss: 2.193520679596029e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2401:\n",
      "train loss: 2.5796841906470473e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2402:\n",
      "train loss: 2.313067098774462e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2403:\n",
      "train loss: 2.4811552388363295e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2404:\n",
      "train loss: 2.2362825497169484e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2405:\n",
      "train loss: 2.5359803604375885e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2406:\n",
      "train loss: 2.270303151895798e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2407:\n",
      "train loss: 2.52215783800136e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2408:\n",
      "train loss: 2.275367909165337e-06\n",
      "Epoch 02410: reducing learning rate of group 0 to 2.8551e-07.\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2409:\n",
      "train loss: 2.5004383006492074e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2410:\n",
      "train loss: 2.2394030072874615e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2411:\n",
      "train loss: 2.3104416369464477e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2412:\n",
      "train loss: 2.075613356815591e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2413:\n",
      "train loss: 2.4592885341699297e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2414:\n",
      "train loss: 2.208731355300915e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2415:\n",
      "train loss: 2.343766357103109e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2416:\n",
      "train loss: 2.1113437873166744e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2417:\n",
      "train loss: 2.4227673148366205e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2418:\n",
      "train loss: 2.1730732770498097e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2419:\n",
      "train loss: 2.377829838462763e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2420:\n",
      "train loss: 2.1443314385173832e-06\n",
      "Epoch 02422: reducing learning rate of group 0 to 2.7123e-07.\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2421:\n",
      "train loss: 2.3914299719446358e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2422:\n",
      "train loss: 2.144602914405156e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2423:\n",
      "train loss: 2.1764288691568464e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2424:\n",
      "train loss: 1.9542849773317118e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2425:\n",
      "train loss: 2.353388812456233e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2426:\n",
      "train loss: 2.118005497047675e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2427:\n",
      "train loss: 2.203535160822408e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2428:\n",
      "train loss: 1.9823819648080255e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2429:\n",
      "train loss: 2.3247216856688695e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2430:\n",
      "train loss: 2.090212999282561e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2431:\n",
      "train loss: 2.2296915564963442e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2432:\n",
      "train loss: 2.007810627075972e-06\n",
      "Epoch 02434: reducing learning rate of group 0 to 2.5767e-07.\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2433:\n",
      "train loss: 2.299839173457748e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2434:\n",
      "train loss: 2.067085516740336e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2435:\n",
      "train loss: 2.034846703482795e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2436:\n",
      "train loss: 1.8235907418490658e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2437:\n",
      "train loss: 2.26806019611839e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2438:\n",
      "train loss: 2.0471797764778998e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2439:\n",
      "train loss: 2.053835271329301e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2440:\n",
      "train loss: 1.842513344625958e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2441:\n",
      "train loss: 2.248870725446252e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2442:\n",
      "train loss: 2.0289216459371422e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2443:\n",
      "train loss: 2.0703682548972746e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2444:\n",
      "train loss: 1.858338463212397e-06\n",
      "Epoch 02446: reducing learning rate of group 0 to 2.4479e-07.\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2445:\n",
      "train loss: 2.2331349565571025e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2446:\n",
      "train loss: 2.014339508760625e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2447:\n",
      "train loss: 1.878227213136973e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2448:\n",
      "train loss: 1.6761390097220453e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2449:\n",
      "train loss: 2.2106077231244527e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2450:\n",
      "train loss: 2.003503260550911e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2451:\n",
      "train loss: 1.8875699423445605e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2452:\n",
      "train loss: 1.6849055136612862e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2453:\n",
      "train loss: 2.2017074678870685e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2454:\n",
      "train loss: 1.995373772186482e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2455:\n",
      "train loss: 1.8941976630374392e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2456:\n",
      "train loss: 1.690957736590353e-06\n",
      "Epoch 02458: reducing learning rate of group 0 to 2.3255e-07.\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2457:\n",
      "train loss: 2.1954278348217784e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2458:\n",
      "train loss: 1.9896875689501723e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2459:\n",
      "train loss: 1.7041770825807834e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2460:\n",
      "train loss: 1.510729881267294e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2461:\n",
      "train loss: 2.1808710108037862e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2462:\n",
      "train loss: 1.985674014961577e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2463:\n",
      "train loss: 1.707297171891235e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2464:\n",
      "train loss: 1.5137936103665492e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2465:\n",
      "train loss: 2.1769812575002125e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2466:\n",
      "train loss: 1.9816344015280793e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2467:\n",
      "train loss: 1.7108605358919171e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2468:\n",
      "train loss: 1.5176770697553372e-06\n",
      "Epoch 02470: reducing learning rate of group 0 to 2.2092e-07.\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2469:\n",
      "train loss: 2.171875823598658e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2470:\n",
      "train loss: 1.9759475158084994e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2471:\n",
      "train loss: 1.5319825463923394e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2472:\n",
      "train loss: 1.3495486738158056e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2473:\n",
      "train loss: 2.153043944565716e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2474:\n",
      "train loss: 1.9650693439177834e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2475:\n",
      "train loss: 1.5440432120997585e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2476:\n",
      "train loss: 1.3635584230047292e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2477:\n",
      "train loss: 2.1360434676584747e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2478:\n",
      "train loss: 1.9457950684918424e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2479:\n",
      "train loss: 1.5648598662981934e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2480:\n",
      "train loss: 1.3865994736899184e-06\n",
      "Epoch 02482: reducing learning rate of group 0 to 2.0987e-07.\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2481:\n",
      "train loss: 2.1097962678624128e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2482:\n",
      "train loss: 1.9170196150184426e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2483:\n",
      "train loss: 1.4200971023392389e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2484:\n",
      "train loss: 1.254387163169635e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2485:\n",
      "train loss: 2.0610432227896055e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2486:\n",
      "train loss: 1.8725414339349032e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2487:\n",
      "train loss: 1.4688624176940103e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2488:\n",
      "train loss: 1.3076514679798894e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2489:\n",
      "train loss: 2.002770597778326e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2490:\n",
      "train loss: 1.8101244054429433e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2491:\n",
      "train loss: 1.5342697430076488e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2492:\n",
      "train loss: 1.375848955793737e-06\n",
      "Epoch 02494: reducing learning rate of group 0 to 1.9938e-07.\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2493:\n",
      "train loss: 1.931864125402318e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2494:\n",
      "train loss: 1.7370683649966375e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2495:\n",
      "train loss: 1.441758911134382e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2496:\n",
      "train loss: 1.2939820628814718e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2497:\n",
      "train loss: 1.843750458105237e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2498:\n",
      "train loss: 1.6543321802817454e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2499:\n",
      "train loss: 1.5275836511025775e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2500:\n",
      "train loss: 1.3814899685086313e-06\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2501:\n",
      "train loss: 1.7562802461416209e-06\n",
      "Epoch 02503: reducing learning rate of group 0 to 1.8941e-07.\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2502:\n",
      "train loss: 1.5673988407153789e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2503:\n",
      "train loss: 1.6127326813091025e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2504:\n",
      "train loss: 1.4703496619791065e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2505:\n",
      "train loss: 1.5173909947081894e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2506:\n",
      "train loss: 1.3456276900032765e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2507:\n",
      "train loss: 1.6656147927371536e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2508:\n",
      "train loss: 1.5128120621421413e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2509:\n",
      "train loss: 1.4852520181298233e-06\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2510:\n",
      "train loss: 1.3232769308160663e-06\n",
      "Epoch 02512: reducing learning rate of group 0 to 1.7994e-07.\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2511:\n",
      "train loss: 1.6786755051819754e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2512:\n",
      "train loss: 1.517268187122894e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2513:\n",
      "train loss: 1.33926954355216e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2514:\n",
      "train loss: 1.1946679246684933e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2515:\n",
      "train loss: 1.6463581229487294e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2516:\n",
      "train loss: 1.482588594013795e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2517:\n",
      "train loss: 1.3831117241759234e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2518:\n",
      "train loss: 1.2455976422708048e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2519:\n",
      "train loss: 1.5910473195792713e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2520:\n",
      "train loss: 1.423949753481413e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2521:\n",
      "train loss: 1.4437898040808312e-06\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2522:\n",
      "train loss: 1.3067576941453932e-06\n",
      "Epoch 02524: reducing learning rate of group 0 to 1.7094e-07.\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2523:\n",
      "train loss: 1.5315471681132183e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2524:\n",
      "train loss: 1.3666003749790198e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2525:\n",
      "train loss: 1.3558167220008103e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2526:\n",
      "train loss: 1.2240331484357144e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2527:\n",
      "train loss: 1.4734460928473612e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2528:\n",
      "train loss: 1.3173356729651166e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2529:\n",
      "train loss: 1.404337427548648e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2530:\n",
      "train loss: 1.270950758368367e-06\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2531:\n",
      "train loss: 1.4295112931679963e-06\n",
      "Epoch 02533: reducing learning rate of group 0 to 1.6240e-07.\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2532:\n",
      "train loss: 1.2766445651162754e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2533:\n",
      "train loss: 1.4416241630998165e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2534:\n",
      "train loss: 1.31097057828825e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2535:\n",
      "train loss: 1.2601846258696301e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2536:\n",
      "train loss: 1.1213766853425962e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2537:\n",
      "train loss: 1.4535814439139103e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2538:\n",
      "train loss: 1.3158454479230905e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2539:\n",
      "train loss: 1.261703658217753e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2540:\n",
      "train loss: 1.1285385911535071e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2541:\n",
      "train loss: 1.4419789249211614e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2542:\n",
      "train loss: 1.3005929140878208e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2543:\n",
      "train loss: 1.280037212460712e-06\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2544:\n",
      "train loss: 1.1493514987366146e-06\n",
      "Epoch 02546: reducing learning rate of group 0 to 1.5428e-07.\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2545:\n",
      "train loss: 1.4196765084328902e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2546:\n",
      "train loss: 1.2772971573917532e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2547:\n",
      "train loss: 1.175321385359196e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2548:\n",
      "train loss: 1.0525882951191793e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2549:\n",
      "train loss: 1.3861472706768218e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2550:\n",
      "train loss: 1.2491001372707806e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2551:\n",
      "train loss: 1.2050325615374222e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2552:\n",
      "train loss: 1.0832902953681158e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2553:\n",
      "train loss: 1.3553046796102007e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2554:\n",
      "train loss: 1.2184816150216095e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2555:\n",
      "train loss: 1.2351765565167343e-06\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2556:\n",
      "train loss: 1.1127467564369756e-06\n",
      "Epoch 02558: reducing learning rate of group 0 to 1.4656e-07.\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2557:\n",
      "train loss: 1.327007515500652e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2558:\n",
      "train loss: 1.1915004508662967e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2559:\n",
      "train loss: 1.1384187609309873e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2560:\n",
      "train loss: 1.0214630703893257e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2561:\n",
      "train loss: 1.2965280905664425e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2562:\n",
      "train loss: 1.1679049456548428e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2563:\n",
      "train loss: 1.1618862808515633e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2564:\n",
      "train loss: 1.0446575409954073e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2565:\n",
      "train loss: 1.2739532147591002e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2566:\n",
      "train loss: 1.1461083565673272e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2567:\n",
      "train loss: 1.182830892723473e-06\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2568:\n",
      "train loss: 1.0647483437366602e-06\n",
      "Epoch 02570: reducing learning rate of group 0 to 1.3923e-07.\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2569:\n",
      "train loss: 1.2548790064826249e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2570:\n",
      "train loss: 1.12812176732063e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2571:\n",
      "train loss: 1.0835058531140683e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2572:\n",
      "train loss: 9.708131905523933e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2573:\n",
      "train loss: 1.232977687040099e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2574:\n",
      "train loss: 1.1126249781598475e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2575:\n",
      "train loss: 1.0989324010415678e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2576:\n",
      "train loss: 9.861295929718724e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2577:\n",
      "train loss: 1.2179363908896862e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2578:\n",
      "train loss: 1.097983135006991e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2579:\n",
      "train loss: 1.1131104916847285e-06\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2580:\n",
      "train loss: 9.99873039428537e-07\n",
      "Epoch 02582: reducing learning rate of group 0 to 1.3227e-07.\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2581:\n",
      "train loss: 1.204699819096519e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2582:\n",
      "train loss: 1.0853285747460062e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2583:\n",
      "train loss: 1.014748715177214e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2584:\n",
      "train loss: 9.06990276884865e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2585:\n",
      "train loss: 1.1872312135811998e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2586:\n",
      "train loss: 1.0736913529226147e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2587:\n",
      "train loss: 1.026479891802016e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2588:\n",
      "train loss: 9.187982338840884e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2589:\n",
      "train loss: 1.175442605791629e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2590:\n",
      "train loss: 1.0620406123004605e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2591:\n",
      "train loss: 1.0379120675216292e-06\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2592:\n",
      "train loss: 9.300396951918668e-07\n",
      "Epoch 02594: reducing learning rate of group 0 to 1.2566e-07.\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2593:\n",
      "train loss: 1.1644288461779598e-06\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2594:\n",
      "train loss: 1.0513354847189041e-06\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2595:\n",
      "train loss: 9.433650298780525e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2596:\n",
      "train loss: 8.408711437925485e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2597:\n",
      "train loss: 1.1486136455607707e-06\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2598:\n",
      "train loss: 1.0409386523340633e-06\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2599:\n",
      "train loss: 9.539265010974429e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2600:\n",
      "train loss: 8.515896229773452e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2601:\n",
      "train loss: 1.1377965689889956e-06\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2602:\n",
      "train loss: 1.030136391589038e-06\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2603:\n",
      "train loss: 9.646228768730754e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2604:\n",
      "train loss: 8.622068834034024e-07\n",
      "Epoch 02606: reducing learning rate of group 0 to 1.1938e-07.\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2605:\n",
      "train loss: 1.1272739468790202e-06\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2606:\n",
      "train loss: 1.0197897929189234e-06\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2607:\n",
      "train loss: 8.750759488804687e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2608:\n",
      "train loss: 7.778539975275379e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2609:\n",
      "train loss: 1.1118034895853298e-06\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2610:\n",
      "train loss: 1.0093898913483799e-06\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2611:\n",
      "train loss: 8.856892831928202e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2612:\n",
      "train loss: 7.886780844609943e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2613:\n",
      "train loss: 1.1008043853066046e-06\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2614:\n",
      "train loss: 9.983276565620957e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2615:\n",
      "train loss: 8.967151734330991e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2616:\n",
      "train loss: 7.9969353122951e-07\n",
      "Epoch 02618: reducing learning rate of group 0 to 1.1341e-07.\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2617:\n",
      "train loss: 1.0898014086570726e-06\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2618:\n",
      "train loss: 9.87419387057853e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2619:\n",
      "train loss: 8.127796150832372e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2620:\n",
      "train loss: 7.207569960712512e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2621:\n",
      "train loss: 1.0738139645263844e-06\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2622:\n",
      "train loss: 9.761632731786284e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2623:\n",
      "train loss: 8.243156174861571e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2624:\n",
      "train loss: 7.32569918648147e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2625:\n",
      "train loss: 1.0617487181668052e-06\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2626:\n",
      "train loss: 9.639623722032901e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2627:\n",
      "train loss: 8.365409475761815e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2628:\n",
      "train loss: 7.448425140680282e-07\n",
      "Epoch 02630: reducing learning rate of group 0 to 1.0774e-07.\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2629:\n",
      "train loss: 1.0494247531892794e-06\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2630:\n",
      "train loss: 9.516731695050897e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2631:\n",
      "train loss: 7.587631660820176e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2632:\n",
      "train loss: 6.718746474686945e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2633:\n",
      "train loss: 1.0321079210908564e-06\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2634:\n",
      "train loss: 9.387485917805308e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2635:\n",
      "train loss: 7.720536848133013e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2636:\n",
      "train loss: 6.855204707445427e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2637:\n",
      "train loss: 1.0181329151535164e-06\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2638:\n",
      "train loss: 9.245717221958814e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2639:\n",
      "train loss: 7.863041036849435e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2640:\n",
      "train loss: 6.998590575463565e-07\n",
      "Epoch 02642: reducing learning rate of group 0 to 1.0235e-07.\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2641:\n",
      "train loss: 1.00370610099259e-06\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2642:\n",
      "train loss: 9.101489024627e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2643:\n",
      "train loss: 7.15166851308329e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2644:\n",
      "train loss: 6.333287841524214e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2645:\n",
      "train loss: 9.8437444178104e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2646:\n",
      "train loss: 8.948983780285589e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2647:\n",
      "train loss: 7.308606979114111e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2648:\n",
      "train loss: 6.49436545150937e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2649:\n",
      "train loss: 9.678985679478274e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2650:\n",
      "train loss: 8.781935829176605e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2651:\n",
      "train loss: 7.476463121086519e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2652:\n",
      "train loss: 6.663007663696563e-07\n",
      "Epoch 02654: reducing learning rate of group 0 to 9.7232e-08.\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2653:\n",
      "train loss: 9.509751564838611e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2654:\n",
      "train loss: 8.613070173210985e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2655:\n",
      "train loss: 6.831938505173252e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2656:\n",
      "train loss: 6.06193301462653e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2657:\n",
      "train loss: 9.295057407490074e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2658:\n",
      "train loss: 8.436931080668942e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2659:\n",
      "train loss: 7.012649767688975e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2660:\n",
      "train loss: 6.246627443607633e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2661:\n",
      "train loss: 9.107225456296296e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2662:\n",
      "train loss: 8.24740184525429e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2663:\n",
      "train loss: 7.202249069237618e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2664:\n",
      "train loss: 6.436045552893521e-07\n",
      "Epoch 02666: reducing learning rate of group 0 to 9.2371e-08.\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2665:\n",
      "train loss: 8.918505266157482e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2666:\n",
      "train loss: 8.060302065463764e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2667:\n",
      "train loss: 6.615279177436861e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2668:\n",
      "train loss: 5.889047074885732e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2669:\n",
      "train loss: 8.691730099858916e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2670:\n",
      "train loss: 7.871020616415616e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2671:\n",
      "train loss: 6.808283203737758e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2672:\n",
      "train loss: 6.084914239787974e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2673:\n",
      "train loss: 8.494248049163235e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2674:\n",
      "train loss: 7.67330128482733e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2675:\n",
      "train loss: 7.004585000794e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2676:\n",
      "train loss: 6.279387382752558e-07\n",
      "Epoch 02678: reducing learning rate of group 0 to 8.7752e-08.\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2677:\n",
      "train loss: 8.302385004693973e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2678:\n",
      "train loss: 7.484867904965167e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2679:\n",
      "train loss: 6.455953498622945e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2680:\n",
      "train loss: 5.766938798155858e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2681:\n",
      "train loss: 8.081614143016984e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2682:\n",
      "train loss: 7.301106433091904e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2683:\n",
      "train loss: 6.642091844245094e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2684:\n",
      "train loss: 5.954503674473272e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2685:\n",
      "train loss: 7.894054011574375e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2686:\n",
      "train loss: 7.114819092331165e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2687:\n",
      "train loss: 6.825513367782755e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2688:\n",
      "train loss: 6.134647851000906e-07\n",
      "Epoch 02690: reducing learning rate of group 0 to 8.3365e-08.\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2689:\n",
      "train loss: 7.718022314905329e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2690:\n",
      "train loss: 6.94362841744704e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2691:\n",
      "train loss: 6.295321771468239e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2692:\n",
      "train loss: 5.637595613182961e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2693:\n",
      "train loss: 7.519769594646574e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2694:\n",
      "train loss: 6.781339160934882e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2695:\n",
      "train loss: 6.459143523408108e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2696:\n",
      "train loss: 5.802063570071253e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2697:\n",
      "train loss: 7.356045666361722e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2698:\n",
      "train loss: 6.619564949898389e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2699:\n",
      "train loss: 6.617436350202152e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2700:\n",
      "train loss: 5.95655267688352e-07\n",
      "Epoch 02702: reducing learning rate of group 0 to 7.9196e-08.\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2701:\n",
      "train loss: 7.206102280822902e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2702:\n",
      "train loss: 6.474872726478126e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2703:\n",
      "train loss: 6.095630386468117e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2704:\n",
      "train loss: 5.466068370487211e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2705:\n",
      "train loss: 7.036088712585714e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2706:\n",
      "train loss: 6.338743260594284e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2707:\n",
      "train loss: 6.233406914859315e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2708:\n",
      "train loss: 5.604685864817688e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2709:\n",
      "train loss: 6.897925828100439e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2710:\n",
      "train loss: 6.202265262812368e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2711:\n",
      "train loss: 6.36666386040252e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2712:\n",
      "train loss: 5.734466761671442e-07\n",
      "Epoch 02714: reducing learning rate of group 0 to 7.5237e-08.\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2713:\n",
      "train loss: 6.772270127406477e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2714:\n",
      "train loss: 6.081489175827386e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2715:\n",
      "train loss: 5.854746859931441e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2716:\n",
      "train loss: 5.252881320047849e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2717:\n",
      "train loss: 6.62558727550626e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2718:\n",
      "train loss: 5.966003725582172e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2719:\n",
      "train loss: 5.972610686047076e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2720:\n",
      "train loss: 5.372334160271545e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2721:\n",
      "train loss: 6.505814013529279e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2722:\n",
      "train loss: 5.847239810115748e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2723:\n",
      "train loss: 6.088741962823614e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2724:\n",
      "train loss: 5.485588419119437e-07\n",
      "Epoch 02726: reducing learning rate of group 0 to 7.1475e-08.\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2725:\n",
      "train loss: 6.396061207057909e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2726:\n",
      "train loss: 5.741840779869987e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2727:\n",
      "train loss: 5.593588132632866e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2728:\n",
      "train loss: 5.019899366455873e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2729:\n",
      "train loss: 6.263974980361544e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2730:\n",
      "train loss: 5.638343174708015e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2731:\n",
      "train loss: 5.70021034345925e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2732:\n",
      "train loss: 5.12882011190221e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2733:\n",
      "train loss: 6.154074408867085e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2734:\n",
      "train loss: 5.528930820008533e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2735:\n",
      "train loss: 5.807364692756011e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2736:\n",
      "train loss: 5.233455398467497e-07\n",
      "Epoch 02738: reducing learning rate of group 0 to 6.7901e-08.\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2737:\n",
      "train loss: 6.052606924170399e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2738:\n",
      "train loss: 5.431598998828567e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2739:\n",
      "train loss: 5.334296613911489e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2740:\n",
      "train loss: 4.788682459849185e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2741:\n",
      "train loss: 5.928891546499913e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2742:\n",
      "train loss: 5.334315887040314e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2743:\n",
      "train loss: 5.435126097727047e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2744:\n",
      "train loss: 4.892187691773257e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2745:\n",
      "train loss: 5.824105926068489e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2746:\n",
      "train loss: 5.229861181546924e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2747:\n",
      "train loss: 5.537311300108142e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2748:\n",
      "train loss: 4.991838252729567e-07\n",
      "Epoch 02750: reducing learning rate of group 0 to 6.4506e-08.\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2749:\n",
      "train loss: 5.727662075792401e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2750:\n",
      "train loss: 5.137703667977162e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2751:\n",
      "train loss: 5.087339635020468e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2752:\n",
      "train loss: 4.56870585787688e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2753:\n",
      "train loss: 5.610440585164527e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2754:\n",
      "train loss: 5.045145965876198e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2755:\n",
      "train loss: 5.183591458308553e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2756:\n",
      "train loss: 4.667759959005029e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2757:\n",
      "train loss: 5.510035913259707e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2758:\n",
      "train loss: 4.94514046042133e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2759:\n",
      "train loss: 5.2810865456167e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2760:\n",
      "train loss: 4.7625034462487666e-07\n",
      "Epoch 02762: reducing learning rate of group 0 to 6.1281e-08.\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2761:\n",
      "train loss: 5.418708414333199e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2762:\n",
      "train loss: 4.858415499705132e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2763:\n",
      "train loss: 4.852396015889659e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2764:\n",
      "train loss: 4.359159630253624e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2765:\n",
      "train loss: 5.308481847475421e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2766:\n",
      "train loss: 4.7711984760828e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2767:\n",
      "train loss: 4.94341758953347e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2768:\n",
      "train loss: 4.4530922136381255e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2769:\n",
      "train loss: 5.213120746944803e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2770:\n",
      "train loss: 4.6763052148432767e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2771:\n",
      "train loss: 5.035543768783985e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2772:\n",
      "train loss: 4.542248783501645e-07\n",
      "Epoch 02774: reducing learning rate of group 0 to 5.8217e-08.\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2773:\n",
      "train loss: 5.127566463753832e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2774:\n",
      "train loss: 4.595654319274231e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2775:\n",
      "train loss: 4.6261825591855315e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2776:\n",
      "train loss: 4.15688103120072e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2777:\n",
      "train loss: 5.024671431905906e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2778:\n",
      "train loss: 4.5140436962438054e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2779:\n",
      "train loss: 4.711825965749032e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2780:\n",
      "train loss: 4.2456565548991963e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2781:\n",
      "train loss: 4.934277952765324e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2782:\n",
      "train loss: 4.424101647980323e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2783:\n",
      "train loss: 4.798786442661822e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2784:\n",
      "train loss: 4.3294647978542354e-07\n",
      "Epoch 02786: reducing learning rate of group 0 to 5.5306e-08.\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2785:\n",
      "train loss: 4.854221531191653e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2786:\n",
      "train loss: 4.3492313789890326e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2787:\n",
      "train loss: 4.4077873112710466e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2788:\n",
      "train loss: 3.9612299107899834e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2789:\n",
      "train loss: 4.758087856340574e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2790:\n",
      "train loss: 4.272576047024987e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2791:\n",
      "train loss: 4.488819363130144e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2792:\n",
      "train loss: 4.0457044964999685e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2793:\n",
      "train loss: 4.6717420811736376e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2794:\n",
      "train loss: 4.1866383683146285e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2795:\n",
      "train loss: 4.5715183841102415e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2796:\n",
      "train loss: 4.125007647393939e-07\n",
      "Epoch 02798: reducing learning rate of group 0 to 5.2541e-08.\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2797:\n",
      "train loss: 4.5964106408489367e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2798:\n",
      "train loss: 4.116866750941686e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2799:\n",
      "train loss: 4.198258505688771e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2800:\n",
      "train loss: 3.7733125829103874e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2801:\n",
      "train loss: 4.5063363084232846e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2802:\n",
      "train loss: 4.0444116573159425e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2803:\n",
      "train loss: 4.2755173415413515e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2804:\n",
      "train loss: 3.854368496365719e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2805:\n",
      "train loss: 4.4231494170502726e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2806:\n",
      "train loss: 3.961625773799011e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2807:\n",
      "train loss: 4.354677000437274e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2808:\n",
      "train loss: 3.9297330035632893e-07\n",
      "Epoch 02810: reducing learning rate of group 0 to 4.9914e-08.\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2809:\n",
      "train loss: 4.352140543180623e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2810:\n",
      "train loss: 3.8967305791234257e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2811:\n",
      "train loss: 3.9980713435471735e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2812:\n",
      "train loss: 3.593489044121557e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2813:\n",
      "train loss: 4.2679418639280895e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2814:\n",
      "train loss: 3.828163578407827e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2815:\n",
      "train loss: 4.0720168397841364e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2816:\n",
      "train loss: 3.671671010612178e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2817:\n",
      "train loss: 4.187344749131989e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2818:\n",
      "train loss: 3.7479909966189585e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2819:\n",
      "train loss: 4.148013400153374e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2820:\n",
      "train loss: 3.7432648607173813e-07\n",
      "Epoch 02822: reducing learning rate of group 0 to 4.7418e-08.\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2821:\n",
      "train loss: 4.120716565856244e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2822:\n",
      "train loss: 3.68826559889005e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2823:\n",
      "train loss: 3.8065033311576463e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2824:\n",
      "train loss: 3.4209373494906496e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2825:\n",
      "train loss: 4.042583586982927e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2826:\n",
      "train loss: 3.6234707912809976e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2827:\n",
      "train loss: 3.8775587891789964e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2828:\n",
      "train loss: 3.49685586242409e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2829:\n",
      "train loss: 3.9639043993062935e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2830:\n",
      "train loss: 3.545254517028158e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2831:\n",
      "train loss: 3.950842753427406e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2832:\n",
      "train loss: 3.564827986013157e-07\n",
      "Epoch 02834: reducing learning rate of group 0 to 4.5047e-08.\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2833:\n",
      "train loss: 3.901863349761529e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2834:\n",
      "train loss: 3.4912489330920075e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2835:\n",
      "train loss: 3.62262489630702e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2836:\n",
      "train loss: 3.254651692107424e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2837:\n",
      "train loss: 3.8301201772494527e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2838:\n",
      "train loss: 3.4300159952085805e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2839:\n",
      "train loss: 3.69152607835622e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2840:\n",
      "train loss: 3.3293412155892954e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2841:\n",
      "train loss: 3.752284672362672e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2842:\n",
      "train loss: 3.3527393208346227e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2843:\n",
      "train loss: 3.76278863311989e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2844:\n",
      "train loss: 3.3938766808206477e-07\n",
      "Epoch 02846: reducing learning rate of group 0 to 4.2795e-08.\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2845:\n",
      "train loss: 3.6952713610355056e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2846:\n",
      "train loss: 3.305441856416702e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2847:\n",
      "train loss: 3.4456142037648436e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2848:\n",
      "train loss: 3.093659207932367e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2849:\n",
      "train loss: 3.6305149331121216e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2850:\n",
      "train loss: 3.24749562017656e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2851:\n",
      "train loss: 3.5135130508994527e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2852:\n",
      "train loss: 3.1687348742400283e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2853:\n",
      "train loss: 3.5519566263165565e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2854:\n",
      "train loss: 3.1697519346730903e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2855:\n",
      "train loss: 3.5836520282408e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2856:\n",
      "train loss: 3.229900782818933e-07\n",
      "Epoch 02858: reducing learning rate of group 0 to 4.0655e-08.\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2857:\n",
      "train loss: 3.5009628484348834e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2858:\n",
      "train loss: 3.1309744551439084e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2859:\n",
      "train loss: 3.274530733332804e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2860:\n",
      "train loss: 2.936840316219516e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2861:\n",
      "train loss: 3.4440807042564743e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2862:\n",
      "train loss: 3.075771123767641e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2863:\n",
      "train loss: 3.3434478818545667e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2864:\n",
      "train loss: 3.0150526364739234e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2865:\n",
      "train loss: 3.362329348802915e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2866:\n",
      "train loss: 2.9954957904835813e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2867:\n",
      "train loss: 3.4137022882048437e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2868:\n",
      "train loss: 3.072729375469907e-07\n",
      "Epoch 02870: reducing learning rate of group 0 to 3.8622e-08.\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2869:\n",
      "train loss: 3.319218600218405e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2870:\n",
      "train loss: 2.968274075517723e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2871:\n",
      "train loss: 3.108706733698635e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2872:\n",
      "train loss: 2.783441346767712e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2873:\n",
      "train loss: 3.271153161694593e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2874:\n",
      "train loss: 2.914534195529298e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2875:\n",
      "train loss: 3.1822367458361834e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2876:\n",
      "train loss: 2.8694124846651735e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2877:\n",
      "train loss: 3.182474126741551e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2878:\n",
      "train loss: 2.8290323288486726e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2879:\n",
      "train loss: 3.253863157894722e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2880:\n",
      "train loss: 2.92268584261829e-07\n",
      "Epoch 02882: reducing learning rate of group 0 to 3.6691e-08.\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2881:\n",
      "train loss: 3.1508069615369574e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2882:\n",
      "train loss: 2.818395404626568e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2883:\n",
      "train loss: 2.947860110982073e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2884:\n",
      "train loss: 2.633428729527849e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2885:\n",
      "train loss: 3.1120639197200967e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2886:\n",
      "train loss: 2.7635049198136167e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2887:\n",
      "train loss: 3.0317738899256416e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2888:\n",
      "train loss: 2.734108142064667e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2889:\n",
      "train loss: 3.011567553141614e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2890:\n",
      "train loss: 2.6701564041194926e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2891:\n",
      "train loss: 3.1050699815587207e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2892:\n",
      "train loss: 2.780315879584667e-07\n",
      "Epoch 02894: reducing learning rate of group 0 to 3.4856e-08.\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2893:\n",
      "train loss: 2.997160806500008e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2894:\n",
      "train loss: 2.68312571650318e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2895:\n",
      "train loss: 2.792464242918406e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2896:\n",
      "train loss: 2.4883183410966554e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2897:\n",
      "train loss: 2.966735095306802e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2898:\n",
      "train loss: 2.6229091469460083e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2899:\n",
      "train loss: 2.89386851785831e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2900:\n",
      "train loss: 2.611561627666047e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2901:\n",
      "train loss: 2.8498067732815546e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2902:\n",
      "train loss: 2.5205961716720116e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2903:\n",
      "train loss: 2.967425683586758e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2904:\n",
      "train loss: 2.6469597739935163e-07\n",
      "Epoch 02906: reducing learning rate of group 0 to 3.3114e-08.\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2905:\n",
      "train loss: 2.858160389858315e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2906:\n",
      "train loss: 2.5625900451510655e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2907:\n",
      "train loss: 2.6450377988040574e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2908:\n",
      "train loss: 2.3520842695268422e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2909:\n",
      "train loss: 2.8345235785854967e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2910:\n",
      "train loss: 2.495753763302973e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2911:\n",
      "train loss: 2.765139524199701e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2912:\n",
      "train loss: 2.4991838409462466e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2913:\n",
      "train loss: 2.7005103295504855e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2914:\n",
      "train loss: 2.3846442319812353e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2915:\n",
      "train loss: 2.840006325145331e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2916:\n",
      "train loss: 2.5274251879291273e-07\n",
      "Epoch 02918: reducing learning rate of group 0 to 3.1458e-08.\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2917:\n",
      "train loss: 2.725154492290836e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2918:\n",
      "train loss: 2.4480560113728764e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2919:\n",
      "train loss: 2.5125754013815236e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2920:\n",
      "train loss: 2.2309950864193503e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2921:\n",
      "train loss: 2.714075801537185e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2922:\n",
      "train loss: 2.3902446840103705e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2923:\n",
      "train loss: 2.629212316734126e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2924:\n",
      "train loss: 2.380268741054402e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2925:\n",
      "train loss: 2.574535444848596e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2926:\n",
      "train loss: 2.2711120192561787e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2927:\n",
      "train loss: 2.716832710956555e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2928:\n",
      "train loss: 2.426339161871749e-07\n",
      "Epoch 02930: reducing learning rate of group 0 to 2.9885e-08.\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2929:\n",
      "train loss: 2.579678979343812e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2930:\n",
      "train loss: 2.3175088238933797e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2931:\n",
      "train loss: 2.4106053114466294e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2932:\n",
      "train loss: 2.1391885935502464e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2933:\n",
      "train loss: 2.590445147605184e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2934:\n",
      "train loss: 2.3007986315390045e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2935:\n",
      "train loss: 2.47387746259566e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2936:\n",
      "train loss: 2.2347693374646444e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2937:\n",
      "train loss: 2.489142107774053e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2938:\n",
      "train loss: 2.2016793886218555e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2939:\n",
      "train loss: 2.564828639761078e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2940:\n",
      "train loss: 2.309918267302052e-07\n",
      "Epoch 02942: reducing learning rate of group 0 to 2.8391e-08.\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2941:\n",
      "train loss: 2.442235634851556e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2942:\n",
      "train loss: 2.1828487048023812e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2943:\n",
      "train loss: 2.330824286266273e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2944:\n",
      "train loss: 2.0819266100567393e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2945:\n",
      "train loss: 2.430727905811497e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2946:\n",
      "train loss: 2.1746325129654004e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2947:\n",
      "train loss: 2.3550979865341033e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2948:\n",
      "train loss: 2.1191493330229258e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2949:\n",
      "train loss: 2.389927046789163e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2950:\n",
      "train loss: 2.1339262344248e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2951:\n",
      "train loss: 2.395859295773322e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2952:\n",
      "train loss: 2.1590422661269347e-07\n",
      "Epoch 02954: reducing learning rate of group 0 to 2.6971e-08.\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2953:\n",
      "train loss: 2.3561364429490094e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2954:\n",
      "train loss: 2.1082956671725263e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2955:\n",
      "train loss: 2.191542759561936e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2956:\n",
      "train loss: 1.9674464158234664e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2957:\n",
      "train loss: 2.3185459906349294e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2958:\n",
      "train loss: 2.078906391387639e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2959:\n",
      "train loss: 2.2261108262495455e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2960:\n",
      "train loss: 2.0068509500951803e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2961:\n",
      "train loss: 2.2779670123559376e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2962:\n",
      "train loss: 2.0402828848184305e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2963:\n",
      "train loss: 2.2622506907413499e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2964:\n",
      "train loss: 2.0417681134713146e-07\n",
      "Epoch 02966: reducing learning rate of group 0 to 2.5623e-08.\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2965:\n",
      "train loss: 2.2453325443608672e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2966:\n",
      "train loss: 2.0120546806322821e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2967:\n",
      "train loss: 2.0718112226476143e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2968:\n",
      "train loss: 1.8615436184944853e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2969:\n",
      "train loss: 2.2106441066584157e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2970:\n",
      "train loss: 1.9895353558122218e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2971:\n",
      "train loss: 2.0933668840216478e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2972:\n",
      "train loss: 1.883443115061418e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2973:\n",
      "train loss: 2.1883714468296707e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2974:\n",
      "train loss: 1.9688342768113868e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2975:\n",
      "train loss: 2.1116051298420903e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2976:\n",
      "train loss: 1.9007936064821896e-07\n",
      "Epoch 02978: reducing learning rate of group 0 to 2.4342e-08.\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2977:\n",
      "train loss: 2.1711560603885043e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2978:\n",
      "train loss: 1.9532841222289185e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2979:\n",
      "train loss: 1.920772507824953e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2980:\n",
      "train loss: 1.7196500562593698e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2981:\n",
      "train loss: 2.1485145858745413e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2982:\n",
      "train loss: 1.9425086005253868e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2983:\n",
      "train loss: 1.9296813676405816e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2984:\n",
      "train loss: 1.727871795334898e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2985:\n",
      "train loss: 2.1399905113346413e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2986:\n",
      "train loss: 1.9346953445401138e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2987:\n",
      "train loss: 1.9359387649743628e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2988:\n",
      "train loss: 1.7336118352603162e-07\n",
      "Epoch 02990: reducing learning rate of group 0 to 2.3124e-08.\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2989:\n",
      "train loss: 2.1337437171930523e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2990:\n",
      "train loss: 1.928733939105207e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2991:\n",
      "train loss: 1.747447553276214e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2992:\n",
      "train loss: 1.55527772540435e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2993:\n",
      "train loss: 2.1174750056659927e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2994:\n",
      "train loss: 1.9220568845253977e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2995:\n",
      "train loss: 1.754172206760314e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2996:\n",
      "train loss: 1.5627476766097844e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2997:\n",
      "train loss: 2.1083935183376225e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2998:\n",
      "train loss: 1.9119064473572838e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2999:\n",
      "train loss: 1.7647901787367622e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 3000:\n",
      "train loss: 1.5743524802418248e-07\n",
      "Epoch 03002: reducing learning rate of group 0 to 2.1968e-08.\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3001:\n",
      "train loss: 2.0951407179822143e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3002:\n",
      "train loss: 1.8974798052204445e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3003:\n",
      "train loss: 1.596221871138439e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3004:\n",
      "train loss: 1.417082065638078e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3005:\n",
      "train loss: 2.065788627593785e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3006:\n",
      "train loss: 1.8752278014196042e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3007:\n",
      "train loss: 1.620670593640924e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3008:\n",
      "train loss: 1.4438478883800578e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3009:\n",
      "train loss: 2.0365527879119353e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3010:\n",
      "train loss: 1.8440604300641314e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3011:\n",
      "train loss: 1.6531510526190609e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 3012:\n",
      "train loss: 1.477650306085101e-07\n",
      "Epoch 03014: reducing learning rate of group 0 to 2.0870e-08.\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3013:\n",
      "train loss: 2.0014777219674118e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3014:\n",
      "train loss: 1.8081344846459286e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3015:\n",
      "train loss: 1.5148928312878123e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3016:\n",
      "train loss: 1.3496187073592768e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3017:\n",
      "train loss: 1.9530490478817997e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3018:\n",
      "train loss: 1.7671058240959034e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3019:\n",
      "train loss: 1.557640080068398e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3020:\n",
      "train loss: 1.3937690388590147e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3021:\n",
      "train loss: 1.908020503154212e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3022:\n",
      "train loss: 1.7216649943447588e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3023:\n",
      "train loss: 1.602992708974748e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 3024:\n",
      "train loss: 1.438852755199494e-07\n",
      "Epoch 03026: reducing learning rate of group 0 to 1.9826e-08.\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3025:\n",
      "train loss: 1.8636339775247633e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3026:\n",
      "train loss: 1.6782882345551527e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3027:\n",
      "train loss: 1.479194961190228e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3028:\n",
      "train loss: 1.3229203647912684e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3029:\n",
      "train loss: 1.8141406667622707e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3030:\n",
      "train loss: 1.637836782323475e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3031:\n",
      "train loss: 1.5195831243753636e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3032:\n",
      "train loss: 1.3630716835382248e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3033:\n",
      "train loss: 1.774693051598983e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3034:\n",
      "train loss: 1.5994443533498063e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3035:\n",
      "train loss: 1.556660118026875e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 3036:\n",
      "train loss: 1.3988875639417708e-07\n",
      "Epoch 03038: reducing learning rate of group 0 to 1.8835e-08.\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3037:\n",
      "train loss: 1.7403542151168138e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3038:\n",
      "train loss: 1.5668230231298255e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3039:\n",
      "train loss: 1.429922739654843e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3040:\n",
      "train loss: 1.2790263578813796e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3041:\n",
      "train loss: 1.7038008518999707e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3042:\n",
      "train loss: 1.539560301587923e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3043:\n",
      "train loss: 1.4564442177578463e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3044:\n",
      "train loss: 1.304891588225437e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3045:\n",
      "train loss: 1.6787364220887016e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3046:\n",
      "train loss: 1.5155663446901179e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3047:\n",
      "train loss: 1.479222580692848e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 3048:\n",
      "train loss: 1.3266539446177158e-07\n",
      "Epoch 03050: reducing learning rate of group 0 to 1.7893e-08.\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3049:\n",
      "train loss: 1.6579812428590868e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3050:\n",
      "train loss: 1.496020235787416e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3051:\n",
      "train loss: 1.3478889489001642e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3052:\n",
      "train loss: 1.2022192327479293e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3053:\n",
      "train loss: 1.6335809133740254e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3054:\n",
      "train loss: 1.4802261443895752e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3055:\n",
      "train loss: 1.3630614854237106e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3056:\n",
      "train loss: 1.216925279614397e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3057:\n",
      "train loss: 1.619315448023413e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3058:\n",
      "train loss: 1.4666058936052053e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3059:\n",
      "train loss: 1.3759090111741112e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 3060:\n",
      "train loss: 1.2291929193915926e-07\n",
      "Epoch 03062: reducing learning rate of group 0 to 1.6999e-08.\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3061:\n",
      "train loss: 1.6075360417409627e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3062:\n",
      "train loss: 1.4554817149274303e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3063:\n",
      "train loss: 1.2442261362088365e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3064:\n",
      "train loss: 1.1044494696288181e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3065:\n",
      "train loss: 1.5905947591683217e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3066:\n",
      "train loss: 1.4464211100226617e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3067:\n",
      "train loss: 1.2528882868456466e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3068:\n",
      "train loss: 1.1128611107060388e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3069:\n",
      "train loss: 1.5823459160384035e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3070:\n",
      "train loss: 1.4384961400927235e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3071:\n",
      "train loss: 1.260365857354752e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 3072:\n",
      "train loss: 1.1200521597151365e-07\n",
      "Epoch 03074: reducing learning rate of group 0 to 1.6149e-08.\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3073:\n",
      "train loss: 1.5753277012883882e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3074:\n",
      "train loss: 1.4317935802448943e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3075:\n",
      "train loss: 1.131741206121973e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3076:\n",
      "train loss: 9.982792724298858e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3077:\n",
      "train loss: 1.5622850845634822e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3078:\n",
      "train loss: 1.4260075442886747e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3079:\n",
      "train loss: 1.137325180100718e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3080:\n",
      "train loss: 1.003794521989293e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3081:\n",
      "train loss: 1.5567301496976076e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3082:\n",
      "train loss: 1.420555331758638e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3083:\n",
      "train loss: 1.142552843293449e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 3084:\n",
      "train loss: 1.008937556757668e-07\n",
      "Epoch 03086: reducing learning rate of group 0 to 1.5341e-08.\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3085:\n",
      "train loss: 1.5515509337642558e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3086:\n",
      "train loss: 1.4154725177015348e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3087:\n",
      "train loss: 1.0193001992281102e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3088:\n",
      "train loss: 8.923982652067726e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3089:\n",
      "train loss: 1.539800145476124e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3090:\n",
      "train loss: 1.4103912858597444e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3091:\n",
      "train loss: 1.0243871857130376e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3092:\n",
      "train loss: 8.97620931976018e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3093:\n",
      "train loss: 1.5343082541807126e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3094:\n",
      "train loss: 1.4047744872644212e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3095:\n",
      "train loss: 1.0300004954320441e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3096:\n",
      "train loss: 9.033705153494733e-08\n",
      "Epoch 03098: reducing learning rate of group 0 to 1.4574e-08.\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3097:\n",
      "train loss: 1.5282717206399195e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3098:\n",
      "train loss: 1.3985884870313606e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3099:\n",
      "train loss: 9.145055704649448e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3100:\n",
      "train loss: 7.945656906321295e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3101:\n",
      "train loss: 1.5147417298289959e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3102:\n",
      "train loss: 1.3909572605481844e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3103:\n",
      "train loss: 9.225589693334965e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3104:\n",
      "train loss: 8.032160497990661e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3105:\n",
      "train loss: 1.505246387334478e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3106:\n",
      "train loss: 1.3808095793880585e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3107:\n",
      "train loss: 9.331891395487465e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3108:\n",
      "train loss: 8.145352157240186e-08\n",
      "Epoch 03110: reducing learning rate of group 0 to 1.3845e-08.\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3109:\n",
      "train loss: 1.4929327024054464e-07\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3110:\n",
      "train loss: 1.3676925390459036e-07\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3111:\n",
      "train loss: 8.312880360740582e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3112:\n",
      "train loss: 7.199514039088552e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3113:\n",
      "train loss: 1.4697021877861767e-07\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3114:\n",
      "train loss: 1.3486189235174143e-07\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3115:\n",
      "train loss: 8.5214217613755e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3116:\n",
      "train loss: 7.429319500462477e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3117:\n",
      "train loss: 1.4439579666538053e-07\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3118:\n",
      "train loss: 1.3205004616082578e-07\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3119:\n",
      "train loss: 8.822714498077682e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3120:\n",
      "train loss: 7.753836002058964e-08\n",
      "Epoch 03122: reducing learning rate of group 0 to 1.3153e-08.\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3121:\n",
      "train loss: 1.4085730653591732e-07\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3122:\n",
      "train loss: 1.282515251615097e-07\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3123:\n",
      "train loss: 8.124695022514059e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3124:\n",
      "train loss: 7.146557856202381e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3125:\n",
      "train loss: 1.3541766335711616e-07\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3126:\n",
      "train loss: 1.229062162754379e-07\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3127:\n",
      "train loss: 8.704741985137163e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3128:\n",
      "train loss: 7.767716784766091e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3129:\n",
      "train loss: 1.2882401478871747e-07\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3130:\n",
      "train loss: 1.1596685970327928e-07\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3131:\n",
      "train loss: 9.425725980286739e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3132:\n",
      "train loss: 8.505415622082705e-08\n",
      "Epoch 03134: reducing learning rate of group 0 to 1.2496e-08.\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3133:\n",
      "train loss: 1.2139988594078882e-07\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3134:\n",
      "train loss: 1.0849425305669087e-07\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3135:\n",
      "train loss: 9.125457582095193e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3136:\n",
      "train loss: 8.253873250353305e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3137:\n",
      "train loss: 1.1354911340336067e-07\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3138:\n",
      "train loss: 1.0121670203001501e-07\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3139:\n",
      "train loss: 9.854114495260404e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3140:\n",
      "train loss: 8.968151100895147e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3141:\n",
      "train loss: 1.0674841781919882e-07\n",
      "Epoch 03143: reducing learning rate of group 0 to 1.1871e-08.\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3142:\n",
      "train loss: 9.476396193216711e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3143:\n",
      "train loss: 1.046007905506068e-07\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3144:\n",
      "train loss: 9.570321226496547e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3145:\n",
      "train loss: 9.158102531291463e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3146:\n",
      "train loss: 8.092149455712089e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3147:\n",
      "train loss: 1.076532466579359e-07\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3148:\n",
      "train loss: 9.794965022285508e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3149:\n",
      "train loss: 9.009310890005969e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3150:\n",
      "train loss: 8.012790817998914e-08\n",
      "Epoch 03152: reducing learning rate of group 0 to 1.1277e-08.\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3151:\n",
      "train loss: 1.0785035690482938e-07\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3152:\n",
      "train loss: 9.762721777425491e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3153:\n",
      "train loss: 8.14864809387715e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3154:\n",
      "train loss: 7.251377843055504e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3155:\n",
      "train loss: 1.0555027700923192e-07\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3156:\n",
      "train loss: 9.536826288423925e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3157:\n",
      "train loss: 8.414780678217037e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3158:\n",
      "train loss: 7.547751756730318e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3159:\n",
      "train loss: 1.024220051630048e-07\n",
      "Epoch 03161: reducing learning rate of group 0 to 1.0713e-08.\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3160:\n",
      "train loss: 9.213048634910702e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3161:\n",
      "train loss: 8.744948585693434e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3162:\n",
      "train loss: 7.91960039490123e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3163:\n",
      "train loss: 8.995850187495533e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3164:\n",
      "train loss: 8.036259217747461e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3165:\n",
      "train loss: 9.002027011232854e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3166:\n",
      "train loss: 8.15357024021002e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3167:\n",
      "train loss: 8.786268326284926e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3168:\n",
      "train loss: 7.850924316899884e-08\n",
      "Epoch 03170: reducing learning rate of group 0 to 1.0178e-08.\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3169:\n",
      "train loss: 9.165112879377446e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3170:\n",
      "train loss: 8.296975727902259e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3171:\n",
      "train loss: 7.815351491297955e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3172:\n",
      "train loss: 6.948002227518228e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3173:\n",
      "train loss: 9.194846944413047e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3174:\n",
      "train loss: 8.3494595463531e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3175:\n",
      "train loss: 7.781859141105708e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3176:\n",
      "train loss: 6.931817920914527e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3177:\n",
      "train loss: 9.196862455599226e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3178:\n",
      "train loss: 8.34029059859561e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3179:\n",
      "train loss: 7.800342942154962e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3180:\n",
      "train loss: 6.958748606528225e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3181:\n",
      "train loss: 9.163447437675336e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3182:\n",
      "train loss: 8.302360543322969e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3183:\n",
      "train loss: 7.841616112563111e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3184:\n",
      "train loss: 7.00315919093863e-08\n",
      "Epoch 03186: reducing learning rate of group 0 to 9.6688e-09.\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3185:\n",
      "train loss: 9.116933022171032e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3186:\n",
      "train loss: 8.255043106434321e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3187:\n",
      "train loss: 7.0822538152913e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3188:\n",
      "train loss: 6.287446038592655e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3189:\n",
      "train loss: 9.02394585567992e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3190:\n",
      "train loss: 8.203160669463225e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3191:\n",
      "train loss: 7.135400043161384e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3192:\n",
      "train loss: 6.34180728818644e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3193:\n",
      "train loss: 8.968903313700503e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3194:\n",
      "train loss: 8.148377680435139e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3195:\n",
      "train loss: 7.189289264963119e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3196:\n",
      "train loss: 6.395088400284253e-08\n",
      "Epoch 03198: reducing learning rate of group 0 to 9.1854e-09.\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3197:\n",
      "train loss: 8.916311983039014e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3198:\n",
      "train loss: 8.097134635110769e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3199:\n",
      "train loss: 6.472180735839386e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3200:\n",
      "train loss: 5.7173157870323136e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3201:\n",
      "train loss: 8.827826376361613e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3202:\n",
      "train loss: 8.04939868836694e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3203:\n",
      "train loss: 6.519603623778035e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3204:\n",
      "train loss: 5.764699648597556e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3205:\n",
      "train loss: 8.780410840624499e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3206:\n",
      "train loss: 8.002568105737982e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3207:\n",
      "train loss: 6.565360086801097e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3208:\n",
      "train loss: 5.809796327577772e-08\n",
      "Epoch 03210: reducing learning rate of group 0 to 8.7261e-09.\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3209:\n",
      "train loss: 8.735717124719814e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3210:\n",
      "train loss: 7.958778470568167e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3211:\n",
      "train loss: 5.879681343213004e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3212:\n",
      "train loss: 5.1619139114815894e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3213:\n",
      "train loss: 8.655091998960801e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3214:\n",
      "train loss: 7.91625750566205e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3215:\n",
      "train loss: 5.9224089811389344e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3216:\n",
      "train loss: 5.2052169179599955e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3217:\n",
      "train loss: 8.61088910622461e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3218:\n",
      "train loss: 7.87174868919195e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3219:\n",
      "train loss: 5.966703675062239e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3220:\n",
      "train loss: 5.249756889629623e-08\n",
      "Epoch 03222: reducing learning rate of group 0 to 8.2898e-09.\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3221:\n",
      "train loss: 8.565658862497779e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3222:\n",
      "train loss: 7.826352145225982e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3223:\n",
      "train loss: 5.320091204066824e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3224:\n",
      "train loss: 4.640312854625124e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3225:\n",
      "train loss: 8.481222281213353e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3226:\n",
      "train loss: 7.77642496414596e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3227:\n",
      "train loss: 5.37177014594281e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3228:\n",
      "train loss: 4.694247948945581e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3229:\n",
      "train loss: 8.424360117323921e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3230:\n",
      "train loss: 7.717414303038323e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3231:\n",
      "train loss: 5.432224861652396e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3232:\n",
      "train loss: 4.756774583692861e-08\n",
      "Epoch 03234: reducing learning rate of group 0 to 7.8753e-09.\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3233:\n",
      "train loss: 8.358913534284988e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3234:\n",
      "train loss: 7.649769989433129e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3235:\n",
      "train loss: 4.8441774718107126e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3236:\n",
      "train loss: 4.206864702585159e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3237:\n",
      "train loss: 8.24505009575151e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3238:\n",
      "train loss: 7.564563256758942e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3239:\n",
      "train loss: 4.934928492906493e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3240:\n",
      "train loss: 4.3040553655378615e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3241:\n",
      "train loss: 8.139881723236824e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3242:\n",
      "train loss: 7.452823948352668e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3243:\n",
      "train loss: 5.0519213111122725e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3244:\n",
      "train loss: 4.427318485043539e-08\n",
      "Epoch 03246: reducing learning rate of group 0 to 7.4815e-09.\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3245:\n",
      "train loss: 8.008625919736317e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3246:\n",
      "train loss: 7.314867304377883e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3247:\n",
      "train loss: 4.5705032903327414e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3248:\n",
      "train loss: 3.9881589495188204e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3249:\n",
      "train loss: 7.807328481665601e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3250:\n",
      "train loss: 7.131946860936868e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3251:\n",
      "train loss: 4.7669878703495045e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3252:\n",
      "train loss: 4.198665353696799e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3253:\n",
      "train loss: 7.581324166048686e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3254:\n",
      "train loss: 6.892801125960707e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3255:\n",
      "train loss: 5.0165623044719015e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3256:\n",
      "train loss: 4.458267021486764e-08\n",
      "Epoch 03258: reducing learning rate of group 0 to 7.1075e-09.\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3257:\n",
      "train loss: 7.311532077362064e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3258:\n",
      "train loss: 6.614271749922729e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3259:\n",
      "train loss: 4.707185160748598e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3260:\n",
      "train loss: 4.189087014463081e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3261:\n",
      "train loss: 6.972528386102152e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3262:\n",
      "train loss: 6.292087998942092e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3263:\n",
      "train loss: 5.0434862113920224e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3264:\n",
      "train loss: 4.535158399372092e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3265:\n",
      "train loss: 6.621722718830002e-08\n",
      "Epoch 03267: reducing learning rate of group 0 to 6.7521e-09.\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3266:\n",
      "train loss: 5.937864071149062e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3267:\n",
      "train loss: 5.397876976921785e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3268:\n",
      "train loss: 4.9086955189127194e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3269:\n",
      "train loss: 5.7066955211613956e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3270:\n",
      "train loss: 5.0738180192487204e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3271:\n",
      "train loss: 5.6739917446389004e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3272:\n",
      "train loss: 5.1603051546305515e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3273:\n",
      "train loss: 5.482030652836616e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3274:\n",
      "train loss: 4.8751754348676805e-08\n",
      "Epoch 03276: reducing learning rate of group 0 to 6.4145e-09.\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3275:\n",
      "train loss: 5.8464267970492965e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3276:\n",
      "train loss: 5.306835135196684e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3277:\n",
      "train loss: 4.830359705131215e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3278:\n",
      "train loss: 4.2837754508272725e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3279:\n",
      "train loss: 5.866220353711629e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3280:\n",
      "train loss: 5.318584470761428e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3281:\n",
      "train loss: 4.851504618899337e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3282:\n",
      "train loss: 4.333887542467791e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3283:\n",
      "train loss: 5.7917603610635904e-08\n",
      "Epoch 03285: reducing learning rate of group 0 to 6.0938e-09.\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3284:\n",
      "train loss: 5.222837344330678e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3285:\n",
      "train loss: 4.9652265312688144e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3286:\n",
      "train loss: 4.484457638047083e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3287:\n",
      "train loss: 5.1335239912253825e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3288:\n",
      "train loss: 4.5940067282135754e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3289:\n",
      "train loss: 5.079335912214856e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3290:\n",
      "train loss: 4.5901978021155696e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3291:\n",
      "train loss: 5.0381863171808835e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3292:\n",
      "train loss: 4.508907782476127e-08\n",
      "Epoch 03294: reducing learning rate of group 0 to 5.7891e-09.\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3293:\n",
      "train loss: 5.154662830190245e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3294:\n",
      "train loss: 4.65575460441674e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3295:\n",
      "train loss: 4.502989952700057e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3296:\n",
      "train loss: 4.0142889047744314e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3297:\n",
      "train loss: 5.148452440919138e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3298:\n",
      "train loss: 4.656283254597631e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3299:\n",
      "train loss: 4.5202184199215574e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3300:\n",
      "train loss: 4.046945612190788e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3301:\n",
      "train loss: 5.103569252045507e-08\n",
      "Epoch 03303: reducing learning rate of group 0 to 5.4996e-09.\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3302:\n",
      "train loss: 4.601006236444815e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3303:\n",
      "train loss: 4.584359815471909e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3304:\n",
      "train loss: 4.139731032636612e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3305:\n",
      "train loss: 4.554298102775622e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3306:\n",
      "train loss: 4.0796988218659233e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3307:\n",
      "train loss: 4.6407810530364525e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3308:\n",
      "train loss: 4.189248095320864e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3309:\n",
      "train loss: 4.512231365058859e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3310:\n",
      "train loss: 4.044742907295277e-08\n",
      "Epoch 03312: reducing learning rate of group 0 to 5.2246e-09.\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3311:\n",
      "train loss: 4.669530053583703e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3312:\n",
      "train loss: 4.212303301165391e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3313:\n",
      "train loss: 4.0607980940311904e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3314:\n",
      "train loss: 3.6249882875477716e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3315:\n",
      "train loss: 4.643080042050836e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3316:\n",
      "train loss: 4.198023513013115e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3317:\n",
      "train loss: 4.085379071322068e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3318:\n",
      "train loss: 3.6584639578770447e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3319:\n",
      "train loss: 4.602774389562378e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3320:\n",
      "train loss: 4.1521440672420175e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3321:\n",
      "train loss: 4.135891048914114e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3322:\n",
      "train loss: 3.7126140826235035e-08\n",
      "Epoch 03324: reducing learning rate of group 0 to 4.9634e-09.\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3323:\n",
      "train loss: 4.546551488201818e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3324:\n",
      "train loss: 4.094584141303236e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3325:\n",
      "train loss: 3.780738628129309e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3326:\n",
      "train loss: 3.3815871814507675e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3327:\n",
      "train loss: 4.45997625953886e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3328:\n",
      "train loss: 4.0258754950043414e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3329:\n",
      "train loss: 3.853644338749791e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3330:\n",
      "train loss: 3.4575673011086645e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3331:\n",
      "train loss: 4.382601343514317e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3332:\n",
      "train loss: 3.947884276719781e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3333:\n",
      "train loss: 3.931589774540306e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3334:\n",
      "train loss: 3.5348638311969664e-08\n",
      "Epoch 03336: reducing learning rate of group 0 to 4.7152e-09.\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3335:\n",
      "train loss: 4.307021632403056e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3336:\n",
      "train loss: 3.874369203665184e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3337:\n",
      "train loss: 3.6096452252739325e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3338:\n",
      "train loss: 3.2324962845666175e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3339:\n",
      "train loss: 4.215982241903327e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3340:\n",
      "train loss: 3.8033621515426996e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3341:\n",
      "train loss: 3.681995818418676e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3342:\n",
      "train loss: 3.3055199191985134e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3343:\n",
      "train loss: 4.143385372356979e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3344:\n",
      "train loss: 3.731675028531755e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3345:\n",
      "train loss: 3.7523569332411235e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3346:\n",
      "train loss: 3.374260859443489e-08\n",
      "Epoch 03348: reducing learning rate of group 0 to 4.4795e-09.\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3347:\n",
      "train loss: 4.076914363268126e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3348:\n",
      "train loss: 3.6676740698605764e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3349:\n",
      "train loss: 3.44027593060422e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3350:\n",
      "train loss: 3.0803563706073257e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3351:\n",
      "train loss: 3.997518374277039e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3352:\n",
      "train loss: 3.607714114157535e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3353:\n",
      "train loss: 3.501095338757757e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3354:\n",
      "train loss: 3.141595714255578e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3355:\n",
      "train loss: 3.936629981791341e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3356:\n",
      "train loss: 3.5475802275781403e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3357:\n",
      "train loss: 3.560126371758123e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3358:\n",
      "train loss: 3.199354688997958e-08\n",
      "Epoch 03360: reducing learning rate of group 0 to 4.2555e-09.\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3359:\n",
      "train loss: 3.880589699509836e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3360:\n",
      "train loss: 3.493451565893335e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3361:\n",
      "train loss: 3.257431639333012e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3362:\n",
      "train loss: 2.9143243508137655e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3363:\n",
      "train loss: 3.810630150624334e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3364:\n",
      "train loss: 3.441665183407174e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3365:\n",
      "train loss: 3.3102097092685795e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3366:\n",
      "train loss: 2.967753333311744e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3367:\n",
      "train loss: 3.757156745098847e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3368:\n",
      "train loss: 3.3885451892282225e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3369:\n",
      "train loss: 3.362623554425963e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3370:\n",
      "train loss: 3.0193360753241346e-08\n",
      "Epoch 03372: reducing learning rate of group 0 to 4.0427e-09.\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3371:\n",
      "train loss: 3.706754581824476e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3372:\n",
      "train loss: 3.339539652133505e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3373:\n",
      "train loss: 3.073026794834029e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3374:\n",
      "train loss: 2.7468585762031304e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3375:\n",
      "train loss: 3.64168086157137e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3376:\n",
      "train loss: 3.291481510084107e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3377:\n",
      "train loss: 3.122187560067464e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3378:\n",
      "train loss: 2.7968307196971458e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3379:\n",
      "train loss: 3.5914107709297625e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3380:\n",
      "train loss: 3.241315398874978e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3381:\n",
      "train loss: 3.171879863628757e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3382:\n",
      "train loss: 2.845945955662518e-08\n",
      "Epoch 03384: reducing learning rate of group 0 to 3.8406e-09.\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3383:\n",
      "train loss: 3.5431655220444846e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3384:\n",
      "train loss: 3.194172974186085e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3385:\n",
      "train loss: 2.8975252882061122e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3386:\n",
      "train loss: 2.5880107840745715e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3387:\n",
      "train loss: 3.480275175747892e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3388:\n",
      "train loss: 3.147331136126717e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3389:\n",
      "train loss: 2.945484059089086e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3390:\n",
      "train loss: 2.6368266305981597e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3391:\n",
      "train loss: 3.431055983178527e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3392:\n",
      "train loss: 3.098110511549857e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3393:\n",
      "train loss: 2.9943229593789554e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3394:\n",
      "train loss: 2.6851957136404158e-08\n",
      "Epoch 03396: reducing learning rate of group 0 to 3.6486e-09.\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3395:\n",
      "train loss: 3.383411639445421e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3396:\n",
      "train loss: 3.051431338443185e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3397:\n",
      "train loss: 2.735624930296966e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3398:\n",
      "train loss: 2.442136803364015e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3399:\n",
      "train loss: 3.321635610755826e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3400:\n",
      "train loss: 3.0048618480630645e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3401:\n",
      "train loss: 2.7832743158144654e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3402:\n",
      "train loss: 2.4906276329539204e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3403:\n",
      "train loss: 3.272717081744842e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3404:\n",
      "train loss: 2.9559151892308753e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3405:\n",
      "train loss: 2.8318576026335314e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3406:\n",
      "train loss: 2.5387704094114112e-08\n",
      "Epoch 03408: reducing learning rate of group 0 to 3.4661e-09.\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3407:\n",
      "train loss: 3.225243483452486e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3408:\n",
      "train loss: 2.9093538016290456e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3409:\n",
      "train loss: 2.588292793343868e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3410:\n",
      "train loss: 2.3100471166062778e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3411:\n",
      "train loss: 3.1643731841895e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3412:\n",
      "train loss: 2.8629224735355187e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3413:\n",
      "train loss: 2.6357488950552317e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3414:\n",
      "train loss: 2.3583044649757698e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3415:\n",
      "train loss: 3.1157023572697785e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3416:\n",
      "train loss: 2.8142348780037925e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3417:\n",
      "train loss: 2.684056273261606e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3418:\n",
      "train loss: 2.4061625278598017e-08\n",
      "Epoch 03420: reducing learning rate of group 0 to 3.2928e-09.\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3419:\n",
      "train loss: 3.068504361335849e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3420:\n",
      "train loss: 2.7679402967143432e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3421:\n",
      "train loss: 2.454685327312447e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3422:\n",
      "train loss: 2.190849517772132e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3423:\n",
      "train loss: 3.008728012397311e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3424:\n",
      "train loss: 2.7218871468630612e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3425:\n",
      "train loss: 2.501703129515742e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3426:\n",
      "train loss: 2.2386187297307472e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3427:\n",
      "train loss: 2.9605777118683916e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3428:\n",
      "train loss: 2.6737494798502444e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3429:\n",
      "train loss: 2.5494267261106723e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3430:\n",
      "train loss: 2.285866277456132e-08\n",
      "Epoch 03432: reducing learning rate of group 0 to 3.1282e-09.\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3431:\n",
      "train loss: 2.9140039374857012e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3432:\n",
      "train loss: 2.628093017368509e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3433:\n",
      "train loss: 2.3331450964358283e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3434:\n",
      "train loss: 2.0828870243576314e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3435:\n",
      "train loss: 2.8556847721283137e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3436:\n",
      "train loss: 2.582820572522003e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3437:\n",
      "train loss: 2.3793221172122926e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3438:\n",
      "train loss: 2.1297644473719176e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3439:\n",
      "train loss: 2.8084642564798216e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3440:\n",
      "train loss: 2.5356487221358732e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3441:\n",
      "train loss: 2.4260399987281332e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3442:\n",
      "train loss: 2.1759732965125478e-08\n",
      "Epoch 03444: reducing learning rate of group 0 to 2.9718e-09.\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3443:\n",
      "train loss: 2.7629505465675687e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3444:\n",
      "train loss: 2.491074820220278e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3445:\n",
      "train loss: 2.221717283325606e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3446:\n",
      "train loss: 1.984235195301024e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3447:\n",
      "train loss: 2.7064918269216265e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3448:\n",
      "train loss: 2.4470154185869554e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3449:\n",
      "train loss: 2.266626521858072e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3450:\n",
      "train loss: 2.0297991707124695e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3451:\n",
      "train loss: 2.6606179339126883e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3452:\n",
      "train loss: 2.401223106087747e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3453:\n",
      "train loss: 2.311926957841712e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3454:\n",
      "train loss: 2.0745607109546844e-08\n",
      "Epoch 03456: reducing learning rate of group 0 to 2.8232e-09.\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3455:\n",
      "train loss: 2.616569550704316e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3456:\n",
      "train loss: 2.3581354142434508e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3457:\n",
      "train loss: 2.1185147260088297e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3458:\n",
      "train loss: 1.893059327155133e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3459:\n",
      "train loss: 2.5623102482702057e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3460:\n",
      "train loss: 2.315647169879927e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3461:\n",
      "train loss: 2.161809318509274e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3462:\n",
      "train loss: 1.9369754858693655e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3463:\n",
      "train loss: 2.5181039153984837e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3464:\n",
      "train loss: 2.2715453129774696e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3465:\n",
      "train loss: 2.205388251179314e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3466:\n",
      "train loss: 1.9799939298010396e-08\n",
      "Epoch 03468: reducing learning rate of group 0 to 2.6820e-09.\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3467:\n",
      "train loss: 2.47580667174886e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3468:\n",
      "train loss: 2.230223002875493e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3469:\n",
      "train loss: 2.0219972005998415e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3470:\n",
      "train loss: 1.807886473829251e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3471:\n",
      "train loss: 2.4239486013437227e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3472:\n",
      "train loss: 2.1895190772360292e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3473:\n",
      "train loss: 2.0634803759083742e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3474:\n",
      "train loss: 1.8499745981766453e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3475:\n",
      "train loss: 2.3815723277570924e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3476:\n",
      "train loss: 2.1472563413110025e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3477:\n",
      "train loss: 2.105196808125148e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3478:\n",
      "train loss: 1.8911176728370477e-08\n",
      "Epoch 03480: reducing learning rate of group 0 to 2.5479e-09.\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3479:\n",
      "train loss: 2.3411478694587402e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3480:\n",
      "train loss: 2.1078150169969327e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3481:\n",
      "train loss: 1.9311285781000093e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3482:\n",
      "train loss: 1.7277495624446293e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3483:\n",
      "train loss: 2.2917321597007707e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3484:\n",
      "train loss: 2.0689429380584902e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3485:\n",
      "train loss: 1.9707711489270246e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3486:\n",
      "train loss: 1.7679989329521677e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3487:\n",
      "train loss: 2.251177562290358e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3488:\n",
      "train loss: 2.028498872335072e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3489:\n",
      "train loss: 2.010649469126959e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3490:\n",
      "train loss: 1.8072952122652737e-08\n",
      "Epoch 03492: reducing learning rate of group 0 to 2.4205e-09.\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3491:\n",
      "train loss: 2.2125919542335942e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3492:\n",
      "train loss: 1.9909040153524427e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3493:\n",
      "train loss: 1.8453767972968128e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3494:\n",
      "train loss: 1.6521767511067803e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3495:\n",
      "train loss: 2.1655238340222504e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3496:\n",
      "train loss: 1.9537728290314543e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3497:\n",
      "train loss: 1.883289029323874e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3498:\n",
      "train loss: 1.6907149767291467e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3499:\n",
      "train loss: 2.126646566556753e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3500:\n",
      "train loss: 1.9149939569240118e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3501:\n",
      "train loss: 1.9214781740019602e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3502:\n",
      "train loss: 1.72830834446713e-08\n",
      "Epoch 03504: reducing learning rate of group 0 to 2.2995e-09.\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3503:\n",
      "train loss: 2.0897593595350407e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3504:\n",
      "train loss: 1.8791158038808446e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3505:\n",
      "train loss: 1.764585868664678e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3506:\n",
      "train loss: 1.5810527391508902e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3507:\n",
      "train loss: 2.044869328519856e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3508:\n",
      "train loss: 1.8435558448107864e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3509:\n",
      "train loss: 1.80095720031694e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3510:\n",
      "train loss: 1.618084846830298e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3511:\n",
      "train loss: 2.0074513355298035e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3512:\n",
      "train loss: 1.806221065118492e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3513:\n",
      "train loss: 1.8376625161847673e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3514:\n",
      "train loss: 1.6541629556063115e-08\n",
      "Epoch 03516: reducing learning rate of group 0 to 2.1845e-09.\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3515:\n",
      "train loss: 1.972092696746012e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3516:\n",
      "train loss: 1.7719156301389614e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3517:\n",
      "train loss: 1.6887614885625553e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3518:\n",
      "train loss: 1.5143976935370882e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3519:\n",
      "train loss: 1.9292236081265784e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3520:\n",
      "train loss: 1.7377626481828093e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3521:\n",
      "train loss: 1.7237792537403043e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3522:\n",
      "train loss: 1.5501253831042393e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3523:\n",
      "train loss: 1.893055303950189e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3524:\n",
      "train loss: 1.7016668152654858e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3525:\n",
      "train loss: 1.7591750970138022e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3526:\n",
      "train loss: 1.584824669397676e-08\n",
      "Epoch 03528: reducing learning rate of group 0 to 2.0753e-09.\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3527:\n",
      "train loss: 1.8591295463877516e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3528:\n",
      "train loss: 1.6688866514666154e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3529:\n",
      "train loss: 1.6177860695798192e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3530:\n",
      "train loss: 1.4520726320683602e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3531:\n",
      "train loss: 1.818252571120623e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3532:\n",
      "train loss: 1.636091520430422e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3533:\n",
      "train loss: 1.651532307565321e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3534:\n",
      "train loss: 1.486591410236461e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3535:\n",
      "train loss: 1.7832405443375052e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3536:\n",
      "train loss: 1.6011554784891283e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3537:\n",
      "train loss: 1.6856474476906675e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3538:\n",
      "train loss: 1.519873287121951e-08\n",
      "Epoch 03540: reducing learning rate of group 0 to 1.9715e-09.\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3539:\n",
      "train loss: 1.7508646321270645e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3540:\n",
      "train loss: 1.5700994510336243e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3541:\n",
      "train loss: 1.5510479906442583e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3542:\n",
      "train loss: 1.3933961731298237e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3543:\n",
      "train loss: 1.71223342589798e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3544:\n",
      "train loss: 1.5388687431926593e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3545:\n",
      "train loss: 1.5833754992912153e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3546:\n",
      "train loss: 1.4265778112871507e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3547:\n",
      "train loss: 1.678524831546414e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3548:\n",
      "train loss: 1.5052635324251378e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3549:\n",
      "train loss: 1.6159736231697825e-08\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3550:\n",
      "train loss: 1.4580995629089324e-08\n",
      "Epoch 03552: reducing learning rate of group 0 to 1.8730e-09.\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3551:\n",
      "train loss: 1.6481782894415803e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3552:\n",
      "train loss: 1.4765341305973906e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3553:\n",
      "train loss: 1.487049707490637e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3554:\n",
      "train loss: 1.336758038984477e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3555:\n",
      "train loss: 1.6124573114297875e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3556:\n",
      "train loss: 1.4474287435836222e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3557:\n",
      "train loss: 1.5175316198091976e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3558:\n",
      "train loss: 1.3682241755472261e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3559:\n",
      "train loss: 1.5804645526592258e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3560:\n",
      "train loss: 1.415584341547992e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3561:\n",
      "train loss: 1.5481378618025506e-08\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3562:\n",
      "train loss: 1.3973811220397262e-08\n",
      "Epoch 03564: reducing learning rate of group 0 to 1.7793e-09.\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3563:\n",
      "train loss: 1.5529556371054484e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3564:\n",
      "train loss: 1.3901361879376279e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3565:\n",
      "train loss: 1.4234887580476073e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3566:\n",
      "train loss: 1.2797839742021437e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3567:\n",
      "train loss: 1.521039293418202e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3568:\n",
      "train loss: 1.3638411629457435e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3569:\n",
      "train loss: 1.4517339204831844e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3570:\n",
      "train loss: 1.3092841848022721e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3571:\n",
      "train loss: 1.491043115473711e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3572:\n",
      "train loss: 1.3340129778425438e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3573:\n",
      "train loss: 1.4801461591955215e-08\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3574:\n",
      "train loss: 1.3357833937103201e-08\n",
      "Epoch 03576: reducing learning rate of group 0 to 1.6903e-09.\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3575:\n",
      "train loss: 1.4669027401378158e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3576:\n",
      "train loss: 1.3124999393780644e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3577:\n",
      "train loss: 1.3588060716940173e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3578:\n",
      "train loss: 1.2210659013399237e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3579:\n",
      "train loss: 1.4391426680637538e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3580:\n",
      "train loss: 1.2890934835036787e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3581:\n",
      "train loss: 1.3851728636805597e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3582:\n",
      "train loss: 1.2491984515689161e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3583:\n",
      "train loss: 1.4105813442829686e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3584:\n",
      "train loss: 1.260697462099275e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3585:\n",
      "train loss: 1.4121058003808177e-08\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3586:\n",
      "train loss: 1.2737472091518393e-08\n",
      "Epoch 03588: reducing learning rate of group 0 to 1.6058e-09.\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3587:\n",
      "train loss: 1.3893143550060666e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3588:\n",
      "train loss: 1.2427353901912279e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3589:\n",
      "train loss: 1.2941813593237336e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3590:\n",
      "train loss: 1.162122955827569e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3591:\n",
      "train loss: 1.3651560921296406e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3592:\n",
      "train loss: 1.22168054650702e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3593:\n",
      "train loss: 1.319371290603694e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3594:\n",
      "train loss: 1.1897131666243531e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3595:\n",
      "train loss: 1.3372786921145602e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3596:\n",
      "train loss: 1.1940786973945241e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3597:\n",
      "train loss: 1.345502450459053e-08\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3598:\n",
      "train loss: 1.2131141166941136e-08\n",
      "Epoch 03600: reducing learning rate of group 0 to 1.5255e-09.\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3599:\n",
      "train loss: 1.3180107791125465e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3600:\n",
      "train loss: 1.178797913187153e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3601:\n",
      "train loss: 1.2314869789940688e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3602:\n",
      "train loss: 1.1051150919209861e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3603:\n",
      "train loss: 1.2968038958435372e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3604:\n",
      "train loss: 1.1599988099743093e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3605:\n",
      "train loss: 1.2550852634600773e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3606:\n",
      "train loss: 1.1316027070377406e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3607:\n",
      "train loss: 1.2700457605981219e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3608:\n",
      "train loss: 1.1336615996309242e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3609:\n",
      "train loss: 1.2799973751370267e-08\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3610:\n",
      "train loss: 1.1538938289561879e-08\n",
      "Epoch 03612: reducing learning rate of group 0 to 1.4493e-09.\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3611:\n",
      "train loss: 1.2519848638008892e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3612:\n",
      "train loss: 1.1198115923362587e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3613:\n",
      "train loss: 1.1708234207632677e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3614:\n",
      "train loss: 1.0505215447621467e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3615:\n",
      "train loss: 1.2325875755085534e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3616:\n",
      "train loss: 1.1028968772962115e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3617:\n",
      "train loss: 1.1921889636735174e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3618:\n",
      "train loss: 1.0748281882961535e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3619:\n",
      "train loss: 1.2077677513292465e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3620:\n",
      "train loss: 1.0785388621568589e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3621:\n",
      "train loss: 1.215158422768062e-08\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3622:\n",
      "train loss: 1.0957174320520428e-08\n",
      "Epoch 03624: reducing learning rate of group 0 to 1.3768e-09.\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3623:\n",
      "train loss: 1.1902528727542793e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3624:\n",
      "train loss: 1.06480946891534e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3625:\n",
      "train loss: 1.1118279914248124e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3626:\n",
      "train loss: 9.979889668971678e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3627:\n",
      "train loss: 1.1713268347452112e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3628:\n",
      "train loss: 1.0488988484759014e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3629:\n",
      "train loss: 1.1309538383144145e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3630:\n",
      "train loss: 1.0196218353736884e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3631:\n",
      "train loss: 1.148739242158001e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3632:\n",
      "train loss: 1.0267174190885606e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3633:\n",
      "train loss: 1.1515269399901694e-08\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3634:\n",
      "train loss: 1.0386342027942124e-08\n",
      "Epoch 03636: reducing learning rate of group 0 to 1.3080e-09.\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3635:\n",
      "train loss: 1.1317723227885284e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3636:\n",
      "train loss: 1.0127751746944609e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3637:\n",
      "train loss: 1.0540050419133377e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3638:\n",
      "train loss: 9.465105830580516e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3639:\n",
      "train loss: 1.1126206236618309e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3640:\n",
      "train loss: 9.96731954980427e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3641:\n",
      "train loss: 1.0721089649466622e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3642:\n",
      "train loss: 9.6663985670634e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3643:\n",
      "train loss: 1.0909789939828788e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3644:\n",
      "train loss: 9.752161013652003e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3645:\n",
      "train loss: 1.0916351473677575e-08\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3646:\n",
      "train loss: 9.845388452501436e-09\n",
      "Epoch 03648: reducing learning rate of group 0 to 1.2426e-09.\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3647:\n",
      "train loss: 1.0744889338778067e-08\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3648:\n",
      "train loss: 9.612769327156813e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3649:\n",
      "train loss: 9.995037164493125e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3650:\n",
      "train loss: 8.974916894865087e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3651:\n",
      "train loss: 1.0549694309199792e-08\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3652:\n",
      "train loss: 9.435636520560657e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3653:\n",
      "train loss: 1.0198109899994132e-08\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3654:\n",
      "train loss: 9.1979973565986e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3655:\n",
      "train loss: 1.0313427149193364e-08\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3656:\n",
      "train loss: 9.199649963090949e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3657:\n",
      "train loss: 1.0409601769677995e-08\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3658:\n",
      "train loss: 9.379814268420594e-09\n",
      "Epoch 03660: reducing learning rate of group 0 to 1.1804e-09.\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3659:\n",
      "train loss: 1.0163478866679524e-08\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3660:\n",
      "train loss: 9.08618405211319e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3661:\n",
      "train loss: 9.511575643528106e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3662:\n",
      "train loss: 8.526963315065866e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3663:\n",
      "train loss: 9.996202555642328e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3664:\n",
      "train loss: 8.915183844808501e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3665:\n",
      "train loss: 9.730070642770516e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3666:\n",
      "train loss: 8.773580834009706e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3667:\n",
      "train loss: 9.744429306162301e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3668:\n",
      "train loss: 8.671616916337816e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3669:\n",
      "train loss: 9.938262478529631e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3670:\n",
      "train loss: 8.934489435204878e-09\n",
      "Epoch 03672: reducing learning rate of group 0 to 1.1214e-09.\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3671:\n",
      "train loss: 9.63935314484347e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3672:\n",
      "train loss: 8.622211505413724e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3673:\n",
      "train loss: 9.022248274899172e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3674:\n",
      "train loss: 8.06793396069952e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3675:\n",
      "train loss: 9.514612690386682e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3676:\n",
      "train loss: 8.465139698780122e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3677:\n",
      "train loss: 9.252886699302934e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3678:\n",
      "train loss: 8.344790322315082e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3679:\n",
      "train loss: 9.230773615036061e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3680:\n",
      "train loss: 8.195634650191687e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3681:\n",
      "train loss: 9.473187138226134e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3682:\n",
      "train loss: 8.50145440676618e-09\n",
      "Epoch 03684: reducing learning rate of group 0 to 1.0653e-09.\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3683:\n",
      "train loss: 9.149550608623747e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3684:\n",
      "train loss: 8.187106599291622e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3685:\n",
      "train loss: 8.558213593106862e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3686:\n",
      "train loss: 7.637815131992956e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3687:\n",
      "train loss: 9.050608444843883e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3688:\n",
      "train loss: 8.029743155723358e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3689:\n",
      "train loss: 8.811197021815994e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3690:\n",
      "train loss: 7.947515740896058e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3691:\n",
      "train loss: 8.741213370205095e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3692:\n",
      "train loss: 7.744828888062594e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3693:\n",
      "train loss: 9.030959916912909e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3694:\n",
      "train loss: 8.081297360380568e-09\n",
      "Epoch 03696: reducing learning rate of group 0 to 1.0121e-09.\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3695:\n",
      "train loss: 8.707777561193915e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3696:\n",
      "train loss: 7.797733135913849e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3697:\n",
      "train loss: 8.106443851854483e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3698:\n",
      "train loss: 7.220002063134708e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3699:\n",
      "train loss: 8.62421533176902e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3700:\n",
      "train loss: 7.619111083378677e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3701:\n",
      "train loss: 8.411774352726772e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3702:\n",
      "train loss: 7.593129655133458e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3703:\n",
      "train loss: 8.27105208875892e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3704:\n",
      "train loss: 7.3132750000550065e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3705:\n",
      "train loss: 8.626741966073313e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3706:\n",
      "train loss: 7.69083362292317e-09\n",
      "Epoch 03708: reducing learning rate of group 0 to 9.6147e-10.\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3707:\n",
      "train loss: 8.305421972698491e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3708:\n",
      "train loss: 7.44934270632478e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3709:\n",
      "train loss: 7.67636401098591e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3710:\n",
      "train loss: 6.824843824297763e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3711:\n",
      "train loss: 8.237550141181496e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3712:\n",
      "train loss: 7.248806239014954e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3713:\n",
      "train loss: 8.03580350226643e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3714:\n",
      "train loss: 7.26536727789117e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3715:\n",
      "train loss: 7.83618270747561e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3716:\n",
      "train loss: 6.9185792134008144e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3717:\n",
      "train loss: 8.252964582904254e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3718:\n",
      "train loss: 7.341009288891457e-09\n",
      "Epoch 03720: reducing learning rate of group 0 to 9.1340e-10.\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3719:\n",
      "train loss: 7.917418830786878e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3720:\n",
      "train loss: 7.114944102327995e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3721:\n",
      "train loss: 7.289979988885449e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3722:\n",
      "train loss: 6.4722661900655365e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3723:\n",
      "train loss: 7.883833416980391e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3724:\n",
      "train loss: 6.937788361740738e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3725:\n",
      "train loss: 7.641856026105516e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3726:\n",
      "train loss: 6.920269526476102e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3727:\n",
      "train loss: 7.466854014322961e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3728:\n",
      "train loss: 6.585418312090868e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3729:\n",
      "train loss: 7.893641995863332e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3730:\n",
      "train loss: 7.042690825504722e-09\n",
      "Epoch 03732: reducing learning rate of group 0 to 8.6773e-10.\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3731:\n",
      "train loss: 7.499040626680978e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3732:\n",
      "train loss: 6.7402268837023034e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3733:\n",
      "train loss: 6.986958576694707e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3734:\n",
      "train loss: 6.197847048387514e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3735:\n",
      "train loss: 7.528441638716908e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3736:\n",
      "train loss: 6.6761251599610776e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3737:\n",
      "train loss: 7.196526250969246e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3738:\n",
      "train loss: 6.505561009107893e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3739:\n",
      "train loss: 7.208215410803669e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3740:\n",
      "train loss: 6.368702688874466e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3741:\n",
      "train loss: 7.469564664383787e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3742:\n",
      "train loss: 6.719843685299362e-09\n",
      "Epoch 03744: reducing learning rate of group 0 to 8.2434e-10.\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3743:\n",
      "train loss: 7.089293039113239e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3744:\n",
      "train loss: 6.341388070807792e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3745:\n",
      "train loss: 6.75952115931637e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3746:\n",
      "train loss: 6.027811469903757e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3747:\n",
      "train loss: 7.078788291989954e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3748:\n",
      "train loss: 6.329309987088802e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3749:\n",
      "train loss: 6.83116432919982e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3750:\n",
      "train loss: 6.1469096005719736e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3751:\n",
      "train loss: 6.944529651614338e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3752:\n",
      "train loss: 6.191931266795468e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3753:\n",
      "train loss: 6.970095720160765e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3754:\n",
      "train loss: 6.281920436523642e-09\n",
      "Epoch 03756: reducing learning rate of group 0 to 7.8312e-10.\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3755:\n",
      "train loss: 6.8323122337084825e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3756:\n",
      "train loss: 6.108785063117992e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3757:\n",
      "train loss: 6.381909781133207e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3758:\n",
      "train loss: 5.7288069984363284e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3759:\n",
      "train loss: 6.71989404203372e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3760:\n",
      "train loss: 6.0188421063579535e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3761:\n",
      "train loss: 6.489433949304816e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3762:\n",
      "train loss: 5.851988852012515e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3763:\n",
      "train loss: 6.594028394216849e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3764:\n",
      "train loss: 5.900395456915852e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3765:\n",
      "train loss: 6.598917056596963e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3766:\n",
      "train loss: 5.956387686494882e-09\n",
      "Epoch 03768: reducing learning rate of group 0 to 7.4397e-10.\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3767:\n",
      "train loss: 6.498616220540829e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3768:\n",
      "train loss: 5.820755574063827e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3769:\n",
      "train loss: 6.041662440692775e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3770:\n",
      "train loss: 5.428923899504162e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3771:\n",
      "train loss: 6.400586309128833e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3772:\n",
      "train loss: 5.756975081386141e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3773:\n",
      "train loss: 6.103952116045469e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3774:\n",
      "train loss: 5.494015077447624e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3775:\n",
      "train loss: 6.3331194954692485e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3776:\n",
      "train loss: 5.694065074110618e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3777:\n",
      "train loss: 6.1593129984150455e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3778:\n",
      "train loss: 5.547289080799949e-09\n",
      "Epoch 03780: reducing learning rate of group 0 to 7.0677e-10.\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3779:\n",
      "train loss: 6.279645996854253e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3780:\n",
      "train loss: 5.645987338250722e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3781:\n",
      "train loss: 5.60702696549399e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3782:\n",
      "train loss: 5.023416910252532e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3783:\n",
      "train loss: 6.210841349081455e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3784:\n",
      "train loss: 5.61186413429869e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3785:\n",
      "train loss: 5.634919444687336e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3786:\n",
      "train loss: 5.04936165722526e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3787:\n",
      "train loss: 6.183571327210555e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3788:\n",
      "train loss: 5.587259600347476e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3789:\n",
      "train loss: 5.653706432317327e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3790:\n",
      "train loss: 5.066249762903903e-09\n",
      "Epoch 03792: reducing learning rate of group 0 to 6.7143e-10.\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3791:\n",
      "train loss: 6.164985889254492e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3792:\n",
      "train loss: 5.570157356421124e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3793:\n",
      "train loss: 5.104723647867249e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3794:\n",
      "train loss: 4.546014246062755e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3795:\n",
      "train loss: 6.1204123959315456e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3796:\n",
      "train loss: 5.554475849225232e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3797:\n",
      "train loss: 5.11892599833534e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3798:\n",
      "train loss: 4.561542716636141e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3799:\n",
      "train loss: 6.100214044536195e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3800:\n",
      "train loss: 5.531663852625222e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3801:\n",
      "train loss: 5.1421561555791785e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3802:\n",
      "train loss: 4.587385962227584e-09\n",
      "Epoch 03804: reducing learning rate of group 0 to 6.3786e-10.\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3803:\n",
      "train loss: 6.069008144694028e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3804:\n",
      "train loss: 5.496865139230932e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3805:\n",
      "train loss: 4.645481976745783e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3806:\n",
      "train loss: 4.123929249789435e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3807:\n",
      "train loss: 5.989381628143737e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3808:\n",
      "train loss: 5.436972421676652e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3809:\n",
      "train loss: 4.712196920516757e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3810:\n",
      "train loss: 4.198311866699496e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3811:\n",
      "train loss: 5.906172584979952e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3812:\n",
      "train loss: 5.346798538455881e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3813:\n",
      "train loss: 4.807134258615485e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3814:\n",
      "train loss: 4.29828849594618e-09\n",
      "Epoch 03816: reducing learning rate of group 0 to 6.0597e-10.\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3815:\n",
      "train loss: 5.800895492192511e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3816:\n",
      "train loss: 5.237708473547875e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3817:\n",
      "train loss: 4.4115430348593944e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3818:\n",
      "train loss: 3.933828433459226e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3819:\n",
      "train loss: 5.650895896571183e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3820:\n",
      "train loss: 5.107217665681859e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3821:\n",
      "train loss: 4.548651813689896e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3822:\n",
      "train loss: 4.076399448828091e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3823:\n",
      "train loss: 5.504654293957176e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3824:\n",
      "train loss: 4.9589092118930895e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3825:\n",
      "train loss: 4.697128347242116e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3826:\n",
      "train loss: 4.224227898813045e-09\n",
      "Epoch 03828: reducing learning rate of group 0 to 5.7567e-10.\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3827:\n",
      "train loss: 5.359089616122367e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3828:\n",
      "train loss: 4.816669335708147e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3829:\n",
      "train loss: 4.3534237350525366e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3830:\n",
      "train loss: 3.902924419596153e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3831:\n",
      "train loss: 5.200318355170532e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3832:\n",
      "train loss: 4.684204984188796e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3833:\n",
      "train loss: 4.485711651700383e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3834:\n",
      "train loss: 4.0342574139224226e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3835:\n",
      "train loss: 5.071749297684486e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3836:\n",
      "train loss: 4.559584435840358e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3837:\n",
      "train loss: 4.6054803004102566e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3838:\n",
      "train loss: 4.149274985876812e-09\n",
      "Epoch 03840: reducing learning rate of group 0 to 5.4688e-10.\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3839:\n",
      "train loss: 4.962332766134905e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3840:\n",
      "train loss: 4.45654605021855e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3841:\n",
      "train loss: 4.244622233611983e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3842:\n",
      "train loss: 3.807425771154167e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3843:\n",
      "train loss: 4.850791354354557e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3844:\n",
      "train loss: 4.372514006222597e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3845:\n",
      "train loss: 4.326157553776584e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3846:\n",
      "train loss: 3.88674583319002e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3847:\n",
      "train loss: 4.7742526440734715e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3848:\n",
      "train loss: 4.299674366087033e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3849:\n",
      "train loss: 4.3948796286777665e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3850:\n",
      "train loss: 3.952046968194876e-09\n",
      "Epoch 03852: reducing learning rate of group 0 to 5.1954e-10.\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3851:\n",
      "train loss: 4.712375187264628e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3852:\n",
      "train loss: 4.2419006513567605e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3853:\n",
      "train loss: 4.014185273466885e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3854:\n",
      "train loss: 3.591167625118123e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3855:\n",
      "train loss: 4.641228832590169e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3856:\n",
      "train loss: 4.195768129605248e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3857:\n",
      "train loss: 4.0585333574886624e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3858:\n",
      "train loss: 3.6342469219944237e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3859:\n",
      "train loss: 4.599393546514495e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3860:\n",
      "train loss: 4.155893836556663e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3861:\n",
      "train loss: 4.0960463089033335e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3862:\n",
      "train loss: 3.6700435184766744e-09\n",
      "Epoch 03864: reducing learning rate of group 0 to 4.9356e-10.\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3863:\n",
      "train loss: 4.565051949336836e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3864:\n",
      "train loss: 4.1235843174508235e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3865:\n",
      "train loss: 3.713679395631744e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3866:\n",
      "train loss: 3.3078361241533257e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3867:\n",
      "train loss: 4.515863097042731e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3868:\n",
      "train loss: 4.097225492712347e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3869:\n",
      "train loss: 3.73891106209064e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3870:\n",
      "train loss: 3.3324105630598897e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3871:\n",
      "train loss: 4.4917027667143135e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3872:\n",
      "train loss: 4.074030131657431e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3873:\n",
      "train loss: 3.760745546106153e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3874:\n",
      "train loss: 3.3534001276271687e-09\n",
      "Epoch 03876: reducing learning rate of group 0 to 4.6888e-10.\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3875:\n",
      "train loss: 4.471223454836201e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3876:\n",
      "train loss: 4.0545333831510075e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3877:\n",
      "train loss: 3.3873087669691532e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3878:\n",
      "train loss: 2.999822752533334e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3879:\n",
      "train loss: 4.433444398870696e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3880:\n",
      "train loss: 4.03785970077489e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3881:\n",
      "train loss: 3.403332480628861e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3882:\n",
      "train loss: 3.0156116914886165e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3883:\n",
      "train loss: 4.417574231294732e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3884:\n",
      "train loss: 4.022366615983335e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3885:\n",
      "train loss: 3.4180672435983255e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3886:\n",
      "train loss: 3.0300248203595297e-09\n",
      "Epoch 03888: reducing learning rate of group 0 to 4.4544e-10.\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3887:\n",
      "train loss: 4.40312811223397e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3888:\n",
      "train loss: 4.008305972805663e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3889:\n",
      "train loss: 3.0594613119962763e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3890:\n",
      "train loss: 2.690786481029925e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3891:\n",
      "train loss: 4.370108673503487e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3892:\n",
      "train loss: 3.9948198858819394e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3893:\n",
      "train loss: 3.072772902128819e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3894:\n",
      "train loss: 2.7043076923174755e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3895:\n",
      "train loss: 4.35602012484505e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3896:\n",
      "train loss: 3.980593114920436e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3897:\n",
      "train loss: 3.0867542338517793e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3898:\n",
      "train loss: 2.7184495908946602e-09\n",
      "Epoch 03900: reducing learning rate of group 0 to 4.2317e-10.\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3899:\n",
      "train loss: 4.3413249694234945e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3900:\n",
      "train loss: 3.965751876180806e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3901:\n",
      "train loss: 2.7480999315544754e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3902:\n",
      "train loss: 2.398802756605196e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3903:\n",
      "train loss: 4.306515880579371e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3904:\n",
      "train loss: 3.94869013951468e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3905:\n",
      "train loss: 2.7657593513437747e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3906:\n",
      "train loss: 2.41748462648369e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3907:\n",
      "train loss: 4.286300578773627e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3908:\n",
      "train loss: 3.927429264351101e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3909:\n",
      "train loss: 2.787624369301326e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3910:\n",
      "train loss: 2.4404273929017163e-09\n",
      "Epoch 03912: reducing learning rate of group 0 to 4.0201e-10.\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3911:\n",
      "train loss: 4.26167970650246e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3912:\n",
      "train loss: 3.901604965318525e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3913:\n",
      "train loss: 2.4785856414936866e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3914:\n",
      "train loss: 2.1509571257450582e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3915:\n",
      "train loss: 4.2119683803141004e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3916:\n",
      "train loss: 3.86650057422648e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3917:\n",
      "train loss: 2.5164138231867556e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3918:\n",
      "train loss: 2.1922028937524653e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3919:\n",
      "train loss: 4.1661776901267535e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3920:\n",
      "train loss: 3.816968978536718e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3921:\n",
      "train loss: 2.568959767268081e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3922:\n",
      "train loss: 2.248541916421227e-09\n",
      "Epoch 03924: reducing learning rate of group 0 to 3.8191e-10.\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3923:\n",
      "train loss: 4.104753724897922e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3924:\n",
      "train loss: 3.7512327395042226e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3925:\n",
      "train loss: 2.3191926545303716e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3926:\n",
      "train loss: 2.0216402882547717e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3927:\n",
      "train loss: 4.0024736150204796e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3928:\n",
      "train loss: 3.6565060925344855e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3929:\n",
      "train loss: 2.4224915039086183e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3930:\n",
      "train loss: 2.134064763399961e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3931:\n",
      "train loss: 3.8795123746306915e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3932:\n",
      "train loss: 3.5243693013846665e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3933:\n",
      "train loss: 2.5621866426386364e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3934:\n",
      "train loss: 2.2810046635641095e-09\n",
      "Epoch 03936: reducing learning rate of group 0 to 3.6281e-10.\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3935:\n",
      "train loss: 3.7251972966501074e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3936:\n",
      "train loss: 3.3634180214219374e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3937:\n",
      "train loss: 2.4246845177754163e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3938:\n",
      "train loss: 2.165717585646614e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3939:\n",
      "train loss: 3.5281338862359593e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3940:\n",
      "train loss: 3.1730657026544622e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3941:\n",
      "train loss: 2.624033102904908e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3942:\n",
      "train loss: 2.3704966868844465e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3943:\n",
      "train loss: 3.322223080088127e-09\n",
      "Epoch 03945: reducing learning rate of group 0 to 3.4467e-10.\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3944:\n",
      "train loss: 2.9664734343911633e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3945:\n",
      "train loss: 2.8293589056203638e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3946:\n",
      "train loss: 2.5828389505173517e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3947:\n",
      "train loss: 2.8378663470348312e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3948:\n",
      "train loss: 2.512993145434266e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3949:\n",
      "train loss: 2.977128769503846e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3950:\n",
      "train loss: 2.712937111991862e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3951:\n",
      "train loss: 2.726584552391195e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3952:\n",
      "train loss: 2.4198457385540607e-09\n",
      "Epoch 03954: reducing learning rate of group 0 to 3.2744e-10.\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3953:\n",
      "train loss: 3.052712372344829e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3954:\n",
      "train loss: 2.7718325944039246e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3955:\n",
      "train loss: 2.4123019108564658e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3956:\n",
      "train loss: 2.1389361832668746e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3957:\n",
      "train loss: 3.039721333815975e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3958:\n",
      "train loss: 2.7535388504154718e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3959:\n",
      "train loss: 2.4481596540461745e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3960:\n",
      "train loss: 2.189577723845598e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3961:\n",
      "train loss: 2.977978239153058e-09\n",
      "Epoch 03963: reducing learning rate of group 0 to 3.1107e-10.\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3962:\n",
      "train loss: 2.6826059791592036e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3963:\n",
      "train loss: 2.5264551136692374e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3964:\n",
      "train loss: 2.2846492143579195e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3965:\n",
      "train loss: 2.6257601942111228e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3966:\n",
      "train loss: 2.3476188197475345e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3967:\n",
      "train loss: 2.5967330918420062e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3968:\n",
      "train loss: 2.3495262853998167e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3969:\n",
      "train loss: 2.567237643465639e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3970:\n",
      "train loss: 2.295508591283481e-09\n",
      "Epoch 03972: reducing learning rate of group 0 to 2.9551e-10.\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3971:\n",
      "train loss: 2.642862768678695e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3972:\n",
      "train loss: 2.3900994236423787e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3973:\n",
      "train loss: 2.2869181155938745e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3974:\n",
      "train loss: 2.0357362750313835e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3975:\n",
      "train loss: 2.6478484888846472e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3976:\n",
      "train loss: 2.4000694459957744e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3977:\n",
      "train loss: 2.2842386242059723e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3978:\n",
      "train loss: 2.0395846172951326e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3979:\n",
      "train loss: 2.6387716777584063e-09\n",
      "Epoch 03981: reducing learning rate of group 0 to 2.8074e-10.\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3980:\n",
      "train loss: 2.3867520970861427e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3981:\n",
      "train loss: 2.301171817194315e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3982:\n",
      "train loss: 2.071260256549279e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3983:\n",
      "train loss: 2.372461944852008e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3984:\n",
      "train loss: 2.1330624966406306e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3985:\n",
      "train loss: 2.3197135831282096e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3986:\n",
      "train loss: 2.088816886957501e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3987:\n",
      "train loss: 2.356096008048593e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3988:\n",
      "train loss: 2.1180715923704306e-09\n",
      "Epoch 03990: reducing learning rate of group 0 to 2.6670e-10.\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3989:\n",
      "train loss: 2.3334605103968014e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3990:\n",
      "train loss: 2.1015746113496902e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3991:\n",
      "train loss: 2.1223209645679125e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3992:\n",
      "train loss: 1.897871306967566e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3993:\n",
      "train loss: 2.329031063110135e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3994:\n",
      "train loss: 2.106752840117926e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3995:\n",
      "train loss: 2.119063175323277e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3996:\n",
      "train loss: 1.8964522378412737e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3997:\n",
      "train loss: 2.3289072117729237e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3998:\n",
      "train loss: 2.1054700836238217e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3999:\n",
      "train loss: 2.1212998017242833e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 4000:\n",
      "train loss: 1.899645117524858e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 4001:\n",
      "train loss: 2.3249267573639406e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 4002:\n",
      "train loss: 2.100984225185652e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 4003:\n",
      "train loss: 2.1261439979050595e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 4004:\n",
      "train loss: 1.904903974546088e-09\n",
      "Epoch 04006: reducing learning rate of group 0 to 2.5337e-10.\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4005:\n",
      "train loss: 2.3193236250554374e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4006:\n",
      "train loss: 2.095253361942863e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4007:\n",
      "train loss: 1.9206343557259394e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4008:\n",
      "train loss: 1.7108550523761782e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4009:\n",
      "train loss: 2.301491568254769e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4010:\n",
      "train loss: 2.088066570252621e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4011:\n",
      "train loss: 1.9282469045258162e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4012:\n",
      "train loss: 1.7189174810604768e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4013:\n",
      "train loss: 2.2930419085599135e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4014:\n",
      "train loss: 2.079439030319435e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4015:\n",
      "train loss: 1.9369036719914926e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 4016:\n",
      "train loss: 1.727674597531978e-09\n",
      "Epoch 04018: reducing learning rate of group 0 to 2.4070e-10.\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4017:\n",
      "train loss: 2.2841799333483156e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4018:\n",
      "train loss: 2.070634056502434e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4019:\n",
      "train loss: 1.7447938728929128e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4020:\n",
      "train loss: 1.5461913301475148e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4021:\n",
      "train loss: 2.264627477666651e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4022:\n",
      "train loss: 2.061431952427942e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4023:\n",
      "train loss: 1.754188796764677e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4024:\n",
      "train loss: 1.5558325222390235e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4025:\n",
      "train loss: 2.254726532695419e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4026:\n",
      "train loss: 2.0514384788980523e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4027:\n",
      "train loss: 1.7641371288353424e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 4028:\n",
      "train loss: 1.5658222195511075e-09\n",
      "Epoch 04030: reducing learning rate of group 0 to 2.2866e-10.\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4029:\n",
      "train loss: 2.2446411482585484e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4030:\n",
      "train loss: 2.0413936030010618e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4031:\n",
      "train loss: 1.583311771122455e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4032:\n",
      "train loss: 1.3951041283827091e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4033:\n",
      "train loss: 2.2243115006920886e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4034:\n",
      "train loss: 2.0308287668990035e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4035:\n",
      "train loss: 1.5941245331332191e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4036:\n",
      "train loss: 1.4062386820150478e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4037:\n",
      "train loss: 2.2127917227461643e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4038:\n",
      "train loss: 2.0190896413485864e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4039:\n",
      "train loss: 1.6059348586392565e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 4040:\n",
      "train loss: 1.4182200150602831e-09\n",
      "Epoch 04042: reducing learning rate of group 0 to 2.1723e-10.\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4041:\n",
      "train loss: 2.200545248021238e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4042:\n",
      "train loss: 2.006720753897587e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4043:\n",
      "train loss: 1.4371109676392811e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4044:\n",
      "train loss: 1.2592115100821466e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4045:\n",
      "train loss: 2.1777031073537678e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4046:\n",
      "train loss: 1.9928245878275203e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4047:\n",
      "train loss: 1.451555980966035e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4048:\n",
      "train loss: 1.2743036476938643e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4049:\n",
      "train loss: 2.161838033675809e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4050:\n",
      "train loss: 1.9763971831172014e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4051:\n",
      "train loss: 1.4683523865393434e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 4052:\n",
      "train loss: 1.2915958442769577e-09\n",
      "Epoch 04054: reducing learning rate of group 0 to 2.0637e-10.\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4053:\n",
      "train loss: 2.1438863067438466e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4054:\n",
      "train loss: 1.9579710700809272e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4055:\n",
      "train loss: 1.314914989125913e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4056:\n",
      "train loss: 1.1479771662370007e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4057:\n",
      "train loss: 2.1139215358152647e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4058:\n",
      "train loss: 1.935765557942045e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4059:\n",
      "train loss: 1.3383402404728683e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4060:\n",
      "train loss: 1.1727603726549882e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4061:\n",
      "train loss: 2.0875559104928387e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4062:\n",
      "train loss: 1.908143074476536e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4063:\n",
      "train loss: 1.366899440218168e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 4064:\n",
      "train loss: 1.2024194020732286e-09\n",
      "Epoch 04066: reducing learning rate of group 0 to 1.9605e-10.\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4065:\n",
      "train loss: 2.0565437676423186e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4066:\n",
      "train loss: 1.8760630172254853e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4067:\n",
      "train loss: 1.2361257332076951e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4068:\n",
      "train loss: 1.0817680094954958e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4069:\n",
      "train loss: 2.0109322578788532e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4070:\n",
      "train loss: 1.8366111607509335e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4071:\n",
      "train loss: 1.2778809910286202e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4072:\n",
      "train loss: 1.1258799375218628e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4073:\n",
      "train loss: 1.9642997185309433e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4074:\n",
      "train loss: 1.7879463984084566e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4075:\n",
      "train loss: 1.3280435922652202e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 4076:\n",
      "train loss: 1.1775451186620715e-09\n",
      "Epoch 04078: reducing learning rate of group 0 to 1.8625e-10.\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4077:\n",
      "train loss: 1.9110565691715117e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4078:\n",
      "train loss: 1.7334727041306497e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4079:\n",
      "train loss: 1.2277286518014373e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4080:\n",
      "train loss: 1.0868866267291565e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4081:\n",
      "train loss: 1.843535845820891e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4082:\n",
      "train loss: 1.6715256070669882e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4083:\n",
      "train loss: 1.2922225745250431e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4084:\n",
      "train loss: 1.1534801513019118e-09\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 4085:\n",
      "train loss: 1.7753399372803841e-09\n",
      "Epoch 04087: reducing learning rate of group 0 to 1.7694e-10.\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4086:\n",
      "train loss: 1.6021687128132376e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4087:\n",
      "train loss: 1.3620879994746226e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4088:\n",
      "train loss: 1.2298835969211618e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4089:\n",
      "train loss: 1.5543389992908826e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4090:\n",
      "train loss: 1.3917923677736684e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4091:\n",
      "train loss: 1.4215861731074098e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4092:\n",
      "train loss: 1.2861960656137375e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4093:\n",
      "train loss: 1.5015723470540634e-09\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 4094:\n",
      "train loss: 1.3425080414299625e-09\n",
      "Epoch 04096: reducing learning rate of group 0 to 1.6809e-10.\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4095:\n",
      "train loss: 1.4672257326204192e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4096:\n",
      "train loss: 1.3281682095562106e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4097:\n",
      "train loss: 1.324200811217962e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4098:\n",
      "train loss: 1.1775818893644696e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4099:\n",
      "train loss: 1.4861169388031353e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4100:\n",
      "train loss: 1.348339708802714e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4101:\n",
      "train loss: 1.3096080942750032e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4102:\n",
      "train loss: 1.16820252617283e-09\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 4103:\n",
      "train loss: 1.4905972558201892e-09\n",
      "Epoch 04105: reducing learning rate of group 0 to 1.5968e-10.\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4104:\n",
      "train loss: 1.3484103440464668e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4105:\n",
      "train loss: 1.3134665967445508e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4106:\n",
      "train loss: 1.1820198340879095e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4107:\n",
      "train loss: 1.342318023882208e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4108:\n",
      "train loss: 1.2062308222094042e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4109:\n",
      "train loss: 1.3226547800076783e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4110:\n",
      "train loss: 1.1908481849445545e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4111:\n",
      "train loss: 1.3339956573334921e-09\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 4112:\n",
      "train loss: 1.1984462513939271e-09\n",
      "Epoch 04114: reducing learning rate of group 0 to 1.5170e-10.\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4113:\n",
      "train loss: 1.3298898868356044e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4114:\n",
      "train loss: 1.197521497219001e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4115:\n",
      "train loss: 1.2020687025229139e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4116:\n",
      "train loss: 1.0748883777545262e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4117:\n",
      "train loss: 1.3244796026676526e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4118:\n",
      "train loss: 1.1959291028106616e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4119:\n",
      "train loss: 1.2064521075598188e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4120:\n",
      "train loss: 1.0817442214085213e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4121:\n",
      "train loss: 1.3155562347462791e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4122:\n",
      "train loss: 1.1852556678486267e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4123:\n",
      "train loss: 1.218568223747343e-09\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 4124:\n",
      "train loss: 1.0950302634224439e-09\n",
      "Epoch 04126: reducing learning rate of group 0 to 1.4411e-10.\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4125:\n",
      "train loss: 1.301461271370506e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4126:\n",
      "train loss: 1.170558899242144e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4127:\n",
      "train loss: 1.1138026312900539e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4128:\n",
      "train loss: 9.977048811472637e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4129:\n",
      "train loss: 1.2768553877135305e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4130:\n",
      "train loss: 1.150299751145892e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4131:\n",
      "train loss: 1.1360154143434483e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4132:\n",
      "train loss: 1.0214163030199998e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4133:\n",
      "train loss: 1.2522571893539755e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4134:\n",
      "train loss: 1.1251379934694085e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4135:\n",
      "train loss: 1.161361666033966e-09\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 4136:\n",
      "train loss: 1.0466454860063295e-09\n",
      "Epoch 04138: reducing learning rate of group 0 to 1.3691e-10.\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4137:\n",
      "train loss: 1.2275733382893056e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4138:\n",
      "train loss: 1.1011628031850226e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4139:\n",
      "train loss: 1.070486192893094e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4140:\n",
      "train loss: 9.61496448359671e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4141:\n",
      "train loss: 1.1982354240493744e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4142:\n",
      "train loss: 1.0771793613427297e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4143:\n",
      "train loss: 1.0952743913046915e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4144:\n",
      "train loss: 9.867129488678175e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4145:\n",
      "train loss: 1.1731228737372905e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4146:\n",
      "train loss: 1.052405692398766e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4147:\n",
      "train loss: 1.1194039583038177e-09\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 4148:\n",
      "train loss: 1.0099972182960742e-09\n",
      "Epoch 04150: reducing learning rate of group 0 to 1.3006e-10.\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4149:\n",
      "train loss: 1.1510035701791188e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4150:\n",
      "train loss: 1.0315485050334802e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4151:\n",
      "train loss: 1.03067849013124e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4152:\n",
      "train loss: 9.262882214514775e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4153:\n",
      "train loss: 1.1263056327011204e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4154:\n",
      "train loss: 1.0122015223209342e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4155:\n",
      "train loss: 1.0505792132204833e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4156:\n",
      "train loss: 9.464548895640517e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4157:\n",
      "train loss: 1.106295517178761e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4158:\n",
      "train loss: 9.925500016141163e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4159:\n",
      "train loss: 1.0696193714208808e-09\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 4160:\n",
      "train loss: 9.647359002189903e-10\n",
      "Epoch 04162: reducing learning rate of group 0 to 1.2356e-10.\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4161:\n",
      "train loss: 1.0890189624515296e-09\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4162:\n",
      "train loss: 9.763610874652266e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4163:\n",
      "train loss: 9.818933881145297e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4164:\n",
      "train loss: 8.820023285415721e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4165:\n",
      "train loss: 1.0684777947778563e-09\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4166:\n",
      "train loss: 9.606027265073253e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4167:\n",
      "train loss: 9.984439195520348e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4168:\n",
      "train loss: 8.990736452571919e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4169:\n",
      "train loss: 1.0512806694143692e-09\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4170:\n",
      "train loss: 9.434987752436766e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4171:\n",
      "train loss: 1.0151899496947723e-09\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 4172:\n",
      "train loss: 9.153138699550966e-10\n",
      "Epoch 04174: reducing learning rate of group 0 to 1.1738e-10.\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4173:\n",
      "train loss: 1.0357880618689334e-09\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4174:\n",
      "train loss: 9.28861115074797e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4175:\n",
      "train loss: 9.312897409455422e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4176:\n",
      "train loss: 8.363612641214933e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4177:\n",
      "train loss: 1.0163859780700806e-09\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4178:\n",
      "train loss: 9.137731965895621e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4179:\n",
      "train loss: 9.473202309272798e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4180:\n",
      "train loss: 8.530503765865689e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4181:\n",
      "train loss: 9.994466624900375e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4182:\n",
      "train loss: 8.968302201514749e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4183:\n",
      "train loss: 9.63969803139532e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 4184:\n",
      "train loss: 8.692505622372174e-10\n",
      "Epoch 04186: reducing learning rate of group 0 to 1.1151e-10.\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4185:\n",
      "train loss: 9.839491255791787e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4186:\n",
      "train loss: 8.821633956584529e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4187:\n",
      "train loss: 8.849892769130917e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4188:\n",
      "train loss: 7.949588947359737e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4189:\n",
      "train loss: 9.647757154270193e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4190:\n",
      "train loss: 8.670612056697008e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4191:\n",
      "train loss: 9.010027204683282e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4192:\n",
      "train loss: 8.115982595433277e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4193:\n",
      "train loss: 9.479239820336751e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4194:\n",
      "train loss: 8.502577859573425e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4195:\n",
      "train loss: 9.174386107138297e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4196:\n",
      "train loss: 8.275208414918989e-10\n",
      "Epoch 04198: reducing learning rate of group 0 to 1.0594e-10.\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4197:\n",
      "train loss: 9.327633718259544e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4198:\n",
      "train loss: 8.359964320234025e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4199:\n",
      "train loss: 8.426284750195332e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4200:\n",
      "train loss: 7.570931617222927e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4201:\n",
      "train loss: 9.144233969688571e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4202:\n",
      "train loss: 8.215300863543554e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4203:\n",
      "train loss: 8.579457294118514e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4204:\n",
      "train loss: 7.729911778091362e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4205:\n",
      "train loss: 8.983451841541768e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4206:\n",
      "train loss: 8.055438724654184e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4207:\n",
      "train loss: 8.735103584973487e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4208:\n",
      "train loss: 7.880025364777359e-10\n",
      "Epoch 04210: reducing learning rate of group 0 to 1.0064e-10.\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4209:\n",
      "train loss: 8.841161237327538e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4210:\n",
      "train loss: 7.922461275533188e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4211:\n",
      "train loss: 8.021312247251833e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4212:\n",
      "train loss: 7.207670286502455e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4213:\n",
      "train loss: 8.669886049846438e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4214:\n",
      "train loss: 7.787445798326889e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4215:\n",
      "train loss: 8.1648229261437e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4216:\n",
      "train loss: 7.357103994116996e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4217:\n",
      "train loss: 8.518430034748725e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4218:\n",
      "train loss: 7.636830477497394e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4219:\n",
      "train loss: 8.31113779226e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4220:\n",
      "train loss: 7.497916133535892e-10\n",
      "Epoch 04222: reducing learning rate of group 0 to 9.5609e-11.\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4221:\n",
      "train loss: 8.385276488674876e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4222:\n",
      "train loss: 7.512962539336133e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4223:\n",
      "train loss: 7.63028950401779e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4224:\n",
      "train loss: 6.856552805957727e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4225:\n",
      "train loss: 8.224493331513892e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4226:\n",
      "train loss: 7.385799170884627e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4227:\n",
      "train loss: 7.766167243063651e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4228:\n",
      "train loss: 6.998626657311458e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4229:\n",
      "train loss: 8.080063458490667e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4230:\n",
      "train loss: 7.24206925893184e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4231:\n",
      "train loss: 7.905505451288849e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4232:\n",
      "train loss: 7.132470809705335e-10\n",
      "Epoch 04234: reducing learning rate of group 0 to 9.0828e-11.\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4233:\n",
      "train loss: 7.953777282612277e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4234:\n",
      "train loss: 7.12516712580029e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4235:\n",
      "train loss: 7.257523939031809e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4236:\n",
      "train loss: 6.521991197612923e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4237:\n",
      "train loss: 7.801660335955833e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4238:\n",
      "train loss: 7.004247223362514e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4239:\n",
      "train loss: 7.38724462561048e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4240:\n",
      "train loss: 6.658075384377832e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4241:\n",
      "train loss: 7.66302054986565e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4242:\n",
      "train loss: 6.86633560829926e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4243:\n",
      "train loss: 7.520480591215566e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4244:\n",
      "train loss: 6.785636119925391e-10\n",
      "Epoch 04246: reducing learning rate of group 0 to 8.6287e-11.\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4245:\n",
      "train loss: 7.543091125571819e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4246:\n",
      "train loss: 6.756026164381674e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4247:\n",
      "train loss: 6.903608121485567e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4248:\n",
      "train loss: 6.204267707858836e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4249:\n",
      "train loss: 7.399497864939013e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4250:\n",
      "train loss: 6.641369301993744e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4251:\n",
      "train loss: 7.027115197116602e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4252:\n",
      "train loss: 6.334279853689556e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4253:\n",
      "train loss: 7.266765985511016e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4254:\n",
      "train loss: 6.509405412880273e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4255:\n",
      "train loss: 7.154073040133804e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4256:\n",
      "train loss: 6.455377439772681e-10\n",
      "Epoch 04258: reducing learning rate of group 0 to 8.1973e-11.\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4257:\n",
      "train loss: 7.153402414958238e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4258:\n",
      "train loss: 6.405944192225018e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4259:\n",
      "train loss: 6.566172222245807e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4260:\n",
      "train loss: 5.901097182866723e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4261:\n",
      "train loss: 7.018343411147724e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4262:\n",
      "train loss: 6.297513892426389e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4263:\n",
      "train loss: 6.683617864013565e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4264:\n",
      "train loss: 6.025264964205333e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4265:\n",
      "train loss: 6.891187144362275e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4266:\n",
      "train loss: 6.17114327275326e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4267:\n",
      "train loss: 6.804695063204775e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4268:\n",
      "train loss: 6.140259063580341e-10\n",
      "Epoch 04270: reducing learning rate of group 0 to 7.7874e-11.\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4269:\n",
      "train loss: 6.784032228482466e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4270:\n",
      "train loss: 6.074175218628288e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4271:\n",
      "train loss: 6.244285930554998e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4272:\n",
      "train loss: 5.611719946208792e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4273:\n",
      "train loss: 6.656971219773062e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4274:\n",
      "train loss: 5.971397771475134e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4275:\n",
      "train loss: 6.356349220921343e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4276:\n",
      "train loss: 5.730822541417122e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4277:\n",
      "train loss: 6.534611625129187e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4278:\n",
      "train loss: 5.849812462966123e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4279:\n",
      "train loss: 6.472240650129266e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4280:\n",
      "train loss: 5.840303996063153e-10\n",
      "Epoch 04282: reducing learning rate of group 0 to 7.3980e-11.\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4281:\n",
      "train loss: 6.433179527190755e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4282:\n",
      "train loss: 5.758998769343918e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4283:\n",
      "train loss: 5.937861045810053e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4284:\n",
      "train loss: 5.336119034264845e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4285:\n",
      "train loss: 6.313707187897031e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4286:\n",
      "train loss: 5.66139859415018e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4287:\n",
      "train loss: 6.045169692475366e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4288:\n",
      "train loss: 5.450821000257456e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4289:\n",
      "train loss: 6.195425672697249e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4290:\n",
      "train loss: 5.543909820617332e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4291:\n",
      "train loss: 6.156428534658779e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4292:\n",
      "train loss: 5.555173446472042e-10\n",
      "Epoch 04294: reducing learning rate of group 0 to 7.0281e-11.\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4293:\n",
      "train loss: 6.099531543218345e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4294:\n",
      "train loss: 5.459254983182752e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4295:\n",
      "train loss: 5.64631924537887e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4296:\n",
      "train loss: 5.073596957902877e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4297:\n",
      "train loss: 5.987538874882444e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4298:\n",
      "train loss: 5.36649966289684e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4299:\n",
      "train loss: 5.749423378891278e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4300:\n",
      "train loss: 5.18461389522266e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4301:\n",
      "train loss: 5.872558750324003e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4302:\n",
      "train loss: 5.252356053028295e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4303:\n",
      "train loss: 5.856612933917793e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4304:\n",
      "train loss: 5.284138876241074e-10\n",
      "Epoch 04306: reducing learning rate of group 0 to 6.6767e-11.\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4305:\n",
      "train loss: 5.782195923704445e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4306:\n",
      "train loss: 5.174151846276265e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4307:\n",
      "train loss: 5.368717901717595e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4308:\n",
      "train loss: 4.823185924812094e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4309:\n",
      "train loss: 5.677787342175957e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4310:\n",
      "train loss: 5.085915483088155e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4311:\n",
      "train loss: 5.468376407513591e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4312:\n",
      "train loss: 4.931536834459214e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4313:\n",
      "train loss: 5.565031702167254e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4314:\n",
      "train loss: 4.974060054378112e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4315:\n",
      "train loss: 5.572228122211066e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4316:\n",
      "train loss: 5.026512322396805e-10\n",
      "Epoch 04318: reducing learning rate of group 0 to 6.3429e-11.\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4317:\n",
      "train loss: 5.480425046387246e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4318:\n",
      "train loss: 4.903032650845555e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4319:\n",
      "train loss: 5.104048462948228e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4320:\n",
      "train loss: 4.583728864591822e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4321:\n",
      "train loss: 5.384022104554587e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4322:\n",
      "train loss: 4.819006372848031e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4323:\n",
      "train loss: 5.201262210668389e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4324:\n",
      "train loss: 4.690813083132106e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4325:\n",
      "train loss: 5.272025131179178e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4326:\n",
      "train loss: 4.708092662092708e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4327:\n",
      "train loss: 5.302620690195206e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4328:\n",
      "train loss: 4.781370109879512e-10\n",
      "Epoch 04330: reducing learning rate of group 0 to 6.0257e-11.\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4329:\n",
      "train loss: 5.193919508803073e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4330:\n",
      "train loss: 4.6457480385253354e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4331:\n",
      "train loss: 4.850879989908398e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4332:\n",
      "train loss: 4.353553385602422e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4333:\n",
      "train loss: 5.106411814387444e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4334:\n",
      "train loss: 4.5655794406647433e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4335:\n",
      "train loss: 4.947221667782711e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4336:\n",
      "train loss: 4.4616080525612744e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4337:\n",
      "train loss: 4.993000374571596e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4338:\n",
      "train loss: 4.4536780710980876e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4339:\n",
      "train loss: 5.047231859713269e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4340:\n",
      "train loss: 4.5476714229430775e-10\n",
      "Epoch 04342: reducing learning rate of group 0 to 5.7245e-11.\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4341:\n",
      "train loss: 4.922915832621487e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4342:\n",
      "train loss: 4.402684938455123e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4343:\n",
      "train loss: 4.6075564952301114e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4344:\n",
      "train loss: 4.130765318311136e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4345:\n",
      "train loss: 4.84559221833851e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4346:\n",
      "train loss: 4.3256103203146727e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4347:\n",
      "train loss: 4.705806819808148e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4348:\n",
      "train loss: 4.2435635777342587e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4349:\n",
      "train loss: 4.727327766309509e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4350:\n",
      "train loss: 4.209908154360497e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4351:\n",
      "train loss: 4.80594741118993e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4352:\n",
      "train loss: 4.3246298017807514e-10\n",
      "Epoch 04354: reducing learning rate of group 0 to 5.4382e-11.\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4353:\n",
      "train loss: 4.668217494217161e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4354:\n",
      "train loss: 4.1749118522753824e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4355:\n",
      "train loss: 4.372411170625913e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4356:\n",
      "train loss: 3.913536025544519e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4357:\n",
      "train loss: 4.602467338786861e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4358:\n",
      "train loss: 4.0989860099700076e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4359:\n",
      "train loss: 4.4777868343039476e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4360:\n",
      "train loss: 4.037781566276574e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4361:\n",
      "train loss: 4.473924216840315e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4362:\n",
      "train loss: 3.975627609843381e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4363:\n",
      "train loss: 4.5795312957565303e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4364:\n",
      "train loss: 4.1119472360713994e-10\n",
      "Epoch 04366: reducing learning rate of group 0 to 5.1663e-11.\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4365:\n",
      "train loss: 4.4315113247313725e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4366:\n",
      "train loss: 3.96449791940908e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4367:\n",
      "train loss: 4.144129311421406e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4368:\n",
      "train loss: 3.7008522709615223e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4369:\n",
      "train loss: 4.3779159362690553e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4370:\n",
      "train loss: 3.885256255312523e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4371:\n",
      "train loss: 4.265845518602205e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4372:\n",
      "train loss: 3.847431645863091e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4373:\n",
      "train loss: 4.231442091703309e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4374:\n",
      "train loss: 3.750337423988338e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4375:\n",
      "train loss: 4.368949421602578e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4376:\n",
      "train loss: 3.909403992060115e-10\n",
      "Epoch 04378: reducing learning rate of group 0 to 4.9080e-11.\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4377:\n",
      "train loss: 4.2160218473060933e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4378:\n",
      "train loss: 3.775218486186763e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4379:\n",
      "train loss: 3.922159557871481e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4380:\n",
      "train loss: 3.493792163620655e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4381:\n",
      "train loss: 4.1720564612090613e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4382:\n",
      "train loss: 3.683836536343085e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4383:\n",
      "train loss: 4.074113886540323e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4384:\n",
      "train loss: 3.677399851313568e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4385:\n",
      "train loss: 3.999313210779804e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4386:\n",
      "train loss: 3.5359023715123867e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4387:\n",
      "train loss: 4.174215860826152e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4388:\n",
      "train loss: 3.7172764611774613e-10\n",
      "Epoch 04390: reducing learning rate of group 0 to 4.6626e-11.\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4389:\n",
      "train loss: 4.0246506016789556e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4390:\n",
      "train loss: 3.610244215786178e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4391:\n",
      "train loss: 3.708458952127154e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4392:\n",
      "train loss: 3.297057228541565e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4393:\n",
      "train loss: 3.9842428966962617e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4394:\n",
      "train loss: 3.497221414780454e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4395:\n",
      "train loss: 3.9018802127434137e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4396:\n",
      "train loss: 3.5281736249215803e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4397:\n",
      "train loss: 3.7804726473987353e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4398:\n",
      "train loss: 3.3376267339641073e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4399:\n",
      "train loss: 3.9950933258943086e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4400:\n",
      "train loss: 3.541647347567934e-10\n",
      "Epoch 04402: reducing learning rate of group 0 to 4.4295e-11.\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4401:\n",
      "train loss: 3.849271282840433e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4402:\n",
      "train loss: 3.462027711185811e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4403:\n",
      "train loss: 3.51054168988574e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4404:\n",
      "train loss: 3.1176206584523255e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4405:\n",
      "train loss: 3.816482716235934e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4406:\n",
      "train loss: 3.3403353346212303e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4407:\n",
      "train loss: 3.7271426778484764e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4408:\n",
      "train loss: 3.379783446116361e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4409:\n",
      "train loss: 3.5867504737296725e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4410:\n",
      "train loss: 3.162744653194275e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4411:\n",
      "train loss: 3.83308548084031e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4412:\n",
      "train loss: 3.4013621846237566e-10\n",
      "Epoch 04414: reducing learning rate of group 0 to 4.2080e-11.\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4413:\n",
      "train loss: 3.654646169896235e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4414:\n",
      "train loss: 3.294402641451347e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4415:\n",
      "train loss: 3.3507603603069084e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4416:\n",
      "train loss: 2.9692279220408976e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4417:\n",
      "train loss: 3.6645591279789693e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4418:\n",
      "train loss: 3.2317254871982785e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4419:\n",
      "train loss: 3.503957519188907e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4420:\n",
      "train loss: 3.1788113767977695e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4421:\n",
      "train loss: 3.457913566981602e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4422:\n",
      "train loss: 3.0458434319422483e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4423:\n",
      "train loss: 3.6536803339136437e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4424:\n",
      "train loss: 3.2796142767750936e-10\n",
      "Epoch 04426: reducing learning rate of group 0 to 3.9976e-11.\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4425:\n",
      "train loss: 3.422825773771996e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4426:\n",
      "train loss: 3.070392280073763e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4427:\n",
      "train loss: 3.2671743887501075e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4428:\n",
      "train loss: 2.9022343153380175e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4429:\n",
      "train loss: 3.4533631856421734e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4430:\n",
      "train loss: 3.090273428196021e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4431:\n",
      "train loss: 3.288607770016216e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4432:\n",
      "train loss: 2.9584865236784597e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4433:\n",
      "train loss: 3.3812027506638636e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4434:\n",
      "train loss: 3.0101501606524245e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4435:\n",
      "train loss: 3.373955131704352e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4436:\n",
      "train loss: 3.0446847402297196e-10\n",
      "Epoch 04438: reducing learning rate of group 0 to 3.7977e-11.\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4437:\n",
      "train loss: 3.3064919567805336e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4438:\n",
      "train loss: 2.951649375703201e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4439:\n",
      "train loss: 3.1042768150795314e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4440:\n",
      "train loss: 2.789359334582709e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4441:\n",
      "train loss: 3.2421993940026194e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4442:\n",
      "train loss: 2.9012389752954546e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4443:\n",
      "train loss: 3.1606696064941984e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4444:\n",
      "train loss: 2.851309421451166e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4445:\n",
      "train loss: 3.1807949819871113e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4446:\n",
      "train loss: 2.8454288611610964e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4447:\n",
      "train loss: 3.21038588071233e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4448:\n",
      "train loss: 2.897368481707147e-10\n",
      "Epoch 04450: reducing learning rate of group 0 to 3.6078e-11.\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4449:\n",
      "train loss: 3.139830501877012e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4450:\n",
      "train loss: 2.8127357112252807e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4451:\n",
      "train loss: 2.934135936495443e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4452:\n",
      "train loss: 2.6356154187985847e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4453:\n",
      "train loss: 3.0977527907433253e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4454:\n",
      "train loss: 2.7862288571244306e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4455:\n",
      "train loss: 2.960924676422554e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4456:\n",
      "train loss: 2.664493952890915e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4457:\n",
      "train loss: 3.0669588232897865e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4458:\n",
      "train loss: 2.756590399087514e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4459:\n",
      "train loss: 2.9878591220715175e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4460:\n",
      "train loss: 2.6907863329138745e-10\n",
      "Epoch 04462: reducing learning rate of group 0 to 3.4274e-11.\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4461:\n",
      "train loss: 3.040408122896678e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4462:\n",
      "train loss: 2.731964672825467e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4463:\n",
      "train loss: 2.7226745200578834e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4464:\n",
      "train loss: 2.4405072114037916e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4465:\n",
      "train loss: 3.0014182751006915e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4466:\n",
      "train loss: 2.7067367624407787e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4467:\n",
      "train loss: 2.748358370148714e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4468:\n",
      "train loss: 2.467607006818098e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4469:\n",
      "train loss: 2.972079056293258e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4470:\n",
      "train loss: 2.6764906264470983e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4471:\n",
      "train loss: 2.7781588090031633e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4472:\n",
      "train loss: 2.497645692791405e-10\n",
      "Epoch 04474: reducing learning rate of group 0 to 3.2561e-11.\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4473:\n",
      "train loss: 2.941080676592561e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4474:\n",
      "train loss: 2.645341783310488e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4475:\n",
      "train loss: 2.536450830814006e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4476:\n",
      "train loss: 2.271534409069555e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4477:\n",
      "train loss: 2.8910347175836783e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4478:\n",
      "train loss: 2.6058826692893277e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4479:\n",
      "train loss: 2.5793158690423117e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4480:\n",
      "train loss: 2.3174195187119007e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4481:\n",
      "train loss: 2.8425916585118e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4482:\n",
      "train loss: 2.5556389626473964e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4483:\n",
      "train loss: 2.630294900251007e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4484:\n",
      "train loss: 2.3687038578531487e-10\n",
      "Epoch 04486: reducing learning rate of group 0 to 3.0933e-11.\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4485:\n",
      "train loss: 2.7916206931866747e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4486:\n",
      "train loss: 2.5054907790006436e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4487:\n",
      "train loss: 2.4205937128912e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4488:\n",
      "train loss: 2.1727745551067376e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4489:\n",
      "train loss: 2.727083895773808e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4490:\n",
      "train loss: 2.4525098010588587e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4491:\n",
      "train loss: 2.475824849185733e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4492:\n",
      "train loss: 2.2294809666863798e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4493:\n",
      "train loss: 2.670276997859792e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4494:\n",
      "train loss: 2.3964167785274925e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4495:\n",
      "train loss: 2.530517973112265e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4496:\n",
      "train loss: 2.2825349799600754e-10\n",
      "Epoch 04498: reducing learning rate of group 0 to 2.9386e-11.\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4497:\n",
      "train loss: 2.619685821524798e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4498:\n",
      "train loss: 2.3488490536038073e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4499:\n",
      "train loss: 2.3292973992059223e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4500:\n",
      "train loss: 2.0925024700180568e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4501:\n",
      "train loss: 2.564597960487216e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4502:\n",
      "train loss: 2.3071698574814512e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4503:\n",
      "train loss: 2.371002155159772e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4504:\n",
      "train loss: 2.1341546277563012e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4505:\n",
      "train loss: 2.5237834788564276e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4506:\n",
      "train loss: 2.2679391740316099e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4507:\n",
      "train loss: 2.4083146693557135e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4508:\n",
      "train loss: 2.1698645474691023e-10\n",
      "Epoch 04510: reducing learning rate of group 0 to 2.7917e-11.\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4509:\n",
      "train loss: 2.4899144194865677e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4510:\n",
      "train loss: 2.236465881464038e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4511:\n",
      "train loss: 2.2039259329889354e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4512:\n",
      "train loss: 1.9764172943503208e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4513:\n",
      "train loss: 2.450534814439973e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4514:\n",
      "train loss: 2.2100624541689993e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4515:\n",
      "train loss: 2.2298426535513892e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4516:\n",
      "train loss: 2.0021246142593646e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4517:\n",
      "train loss: 2.425211100643278e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4518:\n",
      "train loss: 2.1857678670982217e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4519:\n",
      "train loss: 2.2528045715631506e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4520:\n",
      "train loss: 2.0241988078185206e-10\n",
      "Epoch 04522: reducing learning rate of group 0 to 2.6521e-11.\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4521:\n",
      "train loss: 2.4039564740712924e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4522:\n",
      "train loss: 2.1658313301023787e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4523:\n",
      "train loss: 2.0495048146575983e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4524:\n",
      "train loss: 1.8317259043793023e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4525:\n",
      "train loss: 2.375056579530811e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4526:\n",
      "train loss: 2.149191072778299e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4527:\n",
      "train loss: 2.0655219077006341e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4528:\n",
      "train loss: 1.847455648634432e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4529:\n",
      "train loss: 2.3595253604807337e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4530:\n",
      "train loss: 2.134307810789721e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4531:\n",
      "train loss: 2.079441971058073e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4532:\n",
      "train loss: 1.860807751525251e-10\n",
      "Epoch 04534: reducing learning rate of group 0 to 2.5195e-11.\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4533:\n",
      "train loss: 2.34653027942606e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4534:\n",
      "train loss: 2.122071844606361e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4535:\n",
      "train loss: 1.8800688240408347e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4536:\n",
      "train loss: 1.6719324993665699e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4537:\n",
      "train loss: 2.3250573569525221e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4538:\n",
      "train loss: 2.1121728006603613e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4539:\n",
      "train loss: 1.8893480812856785e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4540:\n",
      "train loss: 1.6809241938896673e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4541:\n",
      "train loss: 2.3161472549924666e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4542:\n",
      "train loss: 2.103686469695742e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4543:\n",
      "train loss: 1.8971202704826935e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4544:\n",
      "train loss: 1.688331903085628e-10\n",
      "Epoch 04546: reducing learning rate of group 0 to 2.3935e-11.\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4545:\n",
      "train loss: 2.3088341588789797e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4546:\n",
      "train loss: 2.0967969688244204e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4547:\n",
      "train loss: 1.7033456519500952e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4548:\n",
      "train loss: 1.504751887705061e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4549:\n",
      "train loss: 2.2924605820945345e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4550:\n",
      "train loss: 2.0912093674992626e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4551:\n",
      "train loss: 1.7084957786929764e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4552:\n",
      "train loss: 1.5097551181076416e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4553:\n",
      "train loss: 2.2873326386753317e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4554:\n",
      "train loss: 2.0862503570001733e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4555:\n",
      "train loss: 1.7130266718041387e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4556:\n",
      "train loss: 1.5141848818848464e-10\n",
      "Epoch 04558: reducing learning rate of group 0 to 2.2738e-11.\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4557:\n",
      "train loss: 2.2827481360728365e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4558:\n",
      "train loss: 2.081772579638533e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4559:\n",
      "train loss: 1.527239505272754e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4560:\n",
      "train loss: 1.338381278193455e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4561:\n",
      "train loss: 2.2682837745420968e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4562:\n",
      "train loss: 2.0771928504992392e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4563:\n",
      "train loss: 1.5317321165669664e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4564:\n",
      "train loss: 1.3430865296694423e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4565:\n",
      "train loss: 2.2630643787739606e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4566:\n",
      "train loss: 2.071743958646434e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4567:\n",
      "train loss: 1.5371797614072163e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4568:\n",
      "train loss: 1.3488189299066163e-10\n",
      "Epoch 04570: reducing learning rate of group 0 to 2.1601e-11.\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4569:\n",
      "train loss: 2.2567123423184728e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4570:\n",
      "train loss: 2.0650377879458503e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4571:\n",
      "train loss: 1.3635936725385078e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4572:\n",
      "train loss: 1.1853094608984463e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4573:\n",
      "train loss: 2.2385848108626895e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4574:\n",
      "train loss: 2.0554388663124376e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4575:\n",
      "train loss: 1.3739299865799932e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4576:\n",
      "train loss: 1.1967696068550254e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4577:\n",
      "train loss: 2.2254739007032005e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4578:\n",
      "train loss: 2.0410562414172937e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4579:\n",
      "train loss: 1.3892659215239578e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4580:\n",
      "train loss: 1.2134929310720902e-10\n",
      "Epoch 04582: reducing learning rate of group 0 to 2.0521e-11.\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4581:\n",
      "train loss: 2.20674432603585e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4582:\n",
      "train loss: 2.0206821189835836e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4583:\n",
      "train loss: 1.2395132644718193e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4584:\n",
      "train loss: 1.0752147001684568e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4585:\n",
      "train loss: 2.1693034315211055e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4586:\n",
      "train loss: 1.9884917826817957e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4587:\n",
      "train loss: 1.2751190324403077e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4588:\n",
      "train loss: 1.1149048142073793e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4589:\n",
      "train loss: 2.1243014900724668e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4590:\n",
      "train loss: 1.9389091171281227e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4591:\n",
      "train loss: 1.3285438618772313e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4592:\n",
      "train loss: 1.1726655422564884e-10\n",
      "Epoch 04594: reducing learning rate of group 0 to 1.9495e-11.\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4593:\n",
      "train loss: 2.0611676769773353e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4594:\n",
      "train loss: 1.871013080428384e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4595:\n",
      "train loss: 1.2373420066736305e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4596:\n",
      "train loss: 1.0956447498609132e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4597:\n",
      "train loss: 1.966657625362729e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4598:\n",
      "train loss: 1.7769840351156192e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4599:\n",
      "train loss: 1.3389051949278136e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4600:\n",
      "train loss: 1.2033945618213384e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4601:\n",
      "train loss: 1.8541076998615972e-10\n",
      "Epoch 04603: reducing learning rate of group 0 to 1.8521e-11.\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4602:\n",
      "train loss: 1.6600676766274554e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4603:\n",
      "train loss: 1.4588536817649572e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4604:\n",
      "train loss: 1.3300583746374587e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4605:\n",
      "train loss: 1.579064182166464e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4606:\n",
      "train loss: 1.3991430843817211e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4607:\n",
      "train loss: 1.5577370657617716e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4608:\n",
      "train loss: 1.420894796146749e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4609:\n",
      "train loss: 1.4979757457484834e-10\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4610:\n",
      "train loss: 1.327558524041981e-10\n",
      "Epoch 04612: reducing learning rate of group 0 to 1.7594e-11.\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4611:\n",
      "train loss: 1.619697248568392e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4612:\n",
      "train loss: 1.473105537359111e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4613:\n",
      "train loss: 1.310192076344281e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4614:\n",
      "train loss: 1.1596516147220527e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4615:\n",
      "train loss: 1.6273337287822518e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4616:\n",
      "train loss: 1.475447931393152e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4617:\n",
      "train loss: 1.3196149112256042e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4618:\n",
      "train loss: 1.1791399673203645e-10\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4619:\n",
      "train loss: 1.6000704552405812e-10\n",
      "Epoch 04621: reducing learning rate of group 0 to 1.6715e-11.\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4620:\n",
      "train loss: 1.4416116617453864e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4621:\n",
      "train loss: 1.3588800900546786e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4622:\n",
      "train loss: 1.2285785441723093e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4623:\n",
      "train loss: 1.411807805581839e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4624:\n",
      "train loss: 1.2622465011427628e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4625:\n",
      "train loss: 1.3961970043390929e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4626:\n",
      "train loss: 1.2631841380911638e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4627:\n",
      "train loss: 1.3805489549367865e-10\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4628:\n",
      "train loss: 1.2344999952277202e-10\n",
      "Epoch 04630: reducing learning rate of group 0 to 1.5879e-11.\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4629:\n",
      "train loss: 1.4207533652109482e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4630:\n",
      "train loss: 1.2848727211469835e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4631:\n",
      "train loss: 1.2298042679817542e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4632:\n",
      "train loss: 1.0946935550440479e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4633:\n",
      "train loss: 1.4238609563680568e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4634:\n",
      "train loss: 1.2910930463336177e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4635:\n",
      "train loss: 1.2270489278582921e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4636:\n",
      "train loss: 1.095057805150109e-10\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4637:\n",
      "train loss: 1.4210702401434238e-10\n",
      "Epoch 04639: reducing learning rate of group 0 to 1.5085e-11.\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4638:\n",
      "train loss: 1.2864998836684295e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4639:\n",
      "train loss: 1.233128359919523e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4640:\n",
      "train loss: 1.1088588457101748e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4641:\n",
      "train loss: 1.281063722591585e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4642:\n",
      "train loss: 1.1532291972006165e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4643:\n",
      "train loss: 1.2400959356711548e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4644:\n",
      "train loss: 1.1155302118727437e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4645:\n",
      "train loss: 1.274734866707193e-10\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4646:\n",
      "train loss: 1.1474287868771604e-10\n",
      "Epoch 04648: reducing learning rate of group 0 to 1.4331e-11.\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4647:\n",
      "train loss: 1.2453943278877182e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4648:\n",
      "train loss: 1.1205045065602913e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4649:\n",
      "train loss: 1.1505878733852849e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4650:\n",
      "train loss: 1.0301623387136686e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4651:\n",
      "train loss: 1.2424002447519483e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4652:\n",
      "train loss: 1.1232560660413522e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4653:\n",
      "train loss: 1.1482615207359478e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4654:\n",
      "train loss: 1.0283038234848197e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4655:\n",
      "train loss: 1.2438170036904975e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4656:\n",
      "train loss: 1.1243947060701824e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4657:\n",
      "train loss: 1.1472959918527748e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4658:\n",
      "train loss: 1.0275781843422251e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4659:\n",
      "train loss: 1.2442934799278467e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4660:\n",
      "train loss: 1.1247469247067159e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4661:\n",
      "train loss: 1.1469851793515701e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4662:\n",
      "train loss: 1.0273911624628864e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4663:\n",
      "train loss: 1.24433089731665e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4664:\n",
      "train loss: 1.1247321602459388e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4665:\n",
      "train loss: 1.1469691612761792e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4666:\n",
      "train loss: 1.0274197479840137e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4667:\n",
      "train loss: 1.244193593871069e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4668:\n",
      "train loss: 1.1246057231668399e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4669:\n",
      "train loss: 1.1470413340740344e-10\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4670:\n",
      "train loss: 1.0275141305341794e-10\n",
      "Epoch 04672: reducing learning rate of group 0 to 1.3614e-11.\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4671:\n",
      "train loss: 1.2440180049574248e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4672:\n",
      "train loss: 1.1244258981317998e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4673:\n",
      "train loss: 1.0335511058155968e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4674:\n",
      "train loss: 9.20008631138422e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4675:\n",
      "train loss: 1.2378692288144484e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4676:\n",
      "train loss: 1.1242604095962136e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4677:\n",
      "train loss: 1.0336565845235447e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4678:\n",
      "train loss: 9.201188919007191e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4679:\n",
      "train loss: 1.237699884508285e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4680:\n",
      "train loss: 1.1240997691644964e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4681:\n",
      "train loss: 1.0337489121178146e-10\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4682:\n",
      "train loss: 9.20201896255181e-11\n",
      "Epoch 04684: reducing learning rate of group 0 to 1.2934e-11.\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4683:\n",
      "train loss: 1.237557405695998e-10\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4684:\n",
      "train loss: 1.1239643553141041e-10\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4685:\n",
      "train loss: 9.259208932771972e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4686:\n",
      "train loss: 8.180478431213561e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4687:\n",
      "train loss: 1.2317638851222083e-10\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4688:\n",
      "train loss: 1.123862268304809e-10\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4689:\n",
      "train loss: 9.259547217350109e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4690:\n",
      "train loss: 8.18090510314252e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4691:\n",
      "train loss: 1.231666532252398e-10\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4692:\n",
      "train loss: 1.1237658339727161e-10\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4693:\n",
      "train loss: 9.259744620318894e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4694:\n",
      "train loss: 8.181097430389812e-11\n",
      "Epoch 04696: reducing learning rate of group 0 to 1.2287e-11.\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4695:\n",
      "train loss: 1.2315884340836953e-10\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4696:\n",
      "train loss: 1.1236948957482158e-10\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4697:\n",
      "train loss: 8.23494538507482e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4698:\n",
      "train loss: 7.210191263647595e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4699:\n",
      "train loss: 1.2261248906629239e-10\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4700:\n",
      "train loss: 1.1236429719665872e-10\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4701:\n",
      "train loss: 8.234914461257105e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4702:\n",
      "train loss: 7.210106373216924e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4703:\n",
      "train loss: 1.2260739178076195e-10\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4704:\n",
      "train loss: 1.1235966287992616e-10\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4705:\n",
      "train loss: 8.234854522489764e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4706:\n",
      "train loss: 7.210061749644102e-11\n",
      "Epoch 04708: reducing learning rate of group 0 to 1.1673e-11.\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4707:\n",
      "train loss: 1.2260301083872636e-10\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4708:\n",
      "train loss: 1.1235486033490477e-10\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4709:\n",
      "train loss: 7.261079213091847e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4710:\n",
      "train loss: 6.287667449706721e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4711:\n",
      "train loss: 1.2208609148554154e-10\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4712:\n",
      "train loss: 1.123502028648426e-10\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4713:\n",
      "train loss: 7.261093406119545e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4714:\n",
      "train loss: 6.287611734959846e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4715:\n",
      "train loss: 1.2208025208546867e-10\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4716:\n",
      "train loss: 1.123445990959496e-10\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4717:\n",
      "train loss: 7.261063317206378e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4718:\n",
      "train loss: 6.287746881792099e-11\n",
      "Epoch 04720: reducing learning rate of group 0 to 1.1089e-11.\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4719:\n",
      "train loss: 1.2207253204806613e-10\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4720:\n",
      "train loss: 1.1233789932214013e-10\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4721:\n",
      "train loss: 6.336557657109065e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4722:\n",
      "train loss: 5.411930384325728e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4723:\n",
      "train loss: 1.2157628292337804e-10\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4724:\n",
      "train loss: 1.1232588271868145e-10\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4725:\n",
      "train loss: 6.337283971749838e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4726:\n",
      "train loss: 5.412924799130023e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4727:\n",
      "train loss: 1.215597699565409e-10\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4728:\n",
      "train loss: 1.1230806968374332e-10\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4729:\n",
      "train loss: 6.338765965762836e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4730:\n",
      "train loss: 5.414747928726719e-11\n",
      "Epoch 04732: reducing learning rate of group 0 to 1.0534e-11.\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4731:\n",
      "train loss: 1.2153243006868943e-10\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4732:\n",
      "train loss: 1.1227749272112716e-10\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4733:\n",
      "train loss: 5.463146277650098e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4734:\n",
      "train loss: 4.586027747570903e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4735:\n",
      "train loss: 1.2101800977756543e-10\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4736:\n",
      "train loss: 1.1221512555112181e-10\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4737:\n",
      "train loss: 5.469731718828598e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4738:\n",
      "train loss: 4.5940621001931074e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4739:\n",
      "train loss: 1.2091087078223035e-10\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4740:\n",
      "train loss: 1.1209043420694376e-10\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4741:\n",
      "train loss: 5.483346622987518e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4742:\n",
      "train loss: 4.610349335232416e-11\n",
      "Epoch 04744: reducing learning rate of group 0 to 1.0008e-11.\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4743:\n",
      "train loss: 1.2069846542421525e-10\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4744:\n",
      "train loss: 1.118432047983735e-10\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4745:\n",
      "train loss: 4.676093057002385e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4746:\n",
      "train loss: 3.854690579591995e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4747:\n",
      "train loss: 1.1975542621462508e-10\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4748:\n",
      "train loss: 1.1121611199000683e-10\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4749:\n",
      "train loss: 4.7489174081087815e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4750:\n",
      "train loss: 3.946602761862135e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4751:\n",
      "train loss: 1.1850341678409054e-10\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4752:\n",
      "train loss: 1.0970047959022946e-10\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4753:\n",
      "train loss: 4.9219816324091124e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4754:\n",
      "train loss: 4.159271278385974e-11\n",
      "Epoch 04756: reducing learning rate of group 0 to 9.5074e-12.\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4755:\n",
      "train loss: 1.1569291146018472e-10\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4756:\n",
      "train loss: 1.063406183723898e-10\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4757:\n",
      "train loss: 4.508725008240122e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4758:\n",
      "train loss: 3.889764166645372e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4759:\n",
      "train loss: 1.0854672604375177e-10\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4760:\n",
      "train loss: 9.806358301237555e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4761:\n",
      "train loss: 5.462317371558545e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4762:\n",
      "train loss: 4.980164204033088e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4763:\n",
      "train loss: 9.609156458748403e-11\n",
      "Epoch 04765: reducing learning rate of group 0 to 9.0320e-12.\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4764:\n",
      "train loss: 8.430420498087935e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4765:\n",
      "train loss: 6.927692476293061e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4766:\n",
      "train loss: 6.503193125674729e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4767:\n",
      "train loss: 7.410450239431975e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4768:\n",
      "train loss: 6.352889411839412e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4769:\n",
      "train loss: 8.115692389175839e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4770:\n",
      "train loss: 7.531340362161835e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4771:\n",
      "train loss: 6.586646869447544e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4772:\n",
      "train loss: 5.7633324662048445e-11\n",
      "Epoch 04774: reducing learning rate of group 0 to 8.5804e-12.\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4773:\n",
      "train loss: 8.411531069115152e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4774:\n",
      "train loss: 7.542498683810885e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4775:\n",
      "train loss: 6.130261610007236e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4776:\n",
      "train loss: 5.607783417846743e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4777:\n",
      "train loss: 7.599842229466914e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4778:\n",
      "train loss: 6.537903297164425e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4779:\n",
      "train loss: 7.281428407687611e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4780:\n",
      "train loss: 6.78240366254395e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4781:\n",
      "train loss: 6.570590014579846e-11\n",
      "Epoch 04783: reducing learning rate of group 0 to 8.1514e-12.\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4782:\n",
      "train loss: 5.7034651934760303e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4783:\n",
      "train loss: 7.837467463922218e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4784:\n",
      "train loss: 7.062380854441344e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4785:\n",
      "train loss: 5.963751626527146e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4786:\n",
      "train loss: 5.467344787998223e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4787:\n",
      "train loss: 7.10890169887703e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4788:\n",
      "train loss: 6.083215737474811e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4789:\n",
      "train loss: 7.079016212184642e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4790:\n",
      "train loss: 6.587397218641046e-11\n",
      "Epoch 04792: reducing learning rate of group 0 to 7.7438e-12.\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4791:\n",
      "train loss: 6.159694471586748e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4792:\n",
      "train loss: 5.3692275715356785e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4793:\n",
      "train loss: 6.871467760387409e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4794:\n",
      "train loss: 6.15018448893063e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4795:\n",
      "train loss: 6.17918279245319e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4796:\n",
      "train loss: 5.5863988824025464e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4797:\n",
      "train loss: 6.577321614136719e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4798:\n",
      "train loss: 5.8060985284504485e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4799:\n",
      "train loss: 6.546352753696722e-11\n",
      "Epoch 04801: reducing learning rate of group 0 to 7.3566e-12.\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4800:\n",
      "train loss: 5.950080764041827e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4801:\n",
      "train loss: 6.261052872956018e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4802:\n",
      "train loss: 5.577905114833475e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4803:\n",
      "train loss: 6.126265945260673e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4804:\n",
      "train loss: 5.535203581224429e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4805:\n",
      "train loss: 6.086581275925234e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4806:\n",
      "train loss: 5.424686914743737e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4807:\n",
      "train loss: 6.257509930960419e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4808:\n",
      "train loss: 5.6456039405392784e-11\n",
      "Epoch 04810: reducing learning rate of group 0 to 6.9888e-12.\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4809:\n",
      "train loss: 6.005402465429327e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4810:\n",
      "train loss: 5.37515861745415e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4811:\n",
      "train loss: 5.705561337736325e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4812:\n",
      "train loss: 5.1173897640473085e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4813:\n",
      "train loss: 5.949623421736259e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4814:\n",
      "train loss: 5.3429573148973033e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4815:\n",
      "train loss: 5.751561013001373e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4816:\n",
      "train loss: 5.174924018758764e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4817:\n",
      "train loss: 5.888836364566294e-11\n",
      "Epoch 04819: reducing learning rate of group 0 to 6.6393e-12.\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4818:\n",
      "train loss: 5.281755238875219e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4819:\n",
      "train loss: 5.8143927235197935e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4820:\n",
      "train loss: 5.263892507803394e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4821:\n",
      "train loss: 5.261130834663772e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4822:\n",
      "train loss: 4.703269187461358e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4823:\n",
      "train loss: 5.816062561702432e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4824:\n",
      "train loss: 5.244783575406812e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4825:\n",
      "train loss: 5.299079521146422e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4826:\n",
      "train loss: 4.7574566681276114e-11\n",
      "Epoch 04828: reducing learning rate of group 0 to 6.3074e-12.\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4827:\n",
      "train loss: 5.7523504822650066e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4828:\n",
      "train loss: 5.175317122320959e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4829:\n",
      "train loss: 4.8474640300821024e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4830:\n",
      "train loss: 4.3408773507514265e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4831:\n",
      "train loss: 5.6351432667146933e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4832:\n",
      "train loss: 5.0798737049558545e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4833:\n",
      "train loss: 4.948598067961949e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4834:\n",
      "train loss: 4.446147137025362e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4835:\n",
      "train loss: 5.5297513713758187e-11\n",
      "Epoch 04837: reducing learning rate of group 0 to 5.9920e-12.\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4836:\n",
      "train loss: 4.9768780994086305e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4837:\n",
      "train loss: 5.048300798118744e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4838:\n",
      "train loss: 4.5672845156693925e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4839:\n",
      "train loss: 4.916052947566072e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4840:\n",
      "train loss: 4.3988344520016794e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4841:\n",
      "train loss: 5.116392213104014e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4842:\n",
      "train loss: 4.627822207065792e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4843:\n",
      "train loss: 4.862596184705855e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4844:\n",
      "train loss: 4.3525653464009545e-11\n",
      "Epoch 04846: reducing learning rate of group 0 to 5.6924e-12.\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4845:\n",
      "train loss: 5.156174005209138e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4846:\n",
      "train loss: 4.6626425907009354e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4847:\n",
      "train loss: 4.357394244053191e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4848:\n",
      "train loss: 3.877698156716606e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4849:\n",
      "train loss: 5.1507828604314256e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4850:\n",
      "train loss: 4.6779584403909745e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4851:\n",
      "train loss: 4.3453290231497825e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4852:\n",
      "train loss: 3.868873511092169e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4853:\n",
      "train loss: 5.156798904378291e-11\n",
      "Epoch 04855: reducing learning rate of group 0 to 5.4078e-12.\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4854:\n",
      "train loss: 4.6820565224918185e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4855:\n",
      "train loss: 4.342493048219315e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4856:\n",
      "train loss: 3.891099800307058e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4857:\n",
      "train loss: 4.6820480484788726e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4858:\n",
      "train loss: 4.230580711384507e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4859:\n",
      "train loss: 4.3427457762358065e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4860:\n",
      "train loss: 3.8917710386813206e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4861:\n",
      "train loss: 4.680907290540598e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4862:\n",
      "train loss: 4.2293149721005724e-11\n",
      "Epoch 04864: reducing learning rate of group 0 to 5.1374e-12.\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4863:\n",
      "train loss: 4.343760614560794e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4864:\n",
      "train loss: 3.8927861360709594e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4865:\n",
      "train loss: 4.250880384636527e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4866:\n",
      "train loss: 3.821979383617101e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4867:\n",
      "train loss: 4.322033670164889e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4868:\n",
      "train loss: 3.893528202197578e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4869:\n",
      "train loss: 4.249943579223356e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4870:\n",
      "train loss: 3.821187696503222e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4871:\n",
      "train loss: 4.3224295478369397e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4872:\n",
      "train loss: 3.893960197373298e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4873:\n",
      "train loss: 4.249208092769185e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4874:\n",
      "train loss: 3.8205355048184444e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4875:\n",
      "train loss: 4.32265594077206e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4876:\n",
      "train loss: 3.894160867902323e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4877:\n",
      "train loss: 4.2487680224816716e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4878:\n",
      "train loss: 3.8200382798717976e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4879:\n",
      "train loss: 4.322660248135755e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4880:\n",
      "train loss: 3.8941741532710564e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4881:\n",
      "train loss: 4.2483457294020687e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4882:\n",
      "train loss: 3.81969501376595e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4883:\n",
      "train loss: 4.3227238416681626e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4884:\n",
      "train loss: 3.894137806035663e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4885:\n",
      "train loss: 4.2480477238010166e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4886:\n",
      "train loss: 3.819448443473379e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4887:\n",
      "train loss: 4.3225803105770554e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4888:\n",
      "train loss: 3.894076498081461e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4889:\n",
      "train loss: 4.247832550913654e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4890:\n",
      "train loss: 3.8192721646194296e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4891:\n",
      "train loss: 4.322541032569843e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4892:\n",
      "train loss: 3.8939673934777044e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4893:\n",
      "train loss: 4.247617720434855e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4894:\n",
      "train loss: 3.81916321295803e-11\n",
      "Epoch 04896: reducing learning rate of group 0 to 4.8805e-12.\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4895:\n",
      "train loss: 4.322386545035918e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4896:\n",
      "train loss: 3.893888443473591e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4897:\n",
      "train loss: 3.840404474679078e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4898:\n",
      "train loss: 3.433328316495714e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4899:\n",
      "train loss: 4.3007826634290786e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4900:\n",
      "train loss: 3.8936233841655226e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4901:\n",
      "train loss: 3.8404242090201613e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4902:\n",
      "train loss: 3.433377967777188e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4903:\n",
      "train loss: 4.3002655323450905e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4904:\n",
      "train loss: 3.893191824190282e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4905:\n",
      "train loss: 3.840656304081922e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4906:\n",
      "train loss: 3.433619624835617e-11\n",
      "Epoch 04908: reducing learning rate of group 0 to 4.6365e-12.\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4907:\n",
      "train loss: 4.299726369734791e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4908:\n",
      "train loss: 3.892597625608606e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4909:\n",
      "train loss: 3.4542939679245623e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4910:\n",
      "train loss: 3.067798283921611e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4911:\n",
      "train loss: 4.278532033599143e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4912:\n",
      "train loss: 3.891683582978302e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4913:\n",
      "train loss: 3.455150197853707e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4914:\n",
      "train loss: 3.0687156587427035e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4915:\n",
      "train loss: 4.277167192886174e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4916:\n",
      "train loss: 3.890248132946925e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4917:\n",
      "train loss: 3.456437742522253e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4918:\n",
      "train loss: 3.0701349803938076e-11\n",
      "Epoch 04920: reducing learning rate of group 0 to 4.4047e-12.\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4919:\n",
      "train loss: 4.275411421394321e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4920:\n",
      "train loss: 3.888281542020996e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4921:\n",
      "train loss: 3.0907931548778903e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4922:\n",
      "train loss: 2.7240451474494416e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4923:\n",
      "train loss: 4.25377478977208e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4924:\n",
      "train loss: 3.8857306799684603e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4925:\n",
      "train loss: 3.0933977193751405e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4926:\n",
      "train loss: 2.726933769373067e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4927:\n",
      "train loss: 4.2502924656547653e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4928:\n",
      "train loss: 3.8821199179201076e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4929:\n",
      "train loss: 3.09701684294468e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4930:\n",
      "train loss: 2.7307221964672495e-11\n",
      "Epoch 04932: reducing learning rate of group 0 to 4.1844e-12.\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4931:\n",
      "train loss: 4.245998597036758e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4932:\n",
      "train loss: 3.8775081427699886e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4933:\n",
      "train loss: 2.752553126763793e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4934:\n",
      "train loss: 2.405095761407849e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4935:\n",
      "train loss: 4.2218737919470936e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4936:\n",
      "train loss: 3.8711620850398305e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4937:\n",
      "train loss: 2.7591596493939163e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4938:\n",
      "train loss: 2.4122517850532256e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4939:\n",
      "train loss: 4.2138168454432225e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4940:\n",
      "train loss: 3.8625343700186546e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4941:\n",
      "train loss: 2.768092045879758e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4942:\n",
      "train loss: 2.4218434930641474e-11\n",
      "Epoch 04944: reducing learning rate of group 0 to 3.9752e-12.\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4943:\n",
      "train loss: 4.203283190809813e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4944:\n",
      "train loss: 3.8512828330354626e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4945:\n",
      "train loss: 2.4482576567599458e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4946:\n",
      "train loss: 2.1203920237694575e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4947:\n",
      "train loss: 4.171291659476545e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4948:\n",
      "train loss: 3.8352249506075814e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4949:\n",
      "train loss: 2.4654304220452167e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4950:\n",
      "train loss: 2.139367135335149e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4951:\n",
      "train loss: 4.150039168706222e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4952:\n",
      "train loss: 3.812242222004837e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4953:\n",
      "train loss: 2.489774784291852e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4954:\n",
      "train loss: 2.1654838429498226e-11\n",
      "Epoch 04956: reducing learning rate of group 0 to 3.7765e-12.\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4955:\n",
      "train loss: 4.121175854027235e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4956:\n",
      "train loss: 3.7813543600589974e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4957:\n",
      "train loss: 2.207225592495318e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4958:\n",
      "train loss: 1.902661520004328e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4959:\n",
      "train loss: 4.0633223338004725e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4960:\n",
      "train loss: 3.735268829954169e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4961:\n",
      "train loss: 2.2574733277188877e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4962:\n",
      "train loss: 1.9581453034248192e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4963:\n",
      "train loss: 4.001160648622225e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4964:\n",
      "train loss: 3.6676821739835015e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4965:\n",
      "train loss: 2.3293886034150467e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4966:\n",
      "train loss: 2.0354166611094577e-11\n",
      "Epoch 04968: reducing learning rate of group 0 to 3.5876e-12.\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4967:\n",
      "train loss: 3.9167017649377683e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4968:\n",
      "train loss: 3.5773701891798276e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4969:\n",
      "train loss: 2.1243219644549582e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4970:\n",
      "train loss: 1.8541627908049373e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4971:\n",
      "train loss: 3.7853839588819365e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4972:\n",
      "train loss: 3.450310260660851e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4973:\n",
      "train loss: 2.2615761864602064e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4974:\n",
      "train loss: 2.0016690129020845e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4975:\n",
      "train loss: 3.626487882839447e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4976:\n",
      "train loss: 3.2823068612819674e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4977:\n",
      "train loss: 2.4360843521515167e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4978:\n",
      "train loss: 2.182243980705073e-11\n",
      "Epoch 04980: reducing learning rate of group 0 to 3.4083e-12.\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4979:\n",
      "train loss: 3.440307661329739e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4980:\n",
      "train loss: 3.0919052547271156e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4981:\n",
      "train loss: 2.3433202597025533e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4982:\n",
      "train loss: 2.10740733559868e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4983:\n",
      "train loss: 3.225160053908931e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4984:\n",
      "train loss: 2.8863763609783872e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4985:\n",
      "train loss: 2.5535301459214298e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4986:\n",
      "train loss: 2.319044746670598e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4987:\n",
      "train loss: 3.0158518276116346e-11\n",
      "Epoch 04989: reducing learning rate of group 0 to 3.2378e-12.\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4988:\n",
      "train loss: 2.680702714229234e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4989:\n",
      "train loss: 2.7526837248415903e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4990:\n",
      "train loss: 2.5200777153275162e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4991:\n",
      "train loss: 2.564326854451958e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4992:\n",
      "train loss: 2.264102076564371e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4993:\n",
      "train loss: 2.8738892957797788e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4994:\n",
      "train loss: 2.6168213527162913e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4995:\n",
      "train loss: 2.4915805934305515e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4996:\n",
      "train loss: 2.2148808194915857e-11\n",
      "Epoch 04998: reducing learning rate of group 0 to 3.0760e-12.\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4997:\n",
      "train loss: 2.8994054289806585e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4998:\n",
      "train loss: 2.620299277448438e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4999:\n",
      "train loss: 2.2532723917178216e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 5000:\n",
      "train loss: 2.013488197836024e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 5001:\n",
      "train loss: 2.8173996607055044e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 5002:\n",
      "train loss: 2.5262506768622747e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 5003:\n",
      "train loss: 2.3692695490592733e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 5004:\n",
      "train loss: 2.1456922067714294e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 5005:\n",
      "train loss: 2.676257836788295e-11\n",
      "Epoch 05007: reducing learning rate of group 0 to 2.9222e-12.\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5006:\n",
      "train loss: 2.3789541125316382e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5007:\n",
      "train loss: 2.5181459829461088e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5008:\n",
      "train loss: 2.3009951192399623e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5009:\n",
      "train loss: 2.2946722481257432e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5010:\n",
      "train loss: 2.030677757026256e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5011:\n",
      "train loss: 2.5959785455772293e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5012:\n",
      "train loss: 2.3521285674531728e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5013:\n",
      "train loss: 2.2693005328977036e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 5014:\n",
      "train loss: 2.029059709043233e-11\n",
      "Epoch 05016: reducing learning rate of group 0 to 2.7760e-12.\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5015:\n",
      "train loss: 2.576130972735509e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5016:\n",
      "train loss: 2.3130190366955048e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5017:\n",
      "train loss: 2.0950416411351733e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5018:\n",
      "train loss: 1.8853119633548994e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5019:\n",
      "train loss: 2.4691187617406525e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5020:\n",
      "train loss: 2.199718862458929e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5021:\n",
      "train loss: 2.223425868260878e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5022:\n",
      "train loss: 2.0221178266399456e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 5023:\n",
      "train loss: 2.3328223713966412e-11\n",
      "Epoch 05025: reducing learning rate of group 0 to 2.6372e-12.\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5024:\n",
      "train loss: 2.0665473094594396e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5025:\n",
      "train loss: 2.3491987811815624e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5026:\n",
      "train loss: 2.1459036939607504e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5027:\n",
      "train loss: 2.012450829205341e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5028:\n",
      "train loss: 1.7848331087315245e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5029:\n",
      "train loss: 2.378298970789066e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5030:\n",
      "train loss: 2.1435969492430275e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5031:\n",
      "train loss: 2.042798233097447e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5032:\n",
      "train loss: 1.8385491495803366e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5033:\n",
      "train loss: 2.3072753884219443e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5034:\n",
      "train loss: 2.058522982735333e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5035:\n",
      "train loss: 2.13793591936214e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 5036:\n",
      "train loss: 1.9389303674579883e-11\n",
      "Epoch 05038: reducing learning rate of group 0 to 2.5054e-12.\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5037:\n",
      "train loss: 2.2082128196501695e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5038:\n",
      "train loss: 1.9626526942768698e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5039:\n",
      "train loss: 2.0203821141078323e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5040:\n",
      "train loss: 1.828011782782968e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5041:\n",
      "train loss: 2.114282041845136e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5042:\n",
      "train loss: 1.882158331707382e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5043:\n",
      "train loss: 2.0987631031177277e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5044:\n",
      "train loss: 1.9017190594657647e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 5045:\n",
      "train loss: 2.048961083512938e-11\n",
      "Epoch 05047: reducing learning rate of group 0 to 2.3801e-12.\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5046:\n",
      "train loss: 1.8257192689309293e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5047:\n",
      "train loss: 2.1456635913938503e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5048:\n",
      "train loss: 1.9472540571387585e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5049:\n",
      "train loss: 1.8219578061486084e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5050:\n",
      "train loss: 1.6277417823820326e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5051:\n",
      "train loss: 2.1241854522355474e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5052:\n",
      "train loss: 1.9058093680280106e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5053:\n",
      "train loss: 1.879944269880805e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5054:\n",
      "train loss: 1.6983245427891227e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5055:\n",
      "train loss: 2.0470525100883168e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5056:\n",
      "train loss: 1.8244220725647685e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5057:\n",
      "train loss: 1.9630972042929798e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 5058:\n",
      "train loss: 1.7803417108495718e-11\n",
      "Epoch 05060: reducing learning rate of group 0 to 2.2611e-12.\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5059:\n",
      "train loss: 1.9702084365294508e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5060:\n",
      "train loss: 1.7537899226274772e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5061:\n",
      "train loss: 1.838845908793457e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5062:\n",
      "train loss: 1.6605839805768858e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5063:\n",
      "train loss: 1.9056580948812103e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5064:\n",
      "train loss: 1.7019235891518124e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5065:\n",
      "train loss: 1.889126860157811e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5066:\n",
      "train loss: 1.7080432621674082e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 5067:\n",
      "train loss: 1.8632082971860946e-11\n",
      "Epoch 05069: reducing learning rate of group 0 to 2.1481e-12.\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5068:\n",
      "train loss: 1.664641197131604e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5069:\n",
      "train loss: 1.9211646069443487e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5070:\n",
      "train loss: 1.7426225263389503e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5071:\n",
      "train loss: 1.659772804954577e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5072:\n",
      "train loss: 1.4823291581120708e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5073:\n",
      "train loss: 1.9108754944593204e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5074:\n",
      "train loss: 1.7196810544201475e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5075:\n",
      "train loss: 1.6937155119817436e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5076:\n",
      "train loss: 1.524843078035179e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5077:\n",
      "train loss: 1.863028363679808e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5078:\n",
      "train loss: 1.6677482612596596e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5079:\n",
      "train loss: 1.7481509066191632e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 5080:\n",
      "train loss: 1.580282455617723e-11\n",
      "Epoch 05082: reducing learning rate of group 0 to 2.0406e-12.\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5081:\n",
      "train loss: 1.809116205557813e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5082:\n",
      "train loss: 1.6161094931924494e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5083:\n",
      "train loss: 1.6273347636744015e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5084:\n",
      "train loss: 1.4668615620090015e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5085:\n",
      "train loss: 1.7529994982165915e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5086:\n",
      "train loss: 1.5689537050438993e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5087:\n",
      "train loss: 1.6749557140735256e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5088:\n",
      "train loss: 1.5138242572127786e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5089:\n",
      "train loss: 1.708268018219292e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5090:\n",
      "train loss: 1.5269755187495954e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5091:\n",
      "train loss: 1.7138606822271275e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 5092:\n",
      "train loss: 1.5491939759332177e-11\n",
      "Epoch 05094: reducing learning rate of group 0 to 1.9386e-12.\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5093:\n",
      "train loss: 1.6771498308746346e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5094:\n",
      "train loss: 1.500152093766515e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5095:\n",
      "train loss: 1.5754521358180356e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5096:\n",
      "train loss: 1.4170582587005518e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5097:\n",
      "train loss: 1.6488136109843933e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5098:\n",
      "train loss: 1.4803366193988014e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5099:\n",
      "train loss: 1.595975689074093e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5100:\n",
      "train loss: 1.437819438525506e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5101:\n",
      "train loss: 1.628566169681454e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5102:\n",
      "train loss: 1.46090701347152e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5103:\n",
      "train loss: 1.6145052936627736e-11\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 5104:\n",
      "train loss: 1.455240009984717e-11\n",
      "Epoch 05106: reducing learning rate of group 0 to 1.8417e-12.\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5105:\n",
      "train loss: 1.61268165032454e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5106:\n",
      "train loss: 1.4466627741401764e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5107:\n",
      "train loss: 1.4741772015803212e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5108:\n",
      "train loss: 1.322936300158967e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5109:\n",
      "train loss: 1.5902363295878058e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5110:\n",
      "train loss: 1.4305923720815916e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5111:\n",
      "train loss: 1.4923766922008095e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5112:\n",
      "train loss: 1.3426792806769217e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5113:\n",
      "train loss: 1.569801106762798e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5114:\n",
      "train loss: 1.4098351681868329e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5115:\n",
      "train loss: 1.513248511402077e-11\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 5116:\n",
      "train loss: 1.3633867494444045e-11\n",
      "Epoch 05118: reducing learning rate of group 0 to 1.7496e-12.\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5117:\n",
      "train loss: 1.5498313946211134e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5118:\n",
      "train loss: 1.3907400804331773e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5119:\n",
      "train loss: 1.3858301521037253e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5120:\n",
      "train loss: 1.2438471338463646e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5121:\n",
      "train loss: 1.522461400798538e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5122:\n",
      "train loss: 1.3697039282292244e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5123:\n",
      "train loss: 1.4084045998303226e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5124:\n",
      "train loss: 1.2676227271676063e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5125:\n",
      "train loss: 1.4982594402081223e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5126:\n",
      "train loss: 1.345407800021148e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5127:\n",
      "train loss: 1.4325394756132264e-11\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 5128:\n",
      "train loss: 1.2911014998947288e-11\n",
      "Epoch 05130: reducing learning rate of group 0 to 1.6621e-12.\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5129:\n",
      "train loss: 1.4756868554651624e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5130:\n",
      "train loss: 1.3238543932522715e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5131:\n",
      "train loss: 1.3143519274915575e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5132:\n",
      "train loss: 1.1801071797621459e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5133:\n",
      "train loss: 1.4476464599266555e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5134:\n",
      "train loss: 1.3024078716507075e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5135:\n",
      "train loss: 1.3369813077064154e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5136:\n",
      "train loss: 1.2033132707795549e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5137:\n",
      "train loss: 1.4242280498907637e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5138:\n",
      "train loss: 1.2792055594492416e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5139:\n",
      "train loss: 1.3597626754096397e-11\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 5140:\n",
      "train loss: 1.2254709521614852e-11\n",
      "Epoch 05142: reducing learning rate of group 0 to 1.5790e-12.\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5141:\n",
      "train loss: 1.4030709566027238e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5142:\n",
      "train loss: 1.2590705405811917e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5143:\n",
      "train loss: 1.24719547541172e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5144:\n",
      "train loss: 1.1194646422412637e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5145:\n",
      "train loss: 1.3769770781420085e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5146:\n",
      "train loss: 1.2393792886304833e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5147:\n",
      "train loss: 1.267645318331562e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5148:\n",
      "train loss: 1.1404282401217218e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5149:\n",
      "train loss: 1.3559876879733184e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5150:\n",
      "train loss: 1.2185002933089489e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5151:\n",
      "train loss: 1.2882127107963192e-11\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 5152:\n",
      "train loss: 1.1605376418187927e-11\n",
      "Epoch 05154: reducing learning rate of group 0 to 1.5001e-12.\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5153:\n",
      "train loss: 1.3366782746021296e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5154:\n",
      "train loss: 1.200202644556593e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5155:\n",
      "train loss: 1.1805411957115663e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5156:\n",
      "train loss: 1.0590838134230926e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5157:\n",
      "train loss: 1.3125597870937899e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5158:\n",
      "train loss: 1.1819948220988264e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5159:\n",
      "train loss: 1.1993411630068256e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5160:\n",
      "train loss: 1.0785295380797442e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5161:\n",
      "train loss: 1.2930742366394711e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5162:\n",
      "train loss: 1.1626304591379144e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5163:\n",
      "train loss: 1.2184748016525832e-11\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 5164:\n",
      "train loss: 1.0972674058684026e-11\n",
      "Epoch 05166: reducing learning rate of group 0 to 1.4251e-12.\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5165:\n",
      "train loss: 1.274905214674007e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5166:\n",
      "train loss: 1.1452517064777635e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5167:\n",
      "train loss: 1.1162423183851599e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5168:\n",
      "train loss: 1.0010270971880696e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5169:\n",
      "train loss: 1.251982163386781e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5170:\n",
      "train loss: 1.128040044899378e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5171:\n",
      "train loss: 1.1340982874198888e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5172:\n",
      "train loss: 1.0193332575083693e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5173:\n",
      "train loss: 1.2334247098710616e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5174:\n",
      "train loss: 1.1096244647259015e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5175:\n",
      "train loss: 1.152282006808137e-11\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 5176:\n",
      "train loss: 1.03730212425451e-11\n",
      "Epoch 05178: reducing learning rate of group 0 to 1.3538e-12.\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5177:\n",
      "train loss: 1.216115342339739e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5178:\n",
      "train loss: 1.092917080221241e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5179:\n",
      "train loss: 1.0554287972703538e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5180:\n",
      "train loss: 9.460264197317832e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5181:\n",
      "train loss: 1.1940463353078447e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5182:\n",
      "train loss: 1.0763027185757037e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5183:\n",
      "train loss: 1.0726002772221552e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5184:\n",
      "train loss: 9.636026845588346e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5185:\n",
      "train loss: 1.1762934959557125e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5186:\n",
      "train loss: 1.0585860696852427e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5187:\n",
      "train loss: 1.089955143312082e-11\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 5188:\n",
      "train loss: 9.808080037326383e-12\n",
      "Epoch 05190: reducing learning rate of group 0 to 1.2861e-12.\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5189:\n",
      "train loss: 1.159591836352733e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5190:\n",
      "train loss: 1.0425560720711933e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5191:\n",
      "train loss: 9.982188618342913e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5192:\n",
      "train loss: 8.945387369051433e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5193:\n",
      "train loss: 1.1383389442433241e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5194:\n",
      "train loss: 1.0265041126184629e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5195:\n",
      "train loss: 1.0147517386356394e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5196:\n",
      "train loss: 9.112355766040346e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5197:\n",
      "train loss: 1.121418652810332e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5198:\n",
      "train loss: 1.0096740762424036e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5199:\n",
      "train loss: 1.0313955101183577e-11\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 5200:\n",
      "train loss: 9.27686080886103e-12\n",
      "Epoch 05202: reducing learning rate of group 0 to 1.2218e-12.\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5201:\n",
      "train loss: 1.1053919235703584e-11\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5202:\n",
      "train loss: 9.941289504326028e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5203:\n",
      "train loss: 9.444073343816999e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5204:\n",
      "train loss: 8.458814282671902e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5205:\n",
      "train loss: 1.0850966672416418e-11\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5206:\n",
      "train loss: 9.788472929092729e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5207:\n",
      "train loss: 9.601525046581453e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5208:\n",
      "train loss: 8.61959433367293e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5209:\n",
      "train loss: 1.0689831158346923e-11\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5210:\n",
      "train loss: 9.627655726647427e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5211:\n",
      "train loss: 9.759899802393067e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 5212:\n",
      "train loss: 8.77451674430596e-12\n",
      "Epoch 05214: reducing learning rate of group 0 to 1.1607e-12.\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5213:\n",
      "train loss: 1.0537052385040078e-11\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5214:\n",
      "train loss: 9.480002417340107e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5215:\n",
      "train loss: 8.935075135981273e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5216:\n",
      "train loss: 8.000351069536877e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5217:\n",
      "train loss: 1.0342271629399548e-11\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5218:\n",
      "train loss: 9.333635575646197e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5219:\n",
      "train loss: 9.085974924247825e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5220:\n",
      "train loss: 8.152922004785594e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5221:\n",
      "train loss: 1.018790358014309e-11\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5222:\n",
      "train loss: 9.179123330738077e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5223:\n",
      "train loss: 9.237533492033766e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 5224:\n",
      "train loss: 8.30291393512456e-12\n",
      "Epoch 05226: reducing learning rate of group 0 to 1.1027e-12.\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5225:\n",
      "train loss: 1.004088682549385e-11\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5226:\n",
      "train loss: 9.036596994567963e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5227:\n",
      "train loss: 8.45588047506239e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5228:\n",
      "train loss: 7.567910250687182e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5229:\n",
      "train loss: 9.854509540989248e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5230:\n",
      "train loss: 8.896395159826984e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5231:\n",
      "train loss: 8.600373704486622e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5232:\n",
      "train loss: 7.714644949920926e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5233:\n",
      "train loss: 9.70609812857063e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5234:\n",
      "train loss: 8.747486537204122e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5235:\n",
      "train loss: 8.746284671419426e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 5236:\n",
      "train loss: 7.857974874424377e-12\n",
      "Epoch 05238: reducing learning rate of group 0 to 1.0476e-12.\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5237:\n",
      "train loss: 9.564930020541164e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5238:\n",
      "train loss: 8.611215707621946e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5239:\n",
      "train loss: 8.005082543960669e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5240:\n",
      "train loss: 7.161713760817289e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5241:\n",
      "train loss: 9.385709769454938e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5242:\n",
      "train loss: 8.475160677851826e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5243:\n",
      "train loss: 8.143592877894535e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5244:\n",
      "train loss: 7.303277312744637e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5245:\n",
      "train loss: 9.243087971601619e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5246:\n",
      "train loss: 8.333015095417028e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5247:\n",
      "train loss: 8.283865207878792e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 5248:\n",
      "train loss: 7.442062499454779e-12\n",
      "Epoch 05250: reducing learning rate of group 0 to 9.9517e-13.\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5249:\n",
      "train loss: 9.10801808176661e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5250:\n",
      "train loss: 8.200481831983444e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5251:\n",
      "train loss: 7.582503312724253e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5252:\n",
      "train loss: 6.782684089162451e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5253:\n",
      "train loss: 8.935293710401689e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5254:\n",
      "train loss: 8.070063204405464e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5255:\n",
      "train loss: 7.716305545116906e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5256:\n",
      "train loss: 6.918312218787225e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5257:\n",
      "train loss: 8.798864699621345e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5258:\n",
      "train loss: 7.933670243073706e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5259:\n",
      "train loss: 7.850499713874027e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 5260:\n",
      "train loss: 7.051037559239123e-12\n",
      "Epoch 05262: reducing learning rate of group 0 to 9.4541e-13.\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5261:\n",
      "train loss: 8.669106761250696e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5262:\n",
      "train loss: 7.806824831894368e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5263:\n",
      "train loss: 7.185202163738577e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5264:\n",
      "train loss: 6.426813400059499e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5265:\n",
      "train loss: 8.502999492450863e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5266:\n",
      "train loss: 7.681531802077295e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5267:\n",
      "train loss: 7.312850706311738e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5268:\n",
      "train loss: 6.5566237723217884e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5269:\n",
      "train loss: 8.372431979374546e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5270:\n",
      "train loss: 7.5508388539298e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5271:\n",
      "train loss: 7.441854151914241e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 5272:\n",
      "train loss: 6.682662202512333e-12\n",
      "Epoch 05274: reducing learning rate of group 0 to 8.9814e-13.\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5273:\n",
      "train loss: 8.248058451504539e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5274:\n",
      "train loss: 7.42888468630746e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5275:\n",
      "train loss: 6.8117645764996416e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5276:\n",
      "train loss: 6.089716985290608e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5277:\n",
      "train loss: 8.090613027959262e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5278:\n",
      "train loss: 7.30995401707364e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5279:\n",
      "train loss: 6.93351694785458e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5280:\n",
      "train loss: 6.213499870144966e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5281:\n",
      "train loss: 7.965849584877685e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5282:\n",
      "train loss: 7.18517278857654e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5283:\n",
      "train loss: 7.056009802549625e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 5284:\n",
      "train loss: 6.334991910137686e-12\n",
      "Epoch 05286: reducing learning rate of group 0 to 8.5324e-13.\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5285:\n",
      "train loss: 7.84713722735921e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5286:\n",
      "train loss: 7.069806851754625e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5287:\n",
      "train loss: 6.455923778149914e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5288:\n",
      "train loss: 5.772317446862579e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5289:\n",
      "train loss: 7.697647364268121e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5290:\n",
      "train loss: 6.9562102246658385e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5291:\n",
      "train loss: 6.572018378103607e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5292:\n",
      "train loss: 5.889296949114599e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5293:\n",
      "train loss: 7.579067668327097e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5294:\n",
      "train loss: 6.838043898700547e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5295:\n",
      "train loss: 6.688716163657595e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5296:\n",
      "train loss: 6.003420596219632e-12\n",
      "Epoch 05298: reducing learning rate of group 0 to 8.1057e-13.\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5297:\n",
      "train loss: 7.466971427349626e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5298:\n",
      "train loss: 6.7286631874405706e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5299:\n",
      "train loss: 6.118093392210764e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5300:\n",
      "train loss: 5.4687964726520404e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5301:\n",
      "train loss: 7.326085441098762e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5302:\n",
      "train loss: 6.620782993998537e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5303:\n",
      "train loss: 6.228161618942059e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5304:\n",
      "train loss: 5.579074123750566e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5305:\n",
      "train loss: 7.214492783531826e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5306:\n",
      "train loss: 6.510836957801721e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5307:\n",
      "train loss: 6.3367391875545716e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5308:\n",
      "train loss: 5.6866024627285545e-12\n",
      "Epoch 05310: reducing learning rate of group 0 to 7.7005e-13.\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5309:\n",
      "train loss: 7.107971108517292e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5310:\n",
      "train loss: 6.4066100268953696e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5311:\n",
      "train loss: 5.795093729981703e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5312:\n",
      "train loss: 5.1775736210504e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5313:\n",
      "train loss: 6.975578986616701e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5314:\n",
      "train loss: 6.306667126226318e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5315:\n",
      "train loss: 5.897689421208581e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5316:\n",
      "train loss: 5.280591421913461e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5317:\n",
      "train loss: 6.871316018352808e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5318:\n",
      "train loss: 6.20378834439131e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5319:\n",
      "train loss: 5.998551728746688e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5320:\n",
      "train loss: 5.380128600901697e-12\n",
      "Epoch 05322: reducing learning rate of group 0 to 7.3154e-13.\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5321:\n",
      "train loss: 6.772648979685275e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5322:\n",
      "train loss: 6.1075593693594945e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5323:\n",
      "train loss: 5.482674139927322e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5324:\n",
      "train loss: 4.894166777498258e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5325:\n",
      "train loss: 6.648945944537242e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5326:\n",
      "train loss: 6.01479295921361e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5327:\n",
      "train loss: 5.576232413284268e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5328:\n",
      "train loss: 4.990040629745472e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5329:\n",
      "train loss: 6.5523747806211044e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5330:\n",
      "train loss: 5.918820313536486e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5331:\n",
      "train loss: 5.6706140653612744e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5332:\n",
      "train loss: 5.082823097916611e-12\n",
      "Epoch 05334: reducing learning rate of group 0 to 6.9497e-13.\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5333:\n",
      "train loss: 6.461787002820883e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5334:\n",
      "train loss: 5.83005279901919e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5335:\n",
      "train loss: 5.176982111343401e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5336:\n",
      "train loss: 4.618946639898134e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5337:\n",
      "train loss: 6.344623212979573e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5338:\n",
      "train loss: 5.744048736797282e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5339:\n",
      "train loss: 5.264178695860578e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5340:\n",
      "train loss: 4.706952543351562e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5341:\n",
      "train loss: 6.256782002442638e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5342:\n",
      "train loss: 5.655115203259315e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5343:\n",
      "train loss: 5.3509823544423335e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5344:\n",
      "train loss: 4.7932592605820154e-12\n",
      "Epoch 05346: reducing learning rate of group 0 to 6.6022e-13.\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5345:\n",
      "train loss: 6.171708086810548e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5346:\n",
      "train loss: 5.572824084358627e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5347:\n",
      "train loss: 4.8812765018558665e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5348:\n",
      "train loss: 4.351422495967418e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5349:\n",
      "train loss: 6.063206846276969e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5350:\n",
      "train loss: 5.4927446342327975e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5351:\n",
      "train loss: 4.962237730320211e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5352:\n",
      "train loss: 4.431885323203318e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5353:\n",
      "train loss: 5.981361035964781e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5354:\n",
      "train loss: 5.411068921087797e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5355:\n",
      "train loss: 5.042053065453396e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5356:\n",
      "train loss: 4.5114139312318005e-12\n",
      "Epoch 05358: reducing learning rate of group 0 to 6.2721e-13.\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5357:\n",
      "train loss: 5.903231734890278e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5358:\n",
      "train loss: 5.333761866583023e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5359:\n",
      "train loss: 4.595500012745766e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5360:\n",
      "train loss: 4.09065088186188e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5361:\n",
      "train loss: 5.801125750265963e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5362:\n",
      "train loss: 5.2595002175282024e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5363:\n",
      "train loss: 4.670036448807987e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5364:\n",
      "train loss: 4.166826450633912e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5365:\n",
      "train loss: 5.723690255121493e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5366:\n",
      "train loss: 5.183062830540241e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5367:\n",
      "train loss: 4.745888010179361e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5368:\n",
      "train loss: 4.241606234321907e-12\n",
      "Epoch 05370: reducing learning rate of group 0 to 5.9585e-13.\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5369:\n",
      "train loss: 5.648703229217743e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5370:\n",
      "train loss: 5.109539755066098e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5371:\n",
      "train loss: 4.3206863226259785e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5372:\n",
      "train loss: 3.8423744373016706e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5373:\n",
      "train loss: 5.551388296407763e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5374:\n",
      "train loss: 5.036763837942832e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5375:\n",
      "train loss: 4.3939830480312915e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5376:\n",
      "train loss: 3.916133902176428e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5377:\n",
      "train loss: 5.475143673937702e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5378:\n",
      "train loss: 4.961670945646533e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5379:\n",
      "train loss: 4.467836651717872e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5380:\n",
      "train loss: 3.989888501200858e-12\n",
      "Epoch 05382: reducing learning rate of group 0 to 5.6605e-13.\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5381:\n",
      "train loss: 5.4024677232842925e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5382:\n",
      "train loss: 4.8892372658772814e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5383:\n",
      "train loss: 4.067261964309136e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5384:\n",
      "train loss: 3.6133898406615135e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5385:\n",
      "train loss: 5.30533789844286e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5386:\n",
      "train loss: 4.815906018779187e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5387:\n",
      "train loss: 4.14059718070065e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5388:\n",
      "train loss: 3.687841225773472e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5389:\n",
      "train loss: 5.22902608202486e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5390:\n",
      "train loss: 4.738673906662974e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5391:\n",
      "train loss: 4.216780068279704e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5392:\n",
      "train loss: 3.764261804034494e-12\n",
      "Epoch 05394: reducing learning rate of group 0 to 5.3775e-13.\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5393:\n",
      "train loss: 5.15232405429188e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5394:\n",
      "train loss: 4.662366797194719e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5395:\n",
      "train loss: 3.843128228141471e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5396:\n",
      "train loss: 3.4142607230858585e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5397:\n",
      "train loss: 5.0516660881832946e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5398:\n",
      "train loss: 4.584275258140115e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5399:\n",
      "train loss: 3.923135602457799e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5400:\n",
      "train loss: 3.4964955110180435e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5401:\n",
      "train loss: 4.966392404842809e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5402:\n",
      "train loss: 4.497649017031058e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5403:\n",
      "train loss: 4.008280452333731e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5404:\n",
      "train loss: 3.5825554355404974e-12\n",
      "Epoch 05406: reducing learning rate of group 0 to 5.1086e-13.\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5405:\n",
      "train loss: 4.880028882228353e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5406:\n",
      "train loss: 4.410966245308988e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5407:\n",
      "train loss: 3.668537072943036e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5408:\n",
      "train loss: 3.264523317324486e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5409:\n",
      "train loss: 4.76777160419078e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5410:\n",
      "train loss: 4.318438834093307e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5411:\n",
      "train loss: 3.7624824930076324e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5412:\n",
      "train loss: 3.362813915766411e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5413:\n",
      "train loss: 4.666318176783156e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5414:\n",
      "train loss: 4.215011570967613e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5415:\n",
      "train loss: 3.865070008531389e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5416:\n",
      "train loss: 3.4646319957713512e-12\n",
      "Epoch 05418: reducing learning rate of group 0 to 4.8532e-13.\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5417:\n",
      "train loss: 4.561925084374644e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5418:\n",
      "train loss: 4.110038233935056e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5419:\n",
      "train loss: 3.5635252595176035e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5420:\n",
      "train loss: 3.186216956730227e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5421:\n",
      "train loss: 4.430515170634876e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5422:\n",
      "train loss: 3.9964578038239005e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5423:\n",
      "train loss: 3.6793631519592835e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5424:\n",
      "train loss: 3.3051699099616716e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5425:\n",
      "train loss: 4.306360829963699e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5426:\n",
      "train loss: 3.870439426622025e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5427:\n",
      "train loss: 3.802708153464139e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5428:\n",
      "train loss: 3.428199245009058e-12\n",
      "Epoch 05430: reducing learning rate of group 0 to 4.6106e-13.\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5429:\n",
      "train loss: 4.18196749923152e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5430:\n",
      "train loss: 3.7486837908477945e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5431:\n",
      "train loss: 3.5379280698285542e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5432:\n",
      "train loss: 3.1820440902797437e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5433:\n",
      "train loss: 4.041183214931314e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5434:\n",
      "train loss: 3.6238409372453586e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5435:\n",
      "train loss: 3.662190208534038e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5436:\n",
      "train loss: 3.309297460596951e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5437:\n",
      "train loss: 3.910719023874967e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5438:\n",
      "train loss: 3.4955269980550048e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5439:\n",
      "train loss: 3.782677105109696e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5440:\n",
      "train loss: 3.423113142466433e-12\n",
      "Epoch 05442: reducing learning rate of group 0 to 4.3800e-13.\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5441:\n",
      "train loss: 3.800831960888208e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5442:\n",
      "train loss: 3.3922878756258517e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5443:\n",
      "train loss: 3.509412921181368e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5444:\n",
      "train loss: 3.1622138645894178e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5445:\n",
      "train loss: 3.694501861959844e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5446:\n",
      "train loss: 3.3042677176782422e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5447:\n",
      "train loss: 3.596475314056595e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5448:\n",
      "train loss: 3.2481894900224144e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5449:\n",
      "train loss: 3.609224488640602e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5450:\n",
      "train loss: 3.2237471951141396e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5451:\n",
      "train loss: 3.6633976599005265e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5452:\n",
      "train loss: 3.3038209071948392e-12\n",
      "Epoch 05454: reducing learning rate of group 0 to 4.1610e-13.\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5453:\n",
      "train loss: 3.5608214852401892e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5454:\n",
      "train loss: 3.1865670820334317e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5455:\n",
      "train loss: 3.3424716264922683e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5456:\n",
      "train loss: 2.9980612348036624e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5457:\n",
      "train loss: 3.508731786881812e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5458:\n",
      "train loss: 3.137458948901158e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5459:\n",
      "train loss: 3.400939399919312e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5460:\n",
      "train loss: 3.0647292328894984e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5461:\n",
      "train loss: 3.433753194890957e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5462:\n",
      "train loss: 3.060869556528188e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5463:\n",
      "train loss: 3.4672168524202118e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5464:\n",
      "train loss: 3.118747686251932e-12\n",
      "Epoch 05466: reducing learning rate of group 0 to 3.9530e-13.\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5465:\n",
      "train loss: 3.3879750231075612e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5466:\n",
      "train loss: 3.0278922808038825e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5467:\n",
      "train loss: 3.1591294805407173e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5468:\n",
      "train loss: 2.828999384998094e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5469:\n",
      "train loss: 3.3264107650986127e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5470:\n",
      "train loss: 2.9539146801201e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5471:\n",
      "train loss: 3.2539263317775976e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5472:\n",
      "train loss: 2.9346649999907136e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5473:\n",
      "train loss: 3.2198580077088265e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5474:\n",
      "train loss: 2.856695239101346e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5475:\n",
      "train loss: 3.3221404117371307e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5476:\n",
      "train loss: 2.9698858176547125e-12\n",
      "Epoch 05478: reducing learning rate of group 0 to 3.7553e-13.\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5477:\n",
      "train loss: 3.214330502109879e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5478:\n",
      "train loss: 2.8772719974781863e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5479:\n",
      "train loss: 2.978642678458741e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5480:\n",
      "train loss: 2.653506719365209e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5481:\n",
      "train loss: 3.16774177161091e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5482:\n",
      "train loss: 2.78469895613147e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5483:\n",
      "train loss: 3.1176701889912722e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5484:\n",
      "train loss: 2.8113393157699326e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5485:\n",
      "train loss: 3.0264902874436896e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5486:\n",
      "train loss: 2.6750525543783605e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5487:\n",
      "train loss: 3.1696819379813894e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5488:\n",
      "train loss: 2.7988364591834535e-12\n",
      "Epoch 05490: reducing learning rate of group 0 to 3.5676e-13.\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5489:\n",
      "train loss: 3.094790426716634e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5490:\n",
      "train loss: 2.7754037842825225e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5491:\n",
      "train loss: 2.7944267647034027e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5492:\n",
      "train loss: 2.489983488298141e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5493:\n",
      "train loss: 3.0048699720790874e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5494:\n",
      "train loss: 2.5886856682678717e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5495:\n",
      "train loss: 3.0462012505834346e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5496:\n",
      "train loss: 2.7436527655948917e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5497:\n",
      "train loss: 2.837265078335439e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5498:\n",
      "train loss: 2.521189828860914e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5499:\n",
      "train loss: 3.0051951547916386e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5500:\n",
      "train loss: 2.597055233313157e-12\n",
      "Epoch 05502: reducing learning rate of group 0 to 3.3892e-13.\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5501:\n",
      "train loss: 3.0443461901987715e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5502:\n",
      "train loss: 2.727182661060969e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5503:\n",
      "train loss: 2.6213325012356523e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5504:\n",
      "train loss: 2.3642019875276672e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5505:\n",
      "train loss: 2.8414663420138373e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5506:\n",
      "train loss: 2.3921372401369597e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5507:\n",
      "train loss: 3.0107278815002006e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5508:\n",
      "train loss: 2.710367695040493e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5509:\n",
      "train loss: 2.6671678427325093e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5510:\n",
      "train loss: 2.401064712501568e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5511:\n",
      "train loss: 2.860131140859044e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5512:\n",
      "train loss: 2.4387278830332445e-12\n",
      "Epoch 05514: reducing learning rate of group 0 to 3.2197e-13.\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5513:\n",
      "train loss: 2.9883419437991586e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5514:\n",
      "train loss: 2.6966407648030907e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5515:\n",
      "train loss: 2.4415486133840354e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5516:\n",
      "train loss: 2.218195136270406e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5517:\n",
      "train loss: 2.7553549713199163e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5518:\n",
      "train loss: 2.3115701887252905e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5519:\n",
      "train loss: 2.906842135189747e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5520:\n",
      "train loss: 2.67452242267009e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5521:\n",
      "train loss: 2.4351110449393897e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5522:\n",
      "train loss: 2.172581931586896e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5523:\n",
      "train loss: 2.867064622760865e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5524:\n",
      "train loss: 2.475846560129066e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5525:\n",
      "train loss: 2.7479461589707015e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5526:\n",
      "train loss: 2.5413895487662578e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5527:\n",
      "train loss: 2.5290637092202857e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5528:\n",
      "train loss: 2.2069282493206306e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5529:\n",
      "train loss: 2.9256364884272905e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5530:\n",
      "train loss: 2.633657959871828e-12\n",
      "Epoch 05532: reducing learning rate of group 0 to 3.0587e-13.\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5531:\n",
      "train loss: 2.5200963645170735e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5532:\n",
      "train loss: 2.2744982634830345e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5533:\n",
      "train loss: 2.5582887868655215e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5534:\n",
      "train loss: 2.2596318558153907e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5535:\n",
      "train loss: 2.638496082532557e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5536:\n",
      "train loss: 2.396799468955625e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5537:\n",
      "train loss: 2.4576521633945136e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5538:\n",
      "train loss: 2.185942660403566e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5539:\n",
      "train loss: 2.683607844733283e-12\n",
      "Epoch 05541: reducing learning rate of group 0 to 2.9058e-13.\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5540:\n",
      "train loss: 2.422636808701007e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5541:\n",
      "train loss: 2.4504061379482685e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5542:\n",
      "train loss: 2.2084580581112848e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5543:\n",
      "train loss: 2.412121335222046e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5544:\n",
      "train loss: 2.160795436881349e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5545:\n",
      "train loss: 2.4646888424683156e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5546:\n",
      "train loss: 2.2184847741689214e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5547:\n",
      "train loss: 2.4044738664073108e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5548:\n",
      "train loss: 2.158084729269784e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5549:\n",
      "train loss: 2.4636763147486426e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5550:\n",
      "train loss: 2.215691004536348e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5551:\n",
      "train loss: 2.410863987204126e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5552:\n",
      "train loss: 2.1666217312592083e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5553:\n",
      "train loss: 2.4549133031307384e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5554:\n",
      "train loss: 2.205289360540035e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5555:\n",
      "train loss: 2.422601921806129e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5556:\n",
      "train loss: 2.1793725975919913e-12\n",
      "Epoch 05558: reducing learning rate of group 0 to 2.7605e-13.\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5557:\n",
      "train loss: 2.442810083528795e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5558:\n",
      "train loss: 2.193913491662866e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5559:\n",
      "train loss: 2.204131465039903e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5560:\n",
      "train loss: 1.975066306306006e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5561:\n",
      "train loss: 2.411467281452863e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5562:\n",
      "train loss: 2.170919636370732e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5563:\n",
      "train loss: 2.231284112703624e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5564:\n",
      "train loss: 2.006459628987922e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5565:\n",
      "train loss: 2.3788309172681986e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5566:\n",
      "train loss: 2.1374020662493675e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5567:\n",
      "train loss: 2.2643201672997937e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5568:\n",
      "train loss: 2.0394701501093333e-12\n",
      "Epoch 05570: reducing learning rate of group 0 to 2.6225e-13.\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5569:\n",
      "train loss: 2.3453690952325987e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5570:\n",
      "train loss: 2.106998000406641e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5571:\n",
      "train loss: 2.0724212449412155e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5572:\n",
      "train loss: 1.8587623645754153e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5573:\n",
      "train loss: 2.308284888714643e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5574:\n",
      "train loss: 2.0818507635465112e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5575:\n",
      "train loss: 2.0967133828572887e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5576:\n",
      "train loss: 1.8826476761685123e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5577:\n",
      "train loss: 2.2842878330738887e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5578:\n",
      "train loss: 2.0593619209237526e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5579:\n",
      "train loss: 2.1166955667051814e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5580:\n",
      "train loss: 1.9013502175572283e-12\n",
      "Epoch 05582: reducing learning rate of group 0 to 2.4914e-13.\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5581:\n",
      "train loss: 2.26681739636143e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5582:\n",
      "train loss: 2.0428665549838026e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5583:\n",
      "train loss: 1.922153646032315e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5584:\n",
      "train loss: 1.716788790554718e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5585:\n",
      "train loss: 2.2427118243650206e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5586:\n",
      "train loss: 2.032346345246181e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5587:\n",
      "train loss: 1.931914984877851e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5588:\n",
      "train loss: 1.7257435622850412e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5589:\n",
      "train loss: 2.234647070105429e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5590:\n",
      "train loss: 2.024531005084172e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5591:\n",
      "train loss: 1.9385655886364397e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5592:\n",
      "train loss: 1.731000145346883e-12\n",
      "Epoch 05594: reducing learning rate of group 0 to 2.3668e-13.\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5593:\n",
      "train loss: 2.2282403622034832e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5594:\n",
      "train loss: 2.017630711452638e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5595:\n",
      "train loss: 1.7458393460526677e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5596:\n",
      "train loss: 1.5495243650776428e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5597:\n",
      "train loss: 2.21223833283536e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5598:\n",
      "train loss: 2.0121263505295848e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5599:\n",
      "train loss: 1.7515563696961177e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5600:\n",
      "train loss: 1.5550978067649737e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5601:\n",
      "train loss: 2.2054727511479206e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5602:\n",
      "train loss: 2.0055737169532234e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5603:\n",
      "train loss: 1.7579305928564794e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5604:\n",
      "train loss: 1.5620382574083759e-12\n",
      "Epoch 05606: reducing learning rate of group 0 to 2.2485e-13.\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5605:\n",
      "train loss: 2.198407894272109e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5606:\n",
      "train loss: 1.997276986923288e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5607:\n",
      "train loss: 1.5780514503258122e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5608:\n",
      "train loss: 1.3920081360581987e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5609:\n",
      "train loss: 2.179176706820081e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5610:\n",
      "train loss: 1.9885350605168154e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5611:\n",
      "train loss: 1.5876036043608696e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5612:\n",
      "train loss: 1.4027181930749046e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5613:\n",
      "train loss: 2.1673583795245407e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5614:\n",
      "train loss: 1.975367243828073e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5615:\n",
      "train loss: 1.6005765699663793e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5616:\n",
      "train loss: 1.4156940616393367e-12\n",
      "Epoch 05618: reducing learning rate of group 0 to 2.1360e-13.\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5617:\n",
      "train loss: 2.1529430024364125e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5618:\n",
      "train loss: 1.9607351608373605e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5619:\n",
      "train loss: 1.435831529045631e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5620:\n",
      "train loss: 1.2619461910188476e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5621:\n",
      "train loss: 2.1269486642255193e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5622:\n",
      "train loss: 1.9442001835584745e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5623:\n",
      "train loss: 1.4543012103188653e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5624:\n",
      "train loss: 1.2807651766052165e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5625:\n",
      "train loss: 2.1075530027826977e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5626:\n",
      "train loss: 1.923126846632414e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5627:\n",
      "train loss: 1.4756437992696198e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5628:\n",
      "train loss: 1.3023208608605405e-12\n",
      "Epoch 05630: reducing learning rate of group 0 to 2.0292e-13.\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5629:\n",
      "train loss: 2.084900217747227e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5630:\n",
      "train loss: 1.9002050382142163e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5631:\n",
      "train loss: 1.3293534765087417e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5632:\n",
      "train loss: 1.1665940069276173e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5633:\n",
      "train loss: 2.0498566546588387e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5634:\n",
      "train loss: 1.87271751410207e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5635:\n",
      "train loss: 1.3581731046040235e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5636:\n",
      "train loss: 1.1952859463296095e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5637:\n",
      "train loss: 2.0179375553894892e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5638:\n",
      "train loss: 1.8397444191473564e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5639:\n",
      "train loss: 1.3917047539195183e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5640:\n",
      "train loss: 1.2309960442154919e-12\n",
      "Epoch 05642: reducing learning rate of group 0 to 1.9278e-13.\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5641:\n",
      "train loss: 1.9819261318325846e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5642:\n",
      "train loss: 1.8020525632340877e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5643:\n",
      "train loss: 1.268122310731229e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5644:\n",
      "train loss: 1.1166928480135708e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5645:\n",
      "train loss: 1.933407186694028e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5646:\n",
      "train loss: 1.7617723813495677e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5647:\n",
      "train loss: 1.3120119539788283e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5648:\n",
      "train loss: 1.1616111724460902e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5649:\n",
      "train loss: 1.886305211214684e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5650:\n",
      "train loss: 1.7128027701033762e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5651:\n",
      "train loss: 1.3611782556207103e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5652:\n",
      "train loss: 1.2116713995072437e-12\n",
      "Epoch 05654: reducing learning rate of group 0 to 1.8314e-13.\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5653:\n",
      "train loss: 1.8370270434807854e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5654:\n",
      "train loss: 1.6630690965180815e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5655:\n",
      "train loss: 1.256965308005302e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5656:\n",
      "train loss: 1.116055150091753e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5657:\n",
      "train loss: 1.7774811263246658e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5658:\n",
      "train loss: 1.611224530547187e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5659:\n",
      "train loss: 1.3101997684306159e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5660:\n",
      "train loss: 1.1698383971886373e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5661:\n",
      "train loss: 1.7257763255329239e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5662:\n",
      "train loss: 1.5586214653328554e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5663:\n",
      "train loss: 1.3622433081632225e-12\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5664:\n",
      "train loss: 1.220377495887742e-12\n",
      "Epoch 05666: reducing learning rate of group 0 to 1.7398e-13.\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5665:\n",
      "train loss: 1.6751741035605605e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5666:\n",
      "train loss: 1.5106964746697241e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5667:\n",
      "train loss: 1.263153541114948e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5668:\n",
      "train loss: 1.1266963614505665e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5669:\n",
      "train loss: 1.6240733735747174e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5670:\n",
      "train loss: 1.4676764535976422e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5671:\n",
      "train loss: 1.303603585744003e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5672:\n",
      "train loss: 1.1683997507907511e-12\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5673:\n",
      "train loss: 1.5849390851015658e-12\n",
      "Epoch 05675: reducing learning rate of group 0 to 1.6528e-13.\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5674:\n",
      "train loss: 1.4306165495743011e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5675:\n",
      "train loss: 1.3401715887366231e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5676:\n",
      "train loss: 1.2095323610534555e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5677:\n",
      "train loss: 1.4090810048138053e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5678:\n",
      "train loss: 1.2640044549387802e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5679:\n",
      "train loss: 1.365340535246346e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5680:\n",
      "train loss: 1.2323586728908727e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5681:\n",
      "train loss: 1.3863214297824298e-12\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5682:\n",
      "train loss: 1.2447758777056015e-12\n",
      "Epoch 05684: reducing learning rate of group 0 to 1.5702e-13.\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5683:\n",
      "train loss: 1.3828937663775956e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5684:\n",
      "train loss: 1.247884679708507e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5685:\n",
      "train loss: 1.2425868340969025e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5686:\n",
      "train loss: 1.108382347203591e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5687:\n",
      "train loss: 1.38658254489437e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5688:\n",
      "train loss: 1.2563473702546277e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5689:\n",
      "train loss: 1.2342633196566818e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5690:\n",
      "train loss: 1.1021057947119047e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5691:\n",
      "train loss: 1.3917377099512394e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5692:\n",
      "train loss: 1.261172294440251e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5693:\n",
      "train loss: 1.2295920736360527e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5694:\n",
      "train loss: 1.0980145035852044e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5695:\n",
      "train loss: 1.3953553255915993e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5696:\n",
      "train loss: 1.2645543741990115e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5697:\n",
      "train loss: 1.227151005890742e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5698:\n",
      "train loss: 1.0949300995595099e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5699:\n",
      "train loss: 1.3964007313028087e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5700:\n",
      "train loss: 1.2658182015153742e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5701:\n",
      "train loss: 1.2260964284171737e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5702:\n",
      "train loss: 1.0939723263907402e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5703:\n",
      "train loss: 1.3978226947455605e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5704:\n",
      "train loss: 1.2670133598255976e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5705:\n",
      "train loss: 1.2251341466266196e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5706:\n",
      "train loss: 1.093495327598212e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5707:\n",
      "train loss: 1.3988242344891392e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5708:\n",
      "train loss: 1.2674397659540301e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5709:\n",
      "train loss: 1.2241705957673255e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5710:\n",
      "train loss: 1.0932670093149678e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5711:\n",
      "train loss: 1.3991253367428103e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5712:\n",
      "train loss: 1.2682204132202904e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5713:\n",
      "train loss: 1.2236008811275903e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5714:\n",
      "train loss: 1.0917404668774665e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5715:\n",
      "train loss: 1.399204264308693e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5716:\n",
      "train loss: 1.268103521342754e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5717:\n",
      "train loss: 1.2232517806381514e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5718:\n",
      "train loss: 1.0919248793003654e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5719:\n",
      "train loss: 1.3994636416535338e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5720:\n",
      "train loss: 1.26795898401878e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5721:\n",
      "train loss: 1.22314739918952e-12\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5722:\n",
      "train loss: 1.091647766466773e-12\n",
      "Epoch 05724: reducing learning rate of group 0 to 1.4917e-13.\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5723:\n",
      "train loss: 1.3994188119431009e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5724:\n",
      "train loss: 1.2682134272006666e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5725:\n",
      "train loss: 1.0981904760387971e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5726:\n",
      "train loss: 9.739960787850925e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5727:\n",
      "train loss: 1.392968776875956e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5728:\n",
      "train loss: 1.2680450625542197e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5729:\n",
      "train loss: 1.0986495994425777e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5730:\n",
      "train loss: 9.741298440023235e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5731:\n",
      "train loss: 1.3927179228464423e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5732:\n",
      "train loss: 1.2685895433333256e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5733:\n",
      "train loss: 1.0981271947472291e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5734:\n",
      "train loss: 9.738055559389515e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5735:\n",
      "train loss: 1.3925647921886607e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5736:\n",
      "train loss: 1.2684912826826662e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5737:\n",
      "train loss: 1.0988984578328135e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5738:\n",
      "train loss: 9.734496639031135e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5739:\n",
      "train loss: 1.3927109784502919e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5740:\n",
      "train loss: 1.2681044443107186e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5741:\n",
      "train loss: 1.097879332786607e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5742:\n",
      "train loss: 9.742909535390606e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5743:\n",
      "train loss: 1.3918145155508395e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5744:\n",
      "train loss: 1.2677694707146538e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5745:\n",
      "train loss: 1.0990737867681e-12\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5746:\n",
      "train loss: 9.746548032497826e-13\n",
      "Epoch 05748: reducing learning rate of group 0 to 1.4171e-13.\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5747:\n",
      "train loss: 1.39198034103917e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5748:\n",
      "train loss: 1.2674778210507232e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5749:\n",
      "train loss: 9.80299927975084e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5750:\n",
      "train loss: 8.619425928713624e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5751:\n",
      "train loss: 1.3859101013614526e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5752:\n",
      "train loss: 1.2669071487972926e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5753:\n",
      "train loss: 9.811969212978956e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5754:\n",
      "train loss: 8.621768311926148e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5755:\n",
      "train loss: 1.3850338816329863e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5756:\n",
      "train loss: 1.26709458831193e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5757:\n",
      "train loss: 9.806234261657752e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5758:\n",
      "train loss: 8.617836966010845e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5759:\n",
      "train loss: 1.3855793205940288e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5760:\n",
      "train loss: 1.2666315418623188e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5761:\n",
      "train loss: 9.816136021313247e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5762:\n",
      "train loss: 8.623329104494859e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5763:\n",
      "train loss: 1.3849959860636646e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5764:\n",
      "train loss: 1.2659502880296026e-12\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5765:\n",
      "train loss: 9.8186388458787e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5766:\n",
      "train loss: 8.636463779650938e-13\n",
      "Epoch 05768: reducing learning rate of group 0 to 1.3462e-13.\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5767:\n",
      "train loss: 1.3838941719590432e-12\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5768:\n",
      "train loss: 1.265449807035806e-12\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5769:\n",
      "train loss: 8.697460641858225e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5770:\n",
      "train loss: 7.581421391043136e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5771:\n",
      "train loss: 1.3774343580212043e-12\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5772:\n",
      "train loss: 1.2642526046068674e-12\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5773:\n",
      "train loss: 8.70515736673379e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5774:\n",
      "train loss: 7.590204485712053e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5775:\n",
      "train loss: 1.3761224698263191e-12\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5776:\n",
      "train loss: 1.2634995897443058e-12\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5777:\n",
      "train loss: 8.712967106843266e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5778:\n",
      "train loss: 7.595081466306647e-13\n",
      "Epoch 05780: reducing learning rate of group 0 to 1.2789e-13.\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5779:\n",
      "train loss: 1.3747121160795174e-12\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5780:\n",
      "train loss: 1.2621140344254047e-12\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5781:\n",
      "train loss: 7.65918635881823e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5782:\n",
      "train loss: 6.607336460992224e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5783:\n",
      "train loss: 1.3662545199099382e-12\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5784:\n",
      "train loss: 1.259928035484995e-12\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5785:\n",
      "train loss: 7.687083354313569e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5786:\n",
      "train loss: 6.630555234444348e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5787:\n",
      "train loss: 1.3638757140085149e-12\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5788:\n",
      "train loss: 1.2566718624339475e-12\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5789:\n",
      "train loss: 7.728276246158482e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5790:\n",
      "train loss: 6.665948972849458e-13\n",
      "Epoch 05792: reducing learning rate of group 0 to 1.2150e-13.\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5791:\n",
      "train loss: 1.3585496415681903e-12\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5792:\n",
      "train loss: 1.2500903887595972e-12\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5793:\n",
      "train loss: 6.772722950030348e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5794:\n",
      "train loss: 5.785926867543957e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5795:\n",
      "train loss: 1.345148836272759e-12\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5796:\n",
      "train loss: 1.240197087831287e-12\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5797:\n",
      "train loss: 6.878084397541406e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5798:\n",
      "train loss: 5.910934477789026e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5799:\n",
      "train loss: 1.3293542727164789e-12\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5800:\n",
      "train loss: 1.2231029831820237e-12\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5801:\n",
      "train loss: 7.082121699455861e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5802:\n",
      "train loss: 6.137391799459158e-13\n",
      "Epoch 05804: reducing learning rate of group 0 to 1.1542e-13.\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5803:\n",
      "train loss: 1.302941785694481e-12\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5804:\n",
      "train loss: 1.1921831741292598e-12\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5805:\n",
      "train loss: 6.460630336215029e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5806:\n",
      "train loss: 5.607198473408177e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5807:\n",
      "train loss: 1.250332034200292e-12\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5808:\n",
      "train loss: 1.1377260070815473e-12\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5809:\n",
      "train loss: 7.069345530493679e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5810:\n",
      "train loss: 6.296659441378405e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5811:\n",
      "train loss: 1.173593111903933e-12\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5812:\n",
      "train loss: 1.052659921845191e-12\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5813:\n",
      "train loss: 7.97513410354193e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5814:\n",
      "train loss: 7.244090739463353e-13\n",
      "Epoch 05816: reducing learning rate of group 0 to 1.0965e-13.\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5815:\n",
      "train loss: 1.0764353586884133e-12\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5816:\n",
      "train loss: 9.533988919832528e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5817:\n",
      "train loss: 8.069418102636191e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5818:\n",
      "train loss: 7.388089400111039e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5819:\n",
      "train loss: 9.717492070075849e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5820:\n",
      "train loss: 8.553465326494841e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5821:\n",
      "train loss: 9.035981443079971e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5822:\n",
      "train loss: 8.309291543064393e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5823:\n",
      "train loss: 8.880999905026612e-13\n",
      "Epoch 05825: reducing learning rate of group 0 to 1.0417e-13.\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5824:\n",
      "train loss: 7.785112061425854e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5825:\n",
      "train loss: 9.70916628228874e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5826:\n",
      "train loss: 8.913118576289204e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5827:\n",
      "train loss: 7.535110851737726e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5828:\n",
      "train loss: 6.655910396236648e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5829:\n",
      "train loss: 9.794002341457981e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5830:\n",
      "train loss: 8.843675494395781e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5831:\n",
      "train loss: 7.756035838926447e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5832:\n",
      "train loss: 6.974490135726857e-13\n",
      "Epoch 05834: reducing learning rate of group 0 to 9.8960e-14.\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5833:\n",
      "train loss: 9.400409479784956e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5834:\n",
      "train loss: 8.385582399347748e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5835:\n",
      "train loss: 7.426793405848342e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5836:\n",
      "train loss: 6.735285390474718e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5837:\n",
      "train loss: 8.789484964733476e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5838:\n",
      "train loss: 7.801690397154289e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5839:\n",
      "train loss: 8.020520119158386e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5840:\n",
      "train loss: 7.314049925874039e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5841:\n",
      "train loss: 8.234931091205117e-13\n",
      "Epoch 05843: reducing learning rate of group 0 to 9.4012e-14.\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5842:\n",
      "train loss: 7.283011127338996e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5843:\n",
      "train loss: 8.501195148008413e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5844:\n",
      "train loss: 7.782968343496908e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5845:\n",
      "train loss: 7.066940605558972e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5846:\n",
      "train loss: 6.237982226666379e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5847:\n",
      "train loss: 8.673481996183579e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5848:\n",
      "train loss: 7.892661775554349e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5849:\n",
      "train loss: 7.028942909573107e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5850:\n",
      "train loss: 6.250168000846843e-13\n",
      "Epoch 05852: reducing learning rate of group 0 to 8.9312e-14.\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5851:\n",
      "train loss: 8.613002832509467e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5852:\n",
      "train loss: 7.782336493559601e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5853:\n",
      "train loss: 6.412882211310503e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5854:\n",
      "train loss: 5.714272568528835e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5855:\n",
      "train loss: 8.382840244330627e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5856:\n",
      "train loss: 7.568531716711429e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5857:\n",
      "train loss: 6.643829719553033e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5858:\n",
      "train loss: 5.961965356558389e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5859:\n",
      "train loss: 8.127837561645681e-13\n",
      "Epoch 05861: reducing learning rate of group 0 to 8.4846e-14.\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5860:\n",
      "train loss: 7.30897522847878e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5861:\n",
      "train loss: 6.906860410369928e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5862:\n",
      "train loss: 6.251141723335763e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5863:\n",
      "train loss: 7.152960583379816e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5864:\n",
      "train loss: 6.393182029919669e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5865:\n",
      "train loss: 7.089866712979837e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5866:\n",
      "train loss: 6.421468801641878e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5867:\n",
      "train loss: 6.989683612663253e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5868:\n",
      "train loss: 6.252147482650605e-13\n",
      "Epoch 05870: reducing learning rate of group 0 to 8.0604e-14.\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5869:\n",
      "train loss: 7.213547659277864e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5870:\n",
      "train loss: 6.54341860357186e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5871:\n",
      "train loss: 6.22509153985376e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5872:\n",
      "train loss: 5.536858586936097e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5873:\n",
      "train loss: 7.255342357211771e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5874:\n",
      "train loss: 6.594257737066004e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5875:\n",
      "train loss: 6.178000648860745e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5876:\n",
      "train loss: 5.514485165465916e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5877:\n",
      "train loss: 7.27663111073281e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5878:\n",
      "train loss: 6.600258687123273e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5879:\n",
      "train loss: 6.170514992803595e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5880:\n",
      "train loss: 5.501772112085954e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5881:\n",
      "train loss: 7.276127254467193e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5882:\n",
      "train loss: 6.600112111145615e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5883:\n",
      "train loss: 6.180491735634796e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5884:\n",
      "train loss: 5.509570357925431e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5885:\n",
      "train loss: 7.264253363293359e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5886:\n",
      "train loss: 6.592502385037556e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5887:\n",
      "train loss: 6.192673432228115e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5888:\n",
      "train loss: 5.522654040763062e-13\n",
      "Epoch 05890: reducing learning rate of group 0 to 7.6574e-14.\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5889:\n",
      "train loss: 7.245116496590175e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5890:\n",
      "train loss: 6.575221491440698e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5891:\n",
      "train loss: 5.572046923162229e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5892:\n",
      "train loss: 4.930729123028906e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5893:\n",
      "train loss: 7.196632449303391e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5894:\n",
      "train loss: 6.557180863582014e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5895:\n",
      "train loss: 5.585114029898344e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5896:\n",
      "train loss: 4.948807423148916e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5897:\n",
      "train loss: 7.178384930825931e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5898:\n",
      "train loss: 6.537187093633808e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5899:\n",
      "train loss: 5.598183113729274e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5900:\n",
      "train loss: 4.95842415850765e-13\n",
      "Epoch 05902: reducing learning rate of group 0 to 7.2745e-14.\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5901:\n",
      "train loss: 7.162375625518882e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5902:\n",
      "train loss: 6.525426726853712e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5903:\n",
      "train loss: 5.013304657110438e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5904:\n",
      "train loss: 4.40554229331379e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5905:\n",
      "train loss: 7.117301825122122e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5906:\n",
      "train loss: 6.509314741781676e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5907:\n",
      "train loss: 5.019620413660626e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5908:\n",
      "train loss: 4.4116121030450337e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5909:\n",
      "train loss: 7.105083391906328e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5910:\n",
      "train loss: 6.494780635188319e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5911:\n",
      "train loss: 5.031924386373867e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5912:\n",
      "train loss: 4.428714312003924e-13\n",
      "Epoch 05914: reducing learning rate of group 0 to 6.9108e-14.\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5913:\n",
      "train loss: 7.093135011060825e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5914:\n",
      "train loss: 6.49223976546126e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5915:\n",
      "train loss: 4.469924916986949e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5916:\n",
      "train loss: 3.8993206216082724e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5917:\n",
      "train loss: 7.059350443885799e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5918:\n",
      "train loss: 6.475535389384202e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5919:\n",
      "train loss: 4.4721421770430995e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5920:\n",
      "train loss: 3.9044458344494175e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5921:\n",
      "train loss: 7.044363087858272e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5922:\n",
      "train loss: 6.458340761872679e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5923:\n",
      "train loss: 4.4859264461791433e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5924:\n",
      "train loss: 3.9147492525862234e-13\n",
      "Epoch 05926: reducing learning rate of group 0 to 6.5652e-14.\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5925:\n",
      "train loss: 7.032619577186311e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5926:\n",
      "train loss: 6.440741672794561e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5927:\n",
      "train loss: 3.9533093264463073e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5928:\n",
      "train loss: 3.414995280175891e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5929:\n",
      "train loss: 6.986615412877954e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5930:\n",
      "train loss: 6.435832643059236e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5931:\n",
      "train loss: 3.964283990429723e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5932:\n",
      "train loss: 3.4300357371305105e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5933:\n",
      "train loss: 6.95699780617847e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5934:\n",
      "train loss: 6.413445841164977e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5935:\n",
      "train loss: 3.998469371241695e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5936:\n",
      "train loss: 3.4524966731587287e-13\n",
      "Epoch 05938: reducing learning rate of group 0 to 6.2370e-14.\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5937:\n",
      "train loss: 6.934933407060387e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5938:\n",
      "train loss: 6.379937227789365e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5939:\n",
      "train loss: 3.510585810262395e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5940:\n",
      "train loss: 2.9903735224819894e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5941:\n",
      "train loss: 6.872043356693569e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5942:\n",
      "train loss: 6.339028189525504e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5943:\n",
      "train loss: 3.5530787238728694e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5944:\n",
      "train loss: 3.043504889302606e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5945:\n",
      "train loss: 6.813099358980273e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5946:\n",
      "train loss: 6.277556370821828e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5947:\n",
      "train loss: 3.6173192321328913e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5948:\n",
      "train loss: 3.1259709112327705e-13\n",
      "Epoch 05950: reducing learning rate of group 0 to 5.9251e-14.\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5949:\n",
      "train loss: 6.722825192457162e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5950:\n",
      "train loss: 6.17318682382617e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5951:\n",
      "train loss: 3.234608435399555e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5952:\n",
      "train loss: 2.783019732341631e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5953:\n",
      "train loss: 6.551936759900324e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5954:\n",
      "train loss: 6.005740749107494e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5955:\n",
      "train loss: 3.4241992903613614e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5956:\n",
      "train loss: 2.9892795995454034e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5957:\n",
      "train loss: 6.303993606559449e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5958:\n",
      "train loss: 5.73297450917773e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5959:\n",
      "train loss: 3.7134399078073744e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5960:\n",
      "train loss: 3.2990265763802165e-13\n",
      "Epoch 05962: reducing learning rate of group 0 to 5.6289e-14.\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5961:\n",
      "train loss: 5.974540091707222e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5962:\n",
      "train loss: 5.381262993351323e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5963:\n",
      "train loss: 3.6098033858437894e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5964:\n",
      "train loss: 3.2471286494156567e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5965:\n",
      "train loss: 5.542287198288496e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5966:\n",
      "train loss: 4.954726875382125e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5967:\n",
      "train loss: 4.051723032020863e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5968:\n",
      "train loss: 3.698664933179125e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5969:\n",
      "train loss: 5.080461415995421e-13\n",
      "Epoch 05971: reducing learning rate of group 0 to 5.3474e-14.\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5970:\n",
      "train loss: 4.489799129603614e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5971:\n",
      "train loss: 4.5192180865717277e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5972:\n",
      "train loss: 4.1505056426936786e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5973:\n",
      "train loss: 4.223432467937293e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5974:\n",
      "train loss: 3.7061139767813777e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5975:\n",
      "train loss: 4.802223118184981e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5976:\n",
      "train loss: 4.386881836939553e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5977:\n",
      "train loss: 4.0389408726785123e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5978:\n",
      "train loss: 3.58677868884061e-13\n",
      "Epoch 05980: reducing learning rate of group 0 to 5.0800e-14.\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5979:\n",
      "train loss: 4.858690148827746e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5980:\n",
      "train loss: 4.393747280361836e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5981:\n",
      "train loss: 3.6660725426215334e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5982:\n",
      "train loss: 3.2740540350555855e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5983:\n",
      "train loss: 4.688319712548288e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5984:\n",
      "train loss: 4.181791932620524e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5985:\n",
      "train loss: 3.922127051752852e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5986:\n",
      "train loss: 3.565821381294274e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5987:\n",
      "train loss: 4.38908548224128e-13\n",
      "Epoch 05989: reducing learning rate of group 0 to 4.8260e-14.\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5988:\n",
      "train loss: 3.8848915862480536e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5989:\n",
      "train loss: 4.227622811155459e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5990:\n",
      "train loss: 3.875362106865918e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5991:\n",
      "train loss: 3.7121804726633856e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5992:\n",
      "train loss: 3.275545053041442e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5993:\n",
      "train loss: 4.3694693217703134e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5994:\n",
      "train loss: 3.953409335543189e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5995:\n",
      "train loss: 3.6963120744470907e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5996:\n",
      "train loss: 3.3145133890565707e-13\n",
      "Epoch 05998: reducing learning rate of group 0 to 4.5847e-14.\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5997:\n",
      "train loss: 4.2867822002473624e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5998:\n",
      "train loss: 3.840127306086904e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5999:\n",
      "train loss: 3.45606980065864e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 6000:\n",
      "train loss: 3.118505302878325e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 6001:\n",
      "train loss: 4.0715779354164645e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 6002:\n",
      "train loss: 3.6259231379941713e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 6003:\n",
      "train loss: 3.7062547529069206e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 6004:\n",
      "train loss: 3.376443877981167e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 6005:\n",
      "train loss: 3.8148983500012926e-13\n",
      "Epoch 06007: reducing learning rate of group 0 to 4.3555e-14.\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6006:\n",
      "train loss: 3.3728818551394256e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6007:\n",
      "train loss: 3.919241114755222e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6008:\n",
      "train loss: 3.5838888144924835e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6009:\n",
      "train loss: 3.2931759632207255e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6010:\n",
      "train loss: 2.9181392782036816e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6011:\n",
      "train loss: 3.965470765760162e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6012:\n",
      "train loss: 3.585679888499869e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6013:\n",
      "train loss: 3.3340283791711127e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 6014:\n",
      "train loss: 2.985218612191641e-13\n",
      "Epoch 06016: reducing learning rate of group 0 to 4.1377e-14.\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6015:\n",
      "train loss: 3.859449917223771e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6016:\n",
      "train loss: 3.450584284611824e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6017:\n",
      "train loss: 3.1363832140801707e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6018:\n",
      "train loss: 2.8327487413657036e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6019:\n",
      "train loss: 3.6599568153303793e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6020:\n",
      "train loss: 3.2618334459280217e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6021:\n",
      "train loss: 3.346533585049526e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6022:\n",
      "train loss: 3.0367155175599654e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 6023:\n",
      "train loss: 3.4704270994319836e-13\n",
      "Epoch 06025: reducing learning rate of group 0 to 3.9308e-14.\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6024:\n",
      "train loss: 3.080151265848932e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6025:\n",
      "train loss: 3.510929814388914e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6026:\n",
      "train loss: 3.19880449863411e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6027:\n",
      "train loss: 3.0062487119519443e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6028:\n",
      "train loss: 2.6594182059279605e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6029:\n",
      "train loss: 3.5740917273517364e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6030:\n",
      "train loss: 3.237495641674944e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6031:\n",
      "train loss: 3.0006831493409334e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6032:\n",
      "train loss: 2.674724561678815e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6033:\n",
      "train loss: 3.524358346585535e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6034:\n",
      "train loss: 3.1705914720042194e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6035:\n",
      "train loss: 3.069249277952043e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 6036:\n",
      "train loss: 2.764302886199803e-13\n",
      "Epoch 06038: reducing learning rate of group 0 to 3.7343e-14.\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6037:\n",
      "train loss: 3.432503651581997e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6038:\n",
      "train loss: 3.070085678869315e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6039:\n",
      "train loss: 2.8698162582577014e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6040:\n",
      "train loss: 2.573841623953917e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6041:\n",
      "train loss: 3.3062292688662604e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6042:\n",
      "train loss: 2.969664133727792e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6043:\n",
      "train loss: 2.975447901147323e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6044:\n",
      "train loss: 2.685878752408032e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6045:\n",
      "train loss: 3.1951452935444667e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6046:\n",
      "train loss: 2.8589128196098813e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6047:\n",
      "train loss: 3.0762040186837784e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 6048:\n",
      "train loss: 2.7901625061831767e-13\n",
      "Epoch 06050: reducing learning rate of group 0 to 3.5476e-14.\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6049:\n",
      "train loss: 3.1056083451818143e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6050:\n",
      "train loss: 2.7761761694319876e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6051:\n",
      "train loss: 2.8629751479222565e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6052:\n",
      "train loss: 2.585624545601141e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6053:\n",
      "train loss: 3.02314870041777e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6054:\n",
      "train loss: 2.7063230899338327e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6055:\n",
      "train loss: 2.932552607574803e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6056:\n",
      "train loss: 2.6422232222470447e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 6057:\n",
      "train loss: 2.970128495690645e-13\n",
      "Epoch 06059: reducing learning rate of group 0 to 3.3702e-14.\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6058:\n",
      "train loss: 2.659485043791853e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6059:\n",
      "train loss: 2.9804374849502233e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6060:\n",
      "train loss: 2.7001093757029083e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6061:\n",
      "train loss: 2.629793274855627e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6062:\n",
      "train loss: 2.347318112539488e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6063:\n",
      "train loss: 2.992472070546736e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6064:\n",
      "train loss: 2.708225729061739e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6065:\n",
      "train loss: 2.641491936841971e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6066:\n",
      "train loss: 2.35688737943097e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6067:\n",
      "train loss: 2.9794462096347537e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6068:\n",
      "train loss: 2.690404327218658e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6069:\n",
      "train loss: 2.6512278997063e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 6070:\n",
      "train loss: 2.3768730472232027e-13\n",
      "Epoch 06072: reducing learning rate of group 0 to 3.2017e-14.\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6071:\n",
      "train loss: 2.960191865096802e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6072:\n",
      "train loss: 2.6632185173357056e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6073:\n",
      "train loss: 2.4163829617494867e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6074:\n",
      "train loss: 2.156350138378528e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6075:\n",
      "train loss: 2.9059437433798634e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6076:\n",
      "train loss: 2.633238431324696e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6077:\n",
      "train loss: 2.4552453358021094e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6078:\n",
      "train loss: 2.195096835750421e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6079:\n",
      "train loss: 2.8628042435624584e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6080:\n",
      "train loss: 2.584207330140336e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6081:\n",
      "train loss: 2.4985751898404045e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 6082:\n",
      "train loss: 2.2383938915200285e-13\n",
      "Epoch 06084: reducing learning rate of group 0 to 3.0416e-14.\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6083:\n",
      "train loss: 2.818631356126596e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6084:\n",
      "train loss: 2.546048774394137e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6085:\n",
      "train loss: 2.280541316615753e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6086:\n",
      "train loss: 2.045086381596448e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6087:\n",
      "train loss: 2.768769790401188e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6088:\n",
      "train loss: 2.5060383176383805e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6089:\n",
      "train loss: 2.330979559666915e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6090:\n",
      "train loss: 2.0803301368366337e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6091:\n",
      "train loss: 2.720861644450636e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6092:\n",
      "train loss: 2.462768484870075e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6093:\n",
      "train loss: 2.3724830612845923e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 6094:\n",
      "train loss: 2.1286253006356247e-13\n",
      "Epoch 06096: reducing learning rate of group 0 to 2.8895e-14.\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6095:\n",
      "train loss: 2.680496046332375e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6096:\n",
      "train loss: 2.4149697767253327e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6097:\n",
      "train loss: 2.1659838081036722e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6098:\n",
      "train loss: 1.9377833810920528e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6099:\n",
      "train loss: 2.632263579669494e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6100:\n",
      "train loss: 2.3783058376951115e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6101:\n",
      "train loss: 2.2027813337975978e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6102:\n",
      "train loss: 1.9691434336592932e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6103:\n",
      "train loss: 2.592463070760979e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6104:\n",
      "train loss: 2.3420182246273984e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6105:\n",
      "train loss: 2.2465648713895683e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 6106:\n",
      "train loss: 2.005643105477413e-13\n",
      "Epoch 06108: reducing learning rate of group 0 to 2.7451e-14.\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6107:\n",
      "train loss: 2.566072063376609e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6108:\n",
      "train loss: 2.3086775512479917e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6109:\n",
      "train loss: 2.050416802735829e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6110:\n",
      "train loss: 1.8237864283710674e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6111:\n",
      "train loss: 2.5150536978715545e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6112:\n",
      "train loss: 2.276632482087617e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6113:\n",
      "train loss: 2.0723478103927092e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6114:\n",
      "train loss: 1.856839372273199e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6115:\n",
      "train loss: 2.4900530821029803e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6116:\n",
      "train loss: 2.245342522043082e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6117:\n",
      "train loss: 2.105335366477953e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 6118:\n",
      "train loss: 1.8835022707710954e-13\n",
      "Epoch 06120: reducing learning rate of group 0 to 2.6078e-14.\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6119:\n",
      "train loss: 2.457437096471356e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6120:\n",
      "train loss: 2.222476690949109e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6121:\n",
      "train loss: 1.9172436259783386e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6122:\n",
      "train loss: 1.6978945976071443e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6123:\n",
      "train loss: 2.424172998052683e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6124:\n",
      "train loss: 2.1917736263460966e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6125:\n",
      "train loss: 1.9392666814023302e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6126:\n",
      "train loss: 1.7331737708322982e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6127:\n",
      "train loss: 2.39925265575785e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6128:\n",
      "train loss: 2.1629144820600457e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6129:\n",
      "train loss: 1.970310777074479e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 6130:\n",
      "train loss: 1.7570337312356355e-13\n",
      "Epoch 06132: reducing learning rate of group 0 to 2.4774e-14.\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6131:\n",
      "train loss: 2.365761756040744e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6132:\n",
      "train loss: 2.1421832285934345e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6133:\n",
      "train loss: 1.7878663165558748e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6134:\n",
      "train loss: 1.587986336119331e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6135:\n",
      "train loss: 2.327078114733845e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6136:\n",
      "train loss: 2.1162427911302224e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6137:\n",
      "train loss: 1.8186583940562333e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6138:\n",
      "train loss: 1.6198192306095418e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6139:\n",
      "train loss: 2.295346606308247e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6140:\n",
      "train loss: 2.0804634932854747e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6141:\n",
      "train loss: 1.8469500367425343e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 6142:\n",
      "train loss: 1.651447806892221e-13\n",
      "Epoch 06144: reducing learning rate of group 0 to 2.3535e-14.\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6143:\n",
      "train loss: 2.2703113503547705e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6144:\n",
      "train loss: 2.050477913874538e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6145:\n",
      "train loss: 1.6792525529937468e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6146:\n",
      "train loss: 1.4778819372176435e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6147:\n",
      "train loss: 2.229852575159643e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6148:\n",
      "train loss: 2.0301429246427857e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6149:\n",
      "train loss: 1.7042205070142883e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6150:\n",
      "train loss: 1.5107350859974876e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6151:\n",
      "train loss: 2.2072694643157963e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6152:\n",
      "train loss: 2.0019622559760082e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6153:\n",
      "train loss: 1.7319206669077173e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 6154:\n",
      "train loss: 1.545019797413132e-13\n",
      "Epoch 06156: reducing learning rate of group 0 to 2.2359e-14.\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6155:\n",
      "train loss: 2.1804045260429812e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6156:\n",
      "train loss: 1.9713527933150008e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6157:\n",
      "train loss: 1.576068819641494e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6158:\n",
      "train loss: 1.3930691451795746e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6159:\n",
      "train loss: 2.1321116173653175e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6160:\n",
      "train loss: 1.9397091921110748e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6161:\n",
      "train loss: 1.611303152190214e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6162:\n",
      "train loss: 1.42653154881776e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6163:\n",
      "train loss: 2.1113596646344576e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6164:\n",
      "train loss: 1.9119297354167906e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6165:\n",
      "train loss: 1.638222848492055e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 6166:\n",
      "train loss: 1.4508823622249915e-13\n",
      "Epoch 06168: reducing learning rate of group 0 to 2.1241e-14.\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6167:\n",
      "train loss: 2.078882964884071e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6168:\n",
      "train loss: 1.8873558156694355e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6169:\n",
      "train loss: 1.4766444573579563e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6170:\n",
      "train loss: 1.316267481737953e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6171:\n",
      "train loss: 2.0381313940091985e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6172:\n",
      "train loss: 1.8598619411833405e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6173:\n",
      "train loss: 1.5099014847637452e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6174:\n",
      "train loss: 1.3414289647931386e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6175:\n",
      "train loss: 2.0110522667183888e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6176:\n",
      "train loss: 1.8217483555117238e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6177:\n",
      "train loss: 1.538961233789708e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 6178:\n",
      "train loss: 1.374645641102405e-13\n",
      "Epoch 06180: reducing learning rate of group 0 to 2.0179e-14.\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6179:\n",
      "train loss: 1.9809193449576498e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6180:\n",
      "train loss: 1.7928698957495224e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6181:\n",
      "train loss: 1.4014489838549776e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6182:\n",
      "train loss: 1.239723113638197e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6183:\n",
      "train loss: 1.9393633369543726e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6184:\n",
      "train loss: 1.7672105285799401e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6185:\n",
      "train loss: 1.431472148139368e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6186:\n",
      "train loss: 1.2708783499200798e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6187:\n",
      "train loss: 1.9053622407805645e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6188:\n",
      "train loss: 1.7341112707947222e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6189:\n",
      "train loss: 1.470564117638822e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 6190:\n",
      "train loss: 1.3097914197470551e-13\n",
      "Epoch 06192: reducing learning rate of group 0 to 1.9170e-14.\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6191:\n",
      "train loss: 1.874116407208666e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6192:\n",
      "train loss: 1.6999019851320849e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6193:\n",
      "train loss: 1.3397666225203405e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6194:\n",
      "train loss: 1.1882454525003023e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6195:\n",
      "train loss: 1.835312755469645e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6196:\n",
      "train loss: 1.6616688165434807e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6197:\n",
      "train loss: 1.3818797181119868e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6198:\n",
      "train loss: 1.228313486824063e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6199:\n",
      "train loss: 1.7912555991035294e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6200:\n",
      "train loss: 1.6353632159379875e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6201:\n",
      "train loss: 1.4062417414950911e-13\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 6202:\n",
      "train loss: 1.2630189684723312e-13\n",
      "Epoch 06204: reducing learning rate of group 0 to 1.8211e-14.\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6203:\n",
      "train loss: 1.7627867965250116e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6204:\n",
      "train loss: 1.6003312166558496e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6205:\n",
      "train loss: 1.2923548525487903e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6206:\n",
      "train loss: 1.15401193493289e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6207:\n",
      "train loss: 1.7278384111204585e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6208:\n",
      "train loss: 1.5704664810235453e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6209:\n",
      "train loss: 1.325977082900811e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6210:\n",
      "train loss: 1.1805227013300098e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6211:\n",
      "train loss: 1.6946552261595842e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6212:\n",
      "train loss: 1.532698203683456e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6213:\n",
      "train loss: 1.3573971102884572e-13\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 6214:\n",
      "train loss: 1.2131620564348442e-13\n",
      "Epoch 06216: reducing learning rate of group 0 to 1.7301e-14.\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6215:\n",
      "train loss: 1.6702563599852032e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6216:\n",
      "train loss: 1.5021663200193077e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6217:\n",
      "train loss: 1.2451831495110213e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6218:\n",
      "train loss: 1.116282571171694e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6219:\n",
      "train loss: 1.6221288254370237e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6220:\n",
      "train loss: 1.4691075719850518e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6221:\n",
      "train loss: 1.2797014182326905e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6222:\n",
      "train loss: 1.1388365141503875e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6223:\n",
      "train loss: 1.5867534265965385e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6224:\n",
      "train loss: 1.4255654433549716e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6225:\n",
      "train loss: 1.3125625015067076e-13\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 6226:\n",
      "train loss: 1.1760637843841986e-13\n",
      "Epoch 06228: reducing learning rate of group 0 to 1.6436e-14.\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6227:\n",
      "train loss: 1.5574232723616374e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6228:\n",
      "train loss: 1.4018695902804044e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6229:\n",
      "train loss: 1.2039825352112359e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6230:\n",
      "train loss: 1.0753975677809641e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6231:\n",
      "train loss: 1.5189797215667815e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6232:\n",
      "train loss: 1.3702837613725376e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6233:\n",
      "train loss: 1.231395954441226e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6234:\n",
      "train loss: 1.1055328036147529e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6235:\n",
      "train loss: 1.4879850359542692e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6236:\n",
      "train loss: 1.3450637171891124e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6237:\n",
      "train loss: 1.271853273696526e-13\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 6238:\n",
      "train loss: 1.1298516724005039e-13\n",
      "Epoch 06240: reducing learning rate of group 0 to 1.5614e-14.\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6239:\n",
      "train loss: 1.4666897353033493e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6240:\n",
      "train loss: 1.3160035581427483e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6241:\n",
      "train loss: 1.1565355198464347e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6242:\n",
      "train loss: 1.0326033458645001e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6243:\n",
      "train loss: 1.432148080247063e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6244:\n",
      "train loss: 1.2916008248658334e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6245:\n",
      "train loss: 1.1892141756675456e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6246:\n",
      "train loss: 1.0659756655178275e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6247:\n",
      "train loss: 1.3983137854251816e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6248:\n",
      "train loss: 1.2630511088270398e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6249:\n",
      "train loss: 1.2155170538661483e-13\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 6250:\n",
      "train loss: 1.0881407688092397e-13\n",
      "Epoch 06252: reducing learning rate of group 0 to 1.4833e-14.\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6251:\n",
      "train loss: 1.3677429046352844e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6252:\n",
      "train loss: 1.2278308430163437e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6253:\n",
      "train loss: 1.1187486774959845e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6254:\n",
      "train loss: 1.003729631808191e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6255:\n",
      "train loss: 1.340179491828994e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6256:\n",
      "train loss: 1.2012423630203085e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6257:\n",
      "train loss: 1.1459667399989107e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6258:\n",
      "train loss: 1.0314051759147287e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6259:\n",
      "train loss: 1.3152116261619383e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6260:\n",
      "train loss: 1.1766926855754487e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6261:\n",
      "train loss: 1.1703876752109274e-13\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 6262:\n",
      "train loss: 1.0496500045158823e-13\n",
      "Epoch 06264: reducing learning rate of group 0 to 1.4092e-14.\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6263:\n",
      "train loss: 1.2943093055212772e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6264:\n",
      "train loss: 1.1604007800833984e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6265:\n",
      "train loss: 1.075927174998888e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6266:\n",
      "train loss: 9.662931147227982e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6267:\n",
      "train loss: 1.2690699011717834e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6268:\n",
      "train loss: 1.1446158577400021e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6269:\n",
      "train loss: 1.0889480028493435e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6270:\n",
      "train loss: 9.871290310611005e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6271:\n",
      "train loss: 1.238581280532387e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6272:\n",
      "train loss: 1.1083538139567144e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6273:\n",
      "train loss: 1.1201064405691377e-13\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 6274:\n",
      "train loss: 1.0143241539205864e-13\n",
      "Epoch 06276: reducing learning rate of group 0 to 1.3387e-14.\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6275:\n",
      "train loss: 1.216487957064889e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6276:\n",
      "train loss: 1.0874650944330687e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6277:\n",
      "train loss: 1.0256966857796055e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6278:\n",
      "train loss: 9.253137014073148e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6279:\n",
      "train loss: 1.186476209247972e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6280:\n",
      "train loss: 1.0684893246049424e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6281:\n",
      "train loss: 1.050534873738382e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6282:\n",
      "train loss: 9.499250542067246e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6283:\n",
      "train loss: 1.1641254190013116e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6284:\n",
      "train loss: 1.042017666258956e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6285:\n",
      "train loss: 1.0793823278730009e-13\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 6286:\n",
      "train loss: 9.775338790705956e-14\n",
      "Epoch 06288: reducing learning rate of group 0 to 1.2718e-14.\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6287:\n",
      "train loss: 1.1400193185292732e-13\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6288:\n",
      "train loss: 1.0153477767833539e-13\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6289:\n",
      "train loss: 9.964948876124841e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6290:\n",
      "train loss: 8.995268455701281e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6291:\n",
      "train loss: 1.1089822457927098e-13\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6292:\n",
      "train loss: 9.904125137608363e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6293:\n",
      "train loss: 1.0282553011999585e-13\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6294:\n",
      "train loss: 9.185390060903864e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6295:\n",
      "train loss: 1.0800476322774503e-13\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6296:\n",
      "train loss: 9.638999266059725e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6297:\n",
      "train loss: 1.0554211310643548e-13\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 6298:\n",
      "train loss: 9.494283087036538e-14\n",
      "Epoch 06300: reducing learning rate of group 0 to 1.2082e-14.\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6299:\n",
      "train loss: 1.0548462629383289e-13\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6300:\n",
      "train loss: 9.374515938974425e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6301:\n",
      "train loss: 9.737654488945148e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6302:\n",
      "train loss: 8.780990085737142e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6303:\n",
      "train loss: 1.0330539879620553e-13\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6304:\n",
      "train loss: 9.282173449772094e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6305:\n",
      "train loss: 9.874080673221895e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6306:\n",
      "train loss: 8.906500004774125e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6307:\n",
      "train loss: 1.0174629831880885e-13\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6308:\n",
      "train loss: 9.034743771066837e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6309:\n",
      "train loss: 1.0029532331042822e-13\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 6310:\n",
      "train loss: 9.070355696854808e-14\n",
      "Epoch 06312: reducing learning rate of group 0 to 1.1478e-14.\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6311:\n",
      "train loss: 1.0034900771080865e-13\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6312:\n",
      "train loss: 9.037241418763721e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6313:\n",
      "train loss: 9.19612891467286e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6314:\n",
      "train loss: 8.204425773649342e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6315:\n",
      "train loss: 9.816346553580977e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6316:\n",
      "train loss: 8.775147369939778e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6317:\n",
      "train loss: 9.37022204766721e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6318:\n",
      "train loss: 8.449217760467487e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6319:\n",
      "train loss: 9.692287845860138e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6320:\n",
      "train loss: 8.643206333336131e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6321:\n",
      "train loss: 9.586876760405509e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 6322:\n",
      "train loss: 8.661495020493725e-14\n",
      "Epoch 06324: reducing learning rate of group 0 to 1.0904e-14.\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6323:\n",
      "train loss: 9.472154450819406e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6324:\n",
      "train loss: 8.508011838003587e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6325:\n",
      "train loss: 8.738608911930635e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6326:\n",
      "train loss: 7.811876237324503e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6327:\n",
      "train loss: 9.33872798198839e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6328:\n",
      "train loss: 8.329561684354278e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6329:\n",
      "train loss: 8.923076836715082e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6330:\n",
      "train loss: 8.064728422110933e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6331:\n",
      "train loss: 9.092318434188102e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6332:\n",
      "train loss: 8.152267717050987e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6333:\n",
      "train loss: 9.110304439383836e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 6334:\n",
      "train loss: 8.17263589024808e-14\n",
      "Epoch 06336: reducing learning rate of group 0 to 1.0359e-14.\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6335:\n",
      "train loss: 8.951132000042887e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6336:\n",
      "train loss: 8.014658584847846e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6337:\n",
      "train loss: 8.262261905825067e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6338:\n",
      "train loss: 7.488297577730595e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6339:\n",
      "train loss: 8.859921080256328e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6340:\n",
      "train loss: 7.976629495280144e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6341:\n",
      "train loss: 8.481282121626221e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6342:\n",
      "train loss: 7.610248718782739e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6343:\n",
      "train loss: 8.6060469036569e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6344:\n",
      "train loss: 7.713461873368073e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6345:\n",
      "train loss: 8.600156636901278e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 6346:\n",
      "train loss: 7.691990383742374e-14\n",
      "Epoch 06348: reducing learning rate of group 0 to 9.8406e-15.\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6347:\n",
      "train loss: 8.564616295828034e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6348:\n",
      "train loss: 7.75067758051605e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6349:\n",
      "train loss: 7.914906654167239e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6350:\n",
      "train loss: 7.1121188839744e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6351:\n",
      "train loss: 8.42100802306998e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6352:\n",
      "train loss: 7.484018543022768e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6353:\n",
      "train loss: 8.034743233736347e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6354:\n",
      "train loss: 7.320625233613535e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6355:\n",
      "train loss: 8.130332251615283e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6356:\n",
      "train loss: 7.325760840151552e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6357:\n",
      "train loss: 8.260890404788296e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 6358:\n",
      "train loss: 7.429269937484477e-14\n",
      "Epoch 06360: reducing learning rate of group 0 to 9.3486e-15.\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6359:\n",
      "train loss: 8.10208886233962e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6360:\n",
      "train loss: 7.18633748470442e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6361:\n",
      "train loss: 7.547004025062698e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6362:\n",
      "train loss: 6.745575009794357e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6363:\n",
      "train loss: 7.974235590577107e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6364:\n",
      "train loss: 7.067293914813812e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6365:\n",
      "train loss: 7.805865383953321e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6366:\n",
      "train loss: 6.936752867027884e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6367:\n",
      "train loss: 7.699653197742481e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6368:\n",
      "train loss: 6.872857549161064e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6369:\n",
      "train loss: 7.834243550893298e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 6370:\n",
      "train loss: 7.044572780111543e-14\n",
      "Epoch 06372: reducing learning rate of group 0 to 8.8812e-15.\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6371:\n",
      "train loss: 7.686537321332852e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6372:\n",
      "train loss: 6.847657610107097e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6373:\n",
      "train loss: 7.161622190415791e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6374:\n",
      "train loss: 6.456834749995642e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6375:\n",
      "train loss: 7.610871758279447e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6376:\n",
      "train loss: 6.78780554355804e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6377:\n",
      "train loss: 7.210701059408085e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6378:\n",
      "train loss: 6.537974333082447e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6379:\n",
      "train loss: 7.536842349838204e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6380:\n",
      "train loss: 6.688073977930047e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6381:\n",
      "train loss: 7.372168757724881e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6382:\n",
      "train loss: 6.674510163189403e-14\n",
      "Epoch 06384: reducing learning rate of group 0 to 8.4371e-15.\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6383:\n",
      "train loss: 7.349775873496935e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6384:\n",
      "train loss: 6.57153393784911e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6385:\n",
      "train loss: 6.739664179354511e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6386:\n",
      "train loss: 6.070005386953507e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6387:\n",
      "train loss: 7.16254058844049e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6388:\n",
      "train loss: 6.350230643087736e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6389:\n",
      "train loss: 7.036132838161838e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6390:\n",
      "train loss: 6.314782799247928e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6391:\n",
      "train loss: 6.987759328306442e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6392:\n",
      "train loss: 6.226598489927739e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6393:\n",
      "train loss: 7.151267070652742e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6394:\n",
      "train loss: 6.35949164178142e-14\n",
      "Epoch 06396: reducing learning rate of group 0 to 8.0153e-15.\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6395:\n",
      "train loss: 6.932886791305936e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6396:\n",
      "train loss: 6.156862730813057e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6397:\n",
      "train loss: 6.463428313910823e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6398:\n",
      "train loss: 5.813574645851986e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6399:\n",
      "train loss: 6.783033883854762e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6400:\n",
      "train loss: 6.024341360628956e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6401:\n",
      "train loss: 6.580675085432378e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6402:\n",
      "train loss: 5.878985389968848e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6403:\n",
      "train loss: 6.707016701670001e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6404:\n",
      "train loss: 5.987635603700229e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6405:\n",
      "train loss: 6.68128097374838e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6406:\n",
      "train loss: 6.065136481513557e-14\n",
      "Epoch 06408: reducing learning rate of group 0 to 7.6145e-15.\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6407:\n",
      "train loss: 6.58447165327202e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6408:\n",
      "train loss: 5.887393585860042e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6409:\n",
      "train loss: 6.098992805418219e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6410:\n",
      "train loss: 5.5579261583639e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6411:\n",
      "train loss: 6.479286672816262e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6412:\n",
      "train loss: 5.783410610932123e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6413:\n",
      "train loss: 6.24965192491425e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6414:\n",
      "train loss: 5.6263548428485606e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6415:\n",
      "train loss: 6.3119121561476e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6416:\n",
      "train loss: 5.638430521479181e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6417:\n",
      "train loss: 6.371532171578547e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6418:\n",
      "train loss: 5.768493542279539e-14\n",
      "Epoch 06420: reducing learning rate of group 0 to 7.2338e-15.\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6419:\n",
      "train loss: 6.283775356614711e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6420:\n",
      "train loss: 5.65370606180319e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6421:\n",
      "train loss: 5.693955050489997e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6422:\n",
      "train loss: 5.15136581536925e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6423:\n",
      "train loss: 6.169228955184483e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6424:\n",
      "train loss: 5.44782550024788e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6425:\n",
      "train loss: 6.0321781370699e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6426:\n",
      "train loss: 5.4599461464036634e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6427:\n",
      "train loss: 5.901529464833765e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6428:\n",
      "train loss: 5.2819297239972557e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6429:\n",
      "train loss: 6.230220408613063e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6430:\n",
      "train loss: 5.5350093197534154e-14\n",
      "Epoch 06432: reducing learning rate of group 0 to 6.8721e-15.\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6431:\n",
      "train loss: 5.917828447292128e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6432:\n",
      "train loss: 5.23701155977328e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6433:\n",
      "train loss: 5.538913866444682e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6434:\n",
      "train loss: 4.973447119374189e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6435:\n",
      "train loss: 5.948558809420305e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6436:\n",
      "train loss: 5.2597657093264296e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6437:\n",
      "train loss: 5.5912733369227387e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6438:\n",
      "train loss: 5.0940047555093405e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6439:\n",
      "train loss: 5.736282155877998e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6440:\n",
      "train loss: 5.1263105871824836e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6441:\n",
      "train loss: 5.902262343804473e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6442:\n",
      "train loss: 5.3106300856501844e-14\n",
      "Epoch 06444: reducing learning rate of group 0 to 6.5285e-15.\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6443:\n",
      "train loss: 5.5693078084098664e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6444:\n",
      "train loss: 4.961883884952094e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6445:\n",
      "train loss: 5.304893562692183e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6446:\n",
      "train loss: 4.74576144543655e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6447:\n",
      "train loss: 5.550942247937899e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6448:\n",
      "train loss: 4.953324371671702e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6449:\n",
      "train loss: 5.435154302691724e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6450:\n",
      "train loss: 4.881012623812041e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6451:\n",
      "train loss: 5.399658548543324e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6452:\n",
      "train loss: 4.806280823929982e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6453:\n",
      "train loss: 5.5519790581704586e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6454:\n",
      "train loss: 4.9165181918470244e-14\n",
      "Epoch 06456: reducing learning rate of group 0 to 6.2021e-15.\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6455:\n",
      "train loss: 5.4289214155789935e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6456:\n",
      "train loss: 4.762995349956198e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6457:\n",
      "train loss: 5.018066304844769e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6458:\n",
      "train loss: 4.6038558220213083e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6459:\n",
      "train loss: 5.274906227827251e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6460:\n",
      "train loss: 4.624496821506223e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6461:\n",
      "train loss: 5.1708429359599334e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6462:\n",
      "train loss: 4.7186964485847974e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6463:\n",
      "train loss: 5.12541196289637e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6464:\n",
      "train loss: 4.549688649290551e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6465:\n",
      "train loss: 5.296860594714264e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6466:\n",
      "train loss: 4.7119235328485737e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6467:\n",
      "train loss: 5.129445260005703e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6468:\n",
      "train loss: 4.5929541644260045e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6469:\n",
      "train loss: 5.1706551022804035e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6470:\n",
      "train loss: 4.62808159258241e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6471:\n",
      "train loss: 5.257992028919866e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6472:\n",
      "train loss: 4.6834367566001146e-14\n",
      "Epoch 06474: reducing learning rate of group 0 to 5.8920e-15.\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6473:\n",
      "train loss: 5.1201102362239616e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6474:\n",
      "train loss: 4.5403231988399466e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6475:\n",
      "train loss: 4.7116273640480815e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6476:\n",
      "train loss: 4.206228073243127e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6477:\n",
      "train loss: 5.175484495001397e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6478:\n",
      "train loss: 4.5244864773692044e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6479:\n",
      "train loss: 4.902549038522064e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6480:\n",
      "train loss: 4.4520226537236124e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6481:\n",
      "train loss: 4.912295181392737e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6482:\n",
      "train loss: 4.40355711494766e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6483:\n",
      "train loss: 5.0014804594133014e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6484:\n",
      "train loss: 4.4943231051545757e-14\n",
      "Epoch 06486: reducing learning rate of group 0 to 5.5974e-15.\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6485:\n",
      "train loss: 4.8373562867318135e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6486:\n",
      "train loss: 4.2916977778353344e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6487:\n",
      "train loss: 4.547269247373414e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6488:\n",
      "train loss: 4.054656223517167e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6489:\n",
      "train loss: 4.761445167637059e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6490:\n",
      "train loss: 4.322823960339665e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6491:\n",
      "train loss: 4.654910562883462e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6492:\n",
      "train loss: 4.1449957635180506e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6493:\n",
      "train loss: 4.746533221161412e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6494:\n",
      "train loss: 4.1731826515642864e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6495:\n",
      "train loss: 4.759726427768173e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6496:\n",
      "train loss: 4.282940501905207e-14\n",
      "Epoch 06498: reducing learning rate of group 0 to 5.3175e-15.\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6497:\n",
      "train loss: 4.668973338932131e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6498:\n",
      "train loss: 4.1710820471107547e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6499:\n",
      "train loss: 4.3190093586252e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6500:\n",
      "train loss: 3.809202485738197e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6501:\n",
      "train loss: 4.533376110553977e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6502:\n",
      "train loss: 4.110249860945867e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6503:\n",
      "train loss: 4.327832038711139e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6504:\n",
      "train loss: 3.958634243371335e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6505:\n",
      "train loss: 4.513245613880692e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6506:\n",
      "train loss: 4.005051505764122e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6507:\n",
      "train loss: 4.5103146021146477e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6508:\n",
      "train loss: 4.020926883218282e-14\n",
      "Epoch 06510: reducing learning rate of group 0 to 5.0516e-15.\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6509:\n",
      "train loss: 4.4026534765560196e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6510:\n",
      "train loss: 3.9511170629015204e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6511:\n",
      "train loss: 4.110115151289441e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6512:\n",
      "train loss: 3.638426666953243e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6513:\n",
      "train loss: 4.4009785216286205e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6514:\n",
      "train loss: 3.896583520903207e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6515:\n",
      "train loss: 4.1356306279568224e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6516:\n",
      "train loss: 3.6475446032928555e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6517:\n",
      "train loss: 4.3032409927073844e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6518:\n",
      "train loss: 3.907053208798162e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6519:\n",
      "train loss: 4.157290243608769e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6520:\n",
      "train loss: 3.7641831609719305e-14\n",
      "Epoch 06522: reducing learning rate of group 0 to 4.7990e-15.\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6521:\n",
      "train loss: 4.2419809276347656e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6522:\n",
      "train loss: 3.892290480942415e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6523:\n",
      "train loss: 3.76298147740708e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6524:\n",
      "train loss: 3.42685318835447e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6525:\n",
      "train loss: 4.2283811882459686e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6526:\n",
      "train loss: 3.863187154913086e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6527:\n",
      "train loss: 3.8117153909538254e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6528:\n",
      "train loss: 3.3879454033786774e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6529:\n",
      "train loss: 4.2170111957478116e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6530:\n",
      "train loss: 3.765488268629885e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6531:\n",
      "train loss: 3.839226904070385e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6532:\n",
      "train loss: 3.45659001080258e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6533:\n",
      "train loss: 4.0660375526078706e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6534:\n",
      "train loss: 3.776098460777617e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6535:\n",
      "train loss: 3.841831778161595e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6536:\n",
      "train loss: 3.490784470542303e-14\n",
      "Epoch 06538: reducing learning rate of group 0 to 4.5591e-15.\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6537:\n",
      "train loss: 4.1363231789457e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6538:\n",
      "train loss: 3.7290472108872424e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6539:\n",
      "train loss: 3.465257047262886e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6540:\n",
      "train loss: 3.1547593001350145e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6541:\n",
      "train loss: 4.1261145423611984e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6542:\n",
      "train loss: 3.71901028212946e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6543:\n",
      "train loss: 3.537213388827223e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6544:\n",
      "train loss: 3.0993850718717233e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6545:\n",
      "train loss: 4.1166194448062146e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6546:\n",
      "train loss: 3.687380162835215e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6547:\n",
      "train loss: 3.5503227059188196e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6548:\n",
      "train loss: 3.2061982744686475e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6549:\n",
      "train loss: 4.0688916290012873e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6550:\n",
      "train loss: 3.710988829492777e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6551:\n",
      "train loss: 3.5611290858967814e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6552:\n",
      "train loss: 3.167443474522056e-14\n",
      "Epoch 06554: reducing learning rate of group 0 to 4.3311e-15.\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6553:\n",
      "train loss: 4.029783042825506e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6554:\n",
      "train loss: 3.636195194505339e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6555:\n",
      "train loss: 3.2084885470381846e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6556:\n",
      "train loss: 2.8715561186144006e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6557:\n",
      "train loss: 4.049488008905569e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6558:\n",
      "train loss: 3.66596284362197e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6559:\n",
      "train loss: 3.243661546273816e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6560:\n",
      "train loss: 2.870247654209288e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6561:\n",
      "train loss: 4.011872519409531e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6562:\n",
      "train loss: 3.554436227246092e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6563:\n",
      "train loss: 3.2402053809010453e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6564:\n",
      "train loss: 2.9104347289287036e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6565:\n",
      "train loss: 3.950435980629487e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6566:\n",
      "train loss: 3.544124362635976e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6567:\n",
      "train loss: 3.3024928687044765e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6568:\n",
      "train loss: 2.92374684723912e-14\n",
      "Epoch 06570: reducing learning rate of group 0 to 4.1146e-15.\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6569:\n",
      "train loss: 3.9430528856478126e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6570:\n",
      "train loss: 3.485138597289217e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6571:\n",
      "train loss: 2.934749998313938e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6572:\n",
      "train loss: 2.6414659178844352e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6573:\n",
      "train loss: 3.809631385144342e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6574:\n",
      "train loss: 3.425524661995813e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6575:\n",
      "train loss: 3.091584487451286e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6576:\n",
      "train loss: 2.710303578221657e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6577:\n",
      "train loss: 3.708062448664839e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6578:\n",
      "train loss: 3.3753978584413066e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6579:\n",
      "train loss: 3.1813314241835997e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6580:\n",
      "train loss: 2.747432746116476e-14\n",
      "Epoch 06582: reducing learning rate of group 0 to 3.9088e-15.\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6581:\n",
      "train loss: 3.689888207787758e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6582:\n",
      "train loss: 3.3799881336551925e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6583:\n",
      "train loss: 2.915887423702142e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6584:\n",
      "train loss: 2.5816031125581433e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6585:\n",
      "train loss: 3.6677100957054816e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6586:\n",
      "train loss: 3.2080846153659735e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6587:\n",
      "train loss: 2.9018021633156274e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6588:\n",
      "train loss: 2.6579055386320532e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6589:\n",
      "train loss: 3.5515158742155314e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6590:\n",
      "train loss: 3.1876924940587967e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6591:\n",
      "train loss: 3.015335595424711e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6592:\n",
      "train loss: 2.7513743235429317e-14\n",
      "Epoch 06594: reducing learning rate of group 0 to 3.7134e-15.\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6593:\n",
      "train loss: 3.431931493793578e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6594:\n",
      "train loss: 3.118242305929232e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6595:\n",
      "train loss: 2.803576417596041e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6596:\n",
      "train loss: 2.4434727654534226e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6597:\n",
      "train loss: 3.370122432471101e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6598:\n",
      "train loss: 3.000914331085087e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6599:\n",
      "train loss: 2.8890240839865204e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6600:\n",
      "train loss: 2.518517812241789e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6601:\n",
      "train loss: 3.3530318985205156e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6602:\n",
      "train loss: 2.9899683075364746e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6603:\n",
      "train loss: 2.976275765440746e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6604:\n",
      "train loss: 2.6268546414534025e-14\n",
      "Epoch 06606: reducing learning rate of group 0 to 3.5277e-15.\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6605:\n",
      "train loss: 3.173419840490202e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6606:\n",
      "train loss: 2.881870944889902e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6607:\n",
      "train loss: 2.68492169386299e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6608:\n",
      "train loss: 2.399217299271586e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6609:\n",
      "train loss: 3.120512600399872e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6610:\n",
      "train loss: 2.719657931574962e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6611:\n",
      "train loss: 2.8444276901937423e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6612:\n",
      "train loss: 2.4673165702550496e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6613:\n",
      "train loss: 3.0827406429331514e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6614:\n",
      "train loss: 2.7720202859947025e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6615:\n",
      "train loss: 2.8597326482479347e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6616:\n",
      "train loss: 2.567533070997219e-14\n",
      "Epoch 06618: reducing learning rate of group 0 to 3.3513e-15.\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6617:\n",
      "train loss: 3.021838469652374e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6618:\n",
      "train loss: 2.659202421181863e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6619:\n",
      "train loss: 2.6926027360876892e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6620:\n",
      "train loss: 2.299074938033614e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6621:\n",
      "train loss: 3.0137101179118274e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6622:\n",
      "train loss: 2.6833521685097743e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6623:\n",
      "train loss: 2.7167510447333878e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6624:\n",
      "train loss: 2.354647045638653e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6625:\n",
      "train loss: 2.8403553813961637e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6626:\n",
      "train loss: 2.5595899773105065e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6627:\n",
      "train loss: 2.798665898425752e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6628:\n",
      "train loss: 2.4069444552000156e-14\n",
      "Epoch 06630: reducing learning rate of group 0 to 3.1838e-15.\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6629:\n",
      "train loss: 2.7674250047035843e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6630:\n",
      "train loss: 2.472605140072552e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6631:\n",
      "train loss: 2.523103996380205e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6632:\n",
      "train loss: 2.2488342934762487e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6633:\n",
      "train loss: 2.7858971623251618e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6634:\n",
      "train loss: 2.4399605993427448e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6635:\n",
      "train loss: 2.613626660404602e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6636:\n",
      "train loss: 2.324094740454671e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6637:\n",
      "train loss: 2.7398959355740044e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6638:\n",
      "train loss: 2.421819231393956e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6639:\n",
      "train loss: 2.6220140618169862e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6640:\n",
      "train loss: 2.382982965901372e-14\n",
      "Epoch 06642: reducing learning rate of group 0 to 3.0246e-15.\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6641:\n",
      "train loss: 2.7196971007979602e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6642:\n",
      "train loss: 2.343862958240991e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6643:\n",
      "train loss: 2.4556440018507518e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6644:\n",
      "train loss: 2.1182587314876347e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6645:\n",
      "train loss: 2.6728191798476048e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6646:\n",
      "train loss: 2.2964937255484467e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6647:\n",
      "train loss: 2.4787274651975258e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6648:\n",
      "train loss: 2.1617672127454148e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6649:\n",
      "train loss: 2.6175683478044918e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6650:\n",
      "train loss: 2.388228667726427e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6651:\n",
      "train loss: 2.45152059151822e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6652:\n",
      "train loss: 2.2124769153598458e-14\n",
      "Epoch 06654: reducing learning rate of group 0 to 2.8734e-15.\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6653:\n",
      "train loss: 2.627160539978682e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6654:\n",
      "train loss: 2.4269102003012677e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6655:\n",
      "train loss: 2.1702606047387025e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6656:\n",
      "train loss: 1.8890657739534008e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6657:\n",
      "train loss: 2.6508775399281775e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6658:\n",
      "train loss: 2.3861298728409814e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6659:\n",
      "train loss: 2.2024200676147836e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6660:\n",
      "train loss: 1.976272547577212e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6661:\n",
      "train loss: 2.5700641664372367e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6662:\n",
      "train loss: 2.280058755281286e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6663:\n",
      "train loss: 2.231325690119835e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6664:\n",
      "train loss: 1.9650819606657214e-14\n",
      "Epoch 06666: reducing learning rate of group 0 to 2.7297e-15.\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6665:\n",
      "train loss: 2.617388555755686e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6666:\n",
      "train loss: 2.2981811894294597e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6667:\n",
      "train loss: 2.0381816468866476e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6668:\n",
      "train loss: 1.7794486681383185e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6669:\n",
      "train loss: 2.4761778372980853e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6670:\n",
      "train loss: 2.289572827647883e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6671:\n",
      "train loss: 2.0561369976544703e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6672:\n",
      "train loss: 1.8499382824022876e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6673:\n",
      "train loss: 2.4643862338752396e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6674:\n",
      "train loss: 2.2328738370071515e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6675:\n",
      "train loss: 2.10144955543107e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6676:\n",
      "train loss: 1.98664223810352e-14\n",
      "Epoch 06678: reducing learning rate of group 0 to 2.5932e-15.\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6677:\n",
      "train loss: 2.4061097620953523e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6678:\n",
      "train loss: 2.2265044320088837e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6679:\n",
      "train loss: 1.945571254332389e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6680:\n",
      "train loss: 1.7073804210387458e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6681:\n",
      "train loss: 2.3444615672492292e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6682:\n",
      "train loss: 2.102318780741372e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6683:\n",
      "train loss: 2.0648263117309473e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6684:\n",
      "train loss: 1.848735517760031e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6685:\n",
      "train loss: 2.3218275766453686e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6686:\n",
      "train loss: 2.0514860972234877e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6687:\n",
      "train loss: 2.191326303796995e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6688:\n",
      "train loss: 1.8757498410400493e-14\n",
      "Epoch 06690: reducing learning rate of group 0 to 2.4635e-15.\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6689:\n",
      "train loss: 2.2460787813111285e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6690:\n",
      "train loss: 1.9265617725063174e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6691:\n",
      "train loss: 1.9927780563125254e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6692:\n",
      "train loss: 1.809876822061756e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6693:\n",
      "train loss: 2.099829523138462e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6694:\n",
      "train loss: 1.878665072133034e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6695:\n",
      "train loss: 2.1108604631888888e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6696:\n",
      "train loss: 1.9143756828192197e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6697:\n",
      "train loss: 1.984363831921273e-14\n",
      "Epoch 06699: reducing learning rate of group 0 to 2.3404e-15.\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6698:\n",
      "train loss: 1.788832943309225e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6699:\n",
      "train loss: 2.1281043092084554e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6700:\n",
      "train loss: 1.9057296870620135e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6701:\n",
      "train loss: 1.7668032552301642e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6702:\n",
      "train loss: 1.5580727793476786e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6703:\n",
      "train loss: 2.144086302245234e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6704:\n",
      "train loss: 2.002983520559888e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6705:\n",
      "train loss: 1.7761412096600953e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6706:\n",
      "train loss: 1.6152819488479945e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6707:\n",
      "train loss: 2.193713492923696e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6708:\n",
      "train loss: 1.9269319283400832e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6709:\n",
      "train loss: 1.822361453169843e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6710:\n",
      "train loss: 1.661700197493488e-14\n",
      "Epoch 06712: reducing learning rate of group 0 to 2.2233e-15.\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6711:\n",
      "train loss: 2.1016586991031634e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6712:\n",
      "train loss: 1.8018809618073963e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6713:\n",
      "train loss: 1.6976357083175216e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6714:\n",
      "train loss: 1.547035769368686e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6715:\n",
      "train loss: 1.960327628301649e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6716:\n",
      "train loss: 1.6344186508083944e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6717:\n",
      "train loss: 1.8423104066946405e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6718:\n",
      "train loss: 1.6637060253057896e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6719:\n",
      "train loss: 1.8280590447977678e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6720:\n",
      "train loss: 1.636022725794446e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6721:\n",
      "train loss: 1.8890388417718114e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6722:\n",
      "train loss: 1.747179634340605e-14\n",
      "Epoch 06724: reducing learning rate of group 0 to 2.1122e-15.\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6723:\n",
      "train loss: 1.7664793807396706e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6724:\n",
      "train loss: 1.5826917978816945e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6725:\n",
      "train loss: 1.8613169890431202e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6726:\n",
      "train loss: 1.7189097956060806e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6727:\n",
      "train loss: 1.690910672157869e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6728:\n",
      "train loss: 1.466087582208477e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6729:\n",
      "train loss: 1.7792675028123936e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6730:\n",
      "train loss: 1.6391313155229042e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6731:\n",
      "train loss: 1.7389833747529848e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6732:\n",
      "train loss: 1.567539417749704e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6733:\n",
      "train loss: 1.8306398124255805e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6734:\n",
      "train loss: 1.6745792115595097e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6735:\n",
      "train loss: 1.7301213786758473e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6736:\n",
      "train loss: 1.5740072695434228e-14\n",
      "Epoch 06738: reducing learning rate of group 0 to 2.0066e-15.\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6737:\n",
      "train loss: 1.7771370084215372e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6738:\n",
      "train loss: 1.659895534303866e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6739:\n",
      "train loss: 1.5739216682346845e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6740:\n",
      "train loss: 1.4279352659377033e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6741:\n",
      "train loss: 1.7030037520104553e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6742:\n",
      "train loss: 1.606185308699175e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6743:\n",
      "train loss: 1.608616842943591e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6744:\n",
      "train loss: 1.4819578393505144e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6745:\n",
      "train loss: 1.6744211022326347e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6746:\n",
      "train loss: 1.5299462159509187e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6747:\n",
      "train loss: 1.6591369825738848e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6748:\n",
      "train loss: 1.5165986866672488e-14\n",
      "Epoch 06750: reducing learning rate of group 0 to 1.9062e-15.\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6749:\n",
      "train loss: 1.6762032898381863e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6750:\n",
      "train loss: 1.5944195837655507e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6751:\n",
      "train loss: 1.3973474978721288e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6752:\n",
      "train loss: 1.3303127002915817e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6753:\n",
      "train loss: 1.6950705922513616e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6754:\n",
      "train loss: 1.4667951069923512e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6755:\n",
      "train loss: 1.4295886175232218e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6756:\n",
      "train loss: 1.2617293910602365e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6757:\n",
      "train loss: 1.6609700078754523e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6758:\n",
      "train loss: 1.4307892212832775e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6759:\n",
      "train loss: 1.4856423535018122e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6760:\n",
      "train loss: 1.4265972335387563e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6761:\n",
      "train loss: 1.5903839756140306e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6762:\n",
      "train loss: 1.4093994679319748e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6763:\n",
      "train loss: 1.5564566848846e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6764:\n",
      "train loss: 1.4005450704836765e-14\n",
      "Epoch 06766: reducing learning rate of group 0 to 1.8109e-15.\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6765:\n",
      "train loss: 1.5499331640196276e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6766:\n",
      "train loss: 1.460655583536981e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6767:\n",
      "train loss: 1.4409904380580614e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6768:\n",
      "train loss: 1.3034246055823328e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6769:\n",
      "train loss: 1.5555283645051123e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6770:\n",
      "train loss: 1.4387742871226127e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6771:\n",
      "train loss: 1.3881494664171175e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6772:\n",
      "train loss: 1.3839402048519776e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6773:\n",
      "train loss: 1.5710726964861994e-14\n",
      "Epoch 06775: reducing learning rate of group 0 to 1.7204e-15.\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6774:\n",
      "train loss: 1.4417641525367162e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6775:\n",
      "train loss: 1.4162596768481262e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6776:\n",
      "train loss: 1.3319432171395156e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6777:\n",
      "train loss: 1.3951644919887433e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6778:\n",
      "train loss: 1.2746641223698631e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6779:\n",
      "train loss: 1.5017862662113417e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6780:\n",
      "train loss: 1.3051419312092186e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6781:\n",
      "train loss: 1.408185951411371e-14\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6782:\n",
      "train loss: 1.3389241245101784e-14\n",
      "Epoch 06784: reducing learning rate of group 0 to 1.6344e-15.\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6783:\n",
      "train loss: 1.4758973388770038e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6784:\n",
      "train loss: 1.2893044284856996e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6785:\n",
      "train loss: 1.2776068678489942e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6786:\n",
      "train loss: 1.0777703081060128e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6787:\n",
      "train loss: 1.3785963946913993e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6788:\n",
      "train loss: 1.2863476506985378e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6789:\n",
      "train loss: 1.2636656777501032e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6790:\n",
      "train loss: 1.166133489898198e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6791:\n",
      "train loss: 1.3996329234178516e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6792:\n",
      "train loss: 1.291028335390485e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6793:\n",
      "train loss: 1.3529647480929705e-14\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6794:\n",
      "train loss: 1.1559430784804463e-14\n",
      "Epoch 06796: reducing learning rate of group 0 to 1.5526e-15.\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6795:\n",
      "train loss: 1.3562445275845028e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6796:\n",
      "train loss: 1.2024746027483585e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6797:\n",
      "train loss: 1.2989595444052008e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6798:\n",
      "train loss: 1.1054962362413555e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6799:\n",
      "train loss: 1.2871559600891477e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6800:\n",
      "train loss: 1.2050441571567814e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6801:\n",
      "train loss: 1.3906816964275747e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6802:\n",
      "train loss: 1.1918198462179185e-14\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6803:\n",
      "train loss: 1.2084949960942317e-14\n",
      "Epoch 06805: reducing learning rate of group 0 to 1.4750e-15.\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6804:\n",
      "train loss: 1.1179441610956469e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6805:\n",
      "train loss: 1.4104558489880312e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6806:\n",
      "train loss: 1.2913884076273644e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6807:\n",
      "train loss: 1.0375360741971925e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6808:\n",
      "train loss: 1.0521402139468553e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6809:\n",
      "train loss: 1.2975553515353845e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6810:\n",
      "train loss: 1.2624262779218455e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6811:\n",
      "train loss: 1.1603651773803362e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6812:\n",
      "train loss: 1.1015513798845283e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6813:\n",
      "train loss: 1.326230854274459e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6814:\n",
      "train loss: 1.1488637051008623e-14\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6815:\n",
      "train loss: 1.1971596114864257e-14\n",
      "Epoch 06817: reducing learning rate of group 0 to 1.4013e-15.\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6816:\n",
      "train loss: 1.1175385246137351e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6817:\n",
      "train loss: 1.2656476168877686e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6818:\n",
      "train loss: 1.1334627255537414e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6819:\n",
      "train loss: 1.0879142196474819e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6820:\n",
      "train loss: 1.0513636553870373e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6821:\n",
      "train loss: 1.2022799266832753e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6822:\n",
      "train loss: 1.1388260823778028e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6823:\n",
      "train loss: 1.0527437735376067e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6824:\n",
      "train loss: 9.416351150986833e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6825:\n",
      "train loss: 1.1593692772848689e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6826:\n",
      "train loss: 1.2169667807972744e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6827:\n",
      "train loss: 1.1480896034840834e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6828:\n",
      "train loss: 9.997654695355238e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6829:\n",
      "train loss: 1.1936974529997792e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6830:\n",
      "train loss: 1.106654556666641e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6831:\n",
      "train loss: 1.1359017450082176e-14\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6832:\n",
      "train loss: 1.0118560499549938e-14\n",
      "Epoch 06834: reducing learning rate of group 0 to 1.3312e-15.\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6833:\n",
      "train loss: 1.1909533049345968e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6834:\n",
      "train loss: 1.128976168559841e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6835:\n",
      "train loss: 1.0038943505572995e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6836:\n",
      "train loss: 1.0120843948151778e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6837:\n",
      "train loss: 1.1343056259694759e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6838:\n",
      "train loss: 1.1204203843483376e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6839:\n",
      "train loss: 1.0621325443020805e-14\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6840:\n",
      "train loss: 9.571908587605614e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6841:\n",
      "train loss: 1.1091997058791647e-14\n",
      "Epoch 06843: reducing learning rate of group 0 to 1.2646e-15.\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6842:\n",
      "train loss: 1.109371311854242e-14\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6843:\n",
      "train loss: 1.0470342102069894e-14\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6844:\n",
      "train loss: 9.416076068490807e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6845:\n",
      "train loss: 1.0419720183274757e-14\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6846:\n",
      "train loss: 1.0080044255179708e-14\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6847:\n",
      "train loss: 1.0300558963911353e-14\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6848:\n",
      "train loss: 9.621309953516232e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6849:\n",
      "train loss: 1.1121925250166729e-14\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6850:\n",
      "train loss: 1.0039973434029999e-14\n",
      "Epoch 06852: reducing learning rate of group 0 to 1.2014e-15.\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6851:\n",
      "train loss: 1.0261912744800242e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6852:\n",
      "train loss: 1.0208118831441662e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6853:\n",
      "train loss: 9.860519966464481e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6854:\n",
      "train loss: 9.095825153833458e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6855:\n",
      "train loss: 1.0965397769139551e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6856:\n",
      "train loss: 1.0664017002436766e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6857:\n",
      "train loss: 9.915464546552718e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6858:\n",
      "train loss: 9.074315790398579e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6859:\n",
      "train loss: 1.0949199813395234e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6860:\n",
      "train loss: 1.0363331243802309e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6861:\n",
      "train loss: 9.584148596115093e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6862:\n",
      "train loss: 9.020662049767491e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6863:\n",
      "train loss: 1.062313122270007e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6864:\n",
      "train loss: 9.671540412642312e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6865:\n",
      "train loss: 9.794510200112947e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6866:\n",
      "train loss: 8.968218677268858e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6867:\n",
      "train loss: 9.933311930966774e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6868:\n",
      "train loss: 9.821921485656783e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6869:\n",
      "train loss: 9.454322752689058e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6870:\n",
      "train loss: 8.587847609066444e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6871:\n",
      "train loss: 1.0042449564701488e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6872:\n",
      "train loss: 9.541001168407859e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6873:\n",
      "train loss: 1.0299339130130318e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6874:\n",
      "train loss: 9.539519166351833e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6875:\n",
      "train loss: 9.79386988476207e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6876:\n",
      "train loss: 8.931465386764852e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6877:\n",
      "train loss: 1.029919862853322e-14\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6878:\n",
      "train loss: 9.169547401296357e-15\n",
      "Epoch 06880: reducing learning rate of group 0 to 1.1413e-15.\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6879:\n",
      "train loss: 1.0091555895642e-14\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6880:\n",
      "train loss: 9.60330498855951e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6881:\n",
      "train loss: 8.930314407195208e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6882:\n",
      "train loss: 8.7477595151443e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6883:\n",
      "train loss: 1.0006986016782471e-14\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6884:\n",
      "train loss: 9.041657950521964e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6885:\n",
      "train loss: 9.567734574630973e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6886:\n",
      "train loss: 8.408528590663298e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6887:\n",
      "train loss: 9.561057653439473e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6888:\n",
      "train loss: 8.910126030112232e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6889:\n",
      "train loss: 8.24575256445062e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6890:\n",
      "train loss: 8.370547242425685e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6891:\n",
      "train loss: 9.168817128634075e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6892:\n",
      "train loss: 9.010714183880593e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6893:\n",
      "train loss: 8.745283339064795e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6894:\n",
      "train loss: 8.750743127392058e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6895:\n",
      "train loss: 9.449558208656763e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6896:\n",
      "train loss: 8.69750433445347e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6897:\n",
      "train loss: 8.45936480135446e-15\n",
      "Epoch 06899: reducing learning rate of group 0 to 1.0843e-15.\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6898:\n",
      "train loss: 8.729708481572634e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6899:\n",
      "train loss: 9.1899417436563e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6900:\n",
      "train loss: 8.901464155783422e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6901:\n",
      "train loss: 8.712763129348261e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6902:\n",
      "train loss: 7.696854679068117e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6903:\n",
      "train loss: 9.375590905161505e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6904:\n",
      "train loss: 8.341927827094105e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6905:\n",
      "train loss: 7.86433256396148e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6906:\n",
      "train loss: 7.993411986626839e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6907:\n",
      "train loss: 9.2690442894132e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6908:\n",
      "train loss: 8.766669698076366e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6909:\n",
      "train loss: 8.45655661278295e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6910:\n",
      "train loss: 7.983991211914727e-15\n",
      "Epoch 06912: reducing learning rate of group 0 to 1.0301e-15.\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6911:\n",
      "train loss: 9.203525181692044e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6912:\n",
      "train loss: 8.425144836726463e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6913:\n",
      "train loss: 8.18232463612898e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6914:\n",
      "train loss: 7.798458385248533e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6915:\n",
      "train loss: 8.736130946263073e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6916:\n",
      "train loss: 8.581309505432098e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6917:\n",
      "train loss: 8.942117678721472e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6918:\n",
      "train loss: 8.023209699087998e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6919:\n",
      "train loss: 9.146621739932808e-15\n",
      "Epoch 06921: reducing learning rate of group 0 to 9.7856e-16.\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6920:\n",
      "train loss: 7.915814722918145e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6921:\n",
      "train loss: 8.410696353557953e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6922:\n",
      "train loss: 7.595731594490074e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6923:\n",
      "train loss: 7.921171768479074e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6924:\n",
      "train loss: 7.612484881840301e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6925:\n",
      "train loss: 8.550107724707392e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6926:\n",
      "train loss: 7.990630785748646e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6927:\n",
      "train loss: 8.159749721356277e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6928:\n",
      "train loss: 8.108949820287998e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6929:\n",
      "train loss: 8.241368401018813e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6930:\n",
      "train loss: 7.702173593535712e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6931:\n",
      "train loss: 7.56359365052214e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6932:\n",
      "train loss: 8.033650902114332e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6933:\n",
      "train loss: 8.74489178204528e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6934:\n",
      "train loss: 7.943411341526209e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6935:\n",
      "train loss: 7.602266328084681e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6936:\n",
      "train loss: 7.668784819702571e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6937:\n",
      "train loss: 8.712247880705334e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6938:\n",
      "train loss: 7.821746038741115e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6939:\n",
      "train loss: 7.864415987944864e-15\n",
      "Epoch 06941: reducing learning rate of group 0 to 9.2963e-16.\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6940:\n",
      "train loss: 7.637274377129156e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6941:\n",
      "train loss: 8.159156533143744e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6942:\n",
      "train loss: 8.017831539012131e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6943:\n",
      "train loss: 7.078752057010324e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6944:\n",
      "train loss: 6.344491533966645e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6945:\n",
      "train loss: 8.413872246297758e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6946:\n",
      "train loss: 7.730652575257106e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6947:\n",
      "train loss: 6.871507230768314e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6948:\n",
      "train loss: 7.489959978187648e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6949:\n",
      "train loss: 7.508265021063917e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6950:\n",
      "train loss: 6.975327877435637e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6951:\n",
      "train loss: 7.774967267119127e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6952:\n",
      "train loss: 7.314303403385356e-15\n",
      "Epoch 06954: reducing learning rate of group 0 to 8.8315e-16.\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6953:\n",
      "train loss: 7.479839703647749e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6954:\n",
      "train loss: 6.966695908981849e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6955:\n",
      "train loss: 6.3623745629755234e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6956:\n",
      "train loss: 6.284643814871795e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6957:\n",
      "train loss: 7.389264792366189e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6958:\n",
      "train loss: 7.211857344631735e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6959:\n",
      "train loss: 6.629081346523609e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6960:\n",
      "train loss: 6.171501999479879e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6961:\n",
      "train loss: 7.38108721907268e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6962:\n",
      "train loss: 7.54179160271395e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6963:\n",
      "train loss: 6.381843373256253e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6964:\n",
      "train loss: 5.948691050533415e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6965:\n",
      "train loss: 7.541647377392947e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6966:\n",
      "train loss: 7.210838497034439e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6967:\n",
      "train loss: 7.055268477085887e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6968:\n",
      "train loss: 6.16226523388495e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6969:\n",
      "train loss: 7.40198643440183e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6970:\n",
      "train loss: 6.474376356726561e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6971:\n",
      "train loss: 7.18670529939764e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6972:\n",
      "train loss: 6.468159532218666e-15\n",
      "Epoch 06974: reducing learning rate of group 0 to 8.3899e-16.\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6973:\n",
      "train loss: 6.841846143678216e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6974:\n",
      "train loss: 6.557060262234179e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6975:\n",
      "train loss: 7.0721058779278356e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6976:\n",
      "train loss: 6.853659844370683e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6977:\n",
      "train loss: 6.800624294057802e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6978:\n",
      "train loss: 6.577744834957942e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6979:\n",
      "train loss: 6.6041762918328945e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6980:\n",
      "train loss: 5.753327022811768e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6981:\n",
      "train loss: 6.803756728058088e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6982:\n",
      "train loss: 6.177039897109605e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6983:\n",
      "train loss: 6.906968265176779e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6984:\n",
      "train loss: 6.597835621618069e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6985:\n",
      "train loss: 6.1865772503723275e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6986:\n",
      "train loss: 5.914349623522397e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6987:\n",
      "train loss: 6.9447541593723814e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6988:\n",
      "train loss: 7.111871558491562e-15\n",
      "Epoch 06990: reducing learning rate of group 0 to 7.9704e-16.\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6989:\n",
      "train loss: 6.0195434406086766e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6990:\n",
      "train loss: 5.513544657556212e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6991:\n",
      "train loss: 6.7246800155818e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6992:\n",
      "train loss: 7.035615062618689e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6993:\n",
      "train loss: 6.073584302515851e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6994:\n",
      "train loss: 5.781704121912303e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6995:\n",
      "train loss: 7.123272542666255e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6996:\n",
      "train loss: 6.768356390432309e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6997:\n",
      "train loss: 6.321452280530827e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6998:\n",
      "train loss: 6.1475196682380115e-15\n",
      "Epoch 07000: reducing learning rate of group 0 to 7.5719e-16.\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6999:\n",
      "train loss: 6.5999977573704935e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7000:\n",
      "train loss: 6.397892291006911e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7001:\n",
      "train loss: 5.661005304951385e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7002:\n",
      "train loss: 5.9903772858244715e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7003:\n",
      "train loss: 6.7499483583593305e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7004:\n",
      "train loss: 6.44769487514568e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7005:\n",
      "train loss: 6.390896837675468e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7006:\n",
      "train loss: 6.174637673837143e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 7007:\n",
      "train loss: 5.960792430664002e-15\n",
      "Epoch 07009: reducing learning rate of group 0 to 7.1933e-16.\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7008:\n",
      "train loss: 6.1629876029412805e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7009:\n",
      "train loss: 6.28443633712434e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7010:\n",
      "train loss: 6.62833880581516e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7011:\n",
      "train loss: 5.723201526748248e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7012:\n",
      "train loss: 5.5457283422456225e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7013:\n",
      "train loss: 6.000528961727692e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7014:\n",
      "train loss: 6.048265039047816e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7015:\n",
      "train loss: 5.535174680371322e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 7016:\n",
      "train loss: 6.195503476184389e-15\n",
      "Epoch 07018: reducing learning rate of group 0 to 6.8336e-16.\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7017:\n",
      "train loss: 6.584458551426695e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7018:\n",
      "train loss: 5.827823319265486e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7019:\n",
      "train loss: 5.415114935478635e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7020:\n",
      "train loss: 5.0541518698649225e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7021:\n",
      "train loss: 5.3749404263831726e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7022:\n",
      "train loss: 5.256655489495389e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7023:\n",
      "train loss: 6.3711665970533446e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7024:\n",
      "train loss: 6.091607193312361e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7025:\n",
      "train loss: 5.902821438131708e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7026:\n",
      "train loss: 5.063337952585677e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7027:\n",
      "train loss: 5.800091356063841e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 7028:\n",
      "train loss: 5.717826076386716e-15\n",
      "Epoch 07030: reducing learning rate of group 0 to 6.4919e-16.\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7029:\n",
      "train loss: 6.069489380550763e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7030:\n",
      "train loss: 5.593431147513497e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7031:\n",
      "train loss: 4.635007424207648e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7032:\n",
      "train loss: 4.536646442260847e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7033:\n",
      "train loss: 5.717208433744682e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7034:\n",
      "train loss: 5.777524628149221e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7035:\n",
      "train loss: 4.87172281482604e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7036:\n",
      "train loss: 4.533229495231394e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7037:\n",
      "train loss: 5.136673966773481e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7038:\n",
      "train loss: 4.84121525446419e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7039:\n",
      "train loss: 4.43141979483319e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7040:\n",
      "train loss: 4.8658157921411695e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7041:\n",
      "train loss: 5.548620736373936e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7042:\n",
      "train loss: 4.893388423608324e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7043:\n",
      "train loss: 5.392664409099154e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7044:\n",
      "train loss: 6.0126836926287045e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7045:\n",
      "train loss: 4.395467499993667e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7046:\n",
      "train loss: 3.662269210970982e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7047:\n",
      "train loss: 6.1056311543985476e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7048:\n",
      "train loss: 4.872565066298122e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7049:\n",
      "train loss: 4.961461844203975e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7050:\n",
      "train loss: 4.805628966451243e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7051:\n",
      "train loss: 5.2363531799146786e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7052:\n",
      "train loss: 4.6879583118147975e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7053:\n",
      "train loss: 5.975930499997095e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7054:\n",
      "train loss: 5.687039989141881e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7055:\n",
      "train loss: 3.629049509541569e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7056:\n",
      "train loss: 4.305774378026461e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7057:\n",
      "train loss: 5.095803508416087e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7058:\n",
      "train loss: 4.152852631623155e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7059:\n",
      "train loss: 6.36951756034937e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7060:\n",
      "train loss: 5.815096000446048e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7061:\n",
      "train loss: 3.6557989775226606e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7062:\n",
      "train loss: 3.434802795222596e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7063:\n",
      "train loss: 5.172938107773242e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7064:\n",
      "train loss: 4.59751309019893e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7065:\n",
      "train loss: 6.1187113725726586e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7066:\n",
      "train loss: 6.407984185818747e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7067:\n",
      "train loss: 3.3429173450275233e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7068:\n",
      "train loss: 1.9256607962471623e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7069:\n",
      "train loss: 7.0951204100946625e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7070:\n",
      "train loss: 6.198081323419737e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7071:\n",
      "train loss: 4.344760808261241e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7072:\n",
      "train loss: 5.091477871848337e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7073:\n",
      "train loss: 4.325150484682736e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7074:\n",
      "train loss: 3.62323224678249e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7075:\n",
      "train loss: 6.615268984212504e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 7076:\n",
      "train loss: 5.928025083862558e-15\n",
      "Epoch 07078: reducing learning rate of group 0 to 6.1673e-16.\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7077:\n",
      "train loss: 4.5270551906115746e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7078:\n",
      "train loss: 4.823548313400124e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7079:\n",
      "train loss: 4.92101721434804e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7080:\n",
      "train loss: 4.660285312704563e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7081:\n",
      "train loss: 5.878391953356423e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7082:\n",
      "train loss: 5.171350352980805e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7083:\n",
      "train loss: 3.5053790291995132e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7084:\n",
      "train loss: 4.1941678069583256e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 7085:\n",
      "train loss: 6.220754810185012e-15\n",
      "Epoch 07087: reducing learning rate of group 0 to 5.8590e-16.\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7086:\n",
      "train loss: 5.289888888058436e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7087:\n",
      "train loss: 4.9311083413243185e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7088:\n",
      "train loss: 4.86576530558163e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7089:\n",
      "train loss: 3.888209461262617e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7090:\n",
      "train loss: 4.2070164307749495e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7091:\n",
      "train loss: 5.225650595208561e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7092:\n",
      "train loss: 5.6287176791433484e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7093:\n",
      "train loss: 3.6176893319589845e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 7094:\n",
      "train loss: 4.715437767435305e-15\n",
      "Epoch 07096: reducing learning rate of group 0 to 5.5660e-16.\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7095:\n",
      "train loss: 4.426842033352232e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7096:\n",
      "train loss: 4.346464154379825e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7097:\n",
      "train loss: 4.499023981817366e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7098:\n",
      "train loss: 4.430392191608707e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7099:\n",
      "train loss: 4.280433494456208e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7100:\n",
      "train loss: 4.740009626568408e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7101:\n",
      "train loss: 4.9476323803158574e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7102:\n",
      "train loss: 4.154224861602374e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 7103:\n",
      "train loss: 5.172823660127432e-15\n",
      "Epoch 07105: reducing learning rate of group 0 to 5.2877e-16.\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7104:\n",
      "train loss: 4.01360218737765e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7105:\n",
      "train loss: 4.442053311512071e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7106:\n",
      "train loss: 4.112874941728186e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7107:\n",
      "train loss: 4.893947302187676e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7108:\n",
      "train loss: 4.386057816982462e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7109:\n",
      "train loss: 4.846859244404418e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7110:\n",
      "train loss: 3.8290703297796424e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7111:\n",
      "train loss: 5.072995983518018e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 7112:\n",
      "train loss: 4.0944796842000906e-15\n",
      "Epoch 07114: reducing learning rate of group 0 to 5.0233e-16.\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7113:\n",
      "train loss: 4.1166442580703135e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7114:\n",
      "train loss: 3.4854150845591118e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7115:\n",
      "train loss: 4.2242953664511906e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7116:\n",
      "train loss: 4.196258037549612e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7117:\n",
      "train loss: 3.890078160202834e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7118:\n",
      "train loss: 4.324306279880955e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7119:\n",
      "train loss: 3.7118814721110174e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7120:\n",
      "train loss: 3.7617042239177534e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 7121:\n",
      "train loss: 4.866991791144539e-15\n",
      "Epoch 07123: reducing learning rate of group 0 to 4.7722e-16.\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7122:\n",
      "train loss: 4.803615222106624e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7123:\n",
      "train loss: 3.92126811089237e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7124:\n",
      "train loss: 3.8711458356830476e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7125:\n",
      "train loss: 3.1601283629354128e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7126:\n",
      "train loss: 4.007704145132088e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7127:\n",
      "train loss: 4.452924814725576e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7128:\n",
      "train loss: 4.019932901050224e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7129:\n",
      "train loss: 3.0314992264431786e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 7130:\n",
      "train loss: 4.1596380128850145e-15\n",
      "Epoch 07132: reducing learning rate of group 0 to 4.5336e-16.\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7131:\n",
      "train loss: 4.380110163990854e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7132:\n",
      "train loss: 3.9285444065107796e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7133:\n",
      "train loss: 3.4801281484218218e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7134:\n",
      "train loss: 3.3215358999436807e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7135:\n",
      "train loss: 3.318928017150114e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7136:\n",
      "train loss: 2.7846138751254725e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7137:\n",
      "train loss: 4.436979123940518e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7138:\n",
      "train loss: 4.237811125318327e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 7139:\n",
      "train loss: 3.8499332192397085e-15\n",
      "Epoch 07141: reducing learning rate of group 0 to 4.3069e-16.\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7140:\n",
      "train loss: 3.2090936848806575e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7141:\n",
      "train loss: 4.347295628058935e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7142:\n",
      "train loss: 3.840008472555746e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7143:\n",
      "train loss: 2.7141447171682547e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7144:\n",
      "train loss: 3.4047131892063616e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7145:\n",
      "train loss: 4.064469066694001e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7146:\n",
      "train loss: 4.199641570853647e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7147:\n",
      "train loss: 3.615593125912887e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 7148:\n",
      "train loss: 3.0952222645546484e-15\n",
      "Epoch 07150: reducing learning rate of group 0 to 4.0915e-16.\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7149:\n",
      "train loss: 4.318644857688194e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7150:\n",
      "train loss: 3.844770427517848e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7151:\n",
      "train loss: 3.271085273091373e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7152:\n",
      "train loss: 3.228077078043079e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7153:\n",
      "train loss: 3.552032509159571e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7154:\n",
      "train loss: 3.705584085441009e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7155:\n",
      "train loss: 4.590491666040967e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7156:\n",
      "train loss: 3.837906534741552e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 7157:\n",
      "train loss: 3.3524533102063878e-15\n",
      "Epoch 07159: reducing learning rate of group 0 to 3.8870e-16.\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7158:\n",
      "train loss: 2.8458943059147262e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7159:\n",
      "train loss: 2.7617074510618783e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7160:\n",
      "train loss: 2.8689031844120383e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7161:\n",
      "train loss: 3.1970513805073897e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7162:\n",
      "train loss: 2.397375843729724e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7163:\n",
      "train loss: 2.909248484730877e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7164:\n",
      "train loss: 3.057477038696404e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7165:\n",
      "train loss: 3.5829302150571025e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 7166:\n",
      "train loss: 3.0888518586795543e-15\n",
      "Epoch 07168: reducing learning rate of group 0 to 3.6926e-16.\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7167:\n",
      "train loss: 3.789792844223179e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7168:\n",
      "train loss: 2.870864455588765e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7169:\n",
      "train loss: 1.9737955656848838e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7170:\n",
      "train loss: 2.482127119396037e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7171:\n",
      "train loss: 3.3644429943206446e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7172:\n",
      "train loss: 2.6734506873304505e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7173:\n",
      "train loss: 2.41488390534904e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7174:\n",
      "train loss: 2.7262303744816506e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 7175:\n",
      "train loss: 2.3972446954519004e-15\n",
      "Epoch 07177: reducing learning rate of group 0 to 3.5080e-16.\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7176:\n",
      "train loss: 2.464485976564578e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7177:\n",
      "train loss: 2.956083673811458e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7178:\n",
      "train loss: 3.0499750192477636e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7179:\n",
      "train loss: 2.5825477973664807e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7180:\n",
      "train loss: 2.49220389519802e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7181:\n",
      "train loss: 2.502050222972451e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7182:\n",
      "train loss: 2.0854585951809908e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7183:\n",
      "train loss: 2.9003478975972234e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 7184:\n",
      "train loss: 3.016246894896208e-15\n",
      "Epoch 07186: reducing learning rate of group 0 to 3.3326e-16.\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7185:\n",
      "train loss: 2.9890101334087437e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7186:\n",
      "train loss: 2.6111741895618744e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7187:\n",
      "train loss: 2.079684309363802e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7188:\n",
      "train loss: 2.8609917217983844e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7189:\n",
      "train loss: 2.1701475974686365e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7190:\n",
      "train loss: 2.292884315195408e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7191:\n",
      "train loss: 2.0406951286567485e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7192:\n",
      "train loss: 1.836359318131825e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7193:\n",
      "train loss: 2.1711310015553226e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7194:\n",
      "train loss: 2.1155645919889944e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7195:\n",
      "train loss: 2.856592119876287e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7196:\n",
      "train loss: 2.330407549668573e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7197:\n",
      "train loss: 2.387476533960066e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7198:\n",
      "train loss: 1.9146782893568197e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7199:\n",
      "train loss: 3.0031467126819434e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 7200:\n",
      "train loss: 2.2146862508833905e-15\n",
      "Epoch 07202: reducing learning rate of group 0 to 3.1660e-16.\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7201:\n",
      "train loss: 2.429644339888497e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7202:\n",
      "train loss: 2.1246462126034506e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7203:\n",
      "train loss: 2.2848227308775204e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7204:\n",
      "train loss: 2.1912699123891534e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7205:\n",
      "train loss: 3.128931895173171e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7206:\n",
      "train loss: 3.078964771199094e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7207:\n",
      "train loss: 1.8820186092913438e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7208:\n",
      "train loss: 1.958105885342823e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7209:\n",
      "train loss: 2.0526835648594027e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7210:\n",
      "train loss: 1.7435572933226442e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7211:\n",
      "train loss: 3.283902931159821e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7212:\n",
      "train loss: 3.210459270016438e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7213:\n",
      "train loss: 1.452191896588093e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7214:\n",
      "train loss: 2.140168160075658e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7215:\n",
      "train loss: 2.406044375334396e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7216:\n",
      "train loss: 1.5558245737262922e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7217:\n",
      "train loss: 3.128860320225064e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7218:\n",
      "train loss: 3.501450823354956e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7219:\n",
      "train loss: 2.1036041021885736e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7220:\n",
      "train loss: 1.5606400161744581e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 7221:\n",
      "train loss: 1.5732771463086795e-15\n",
      "Epoch 07223: reducing learning rate of group 0 to 3.0077e-16.\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7222:\n",
      "train loss: 1.780629350970519e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7223:\n",
      "train loss: 3.778627242645268e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7224:\n",
      "train loss: 3.0358524299262665e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7225:\n",
      "train loss: 2.1039382947766285e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7226:\n",
      "train loss: 2.0068491677873824e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7227:\n",
      "train loss: 1.8378175372139602e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7228:\n",
      "train loss: 1.5695278907203736e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7229:\n",
      "train loss: 2.526444126824166e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 7230:\n",
      "train loss: 1.6814633511406723e-15\n",
      "Epoch 07232: reducing learning rate of group 0 to 2.8573e-16.\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7231:\n",
      "train loss: 2.4009111643202977e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7232:\n",
      "train loss: 2.2217624596476442e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7233:\n",
      "train loss: 1.8225169477014156e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7234:\n",
      "train loss: 2.223826955956287e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7235:\n",
      "train loss: 2.325304773486792e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7236:\n",
      "train loss: 1.9147469511177046e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7237:\n",
      "train loss: 1.8949887426754374e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7238:\n",
      "train loss: 2.1266554264761873e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 7239:\n",
      "train loss: 2.0471050974448684e-15\n",
      "Epoch 07241: reducing learning rate of group 0 to 2.7144e-16.\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7240:\n",
      "train loss: 2.457618036610888e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7241:\n",
      "train loss: 2.881076726796212e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7242:\n",
      "train loss: 2.570672641455451e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7243:\n",
      "train loss: 2.2146134470208468e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7244:\n",
      "train loss: 2.249591195896667e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7245:\n",
      "train loss: 2.026232212634178e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7246:\n",
      "train loss: 1.821241772117424e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7247:\n",
      "train loss: 2.147336336504742e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 7248:\n",
      "train loss: 2.490076492520963e-15\n",
      "Epoch 07250: reducing learning rate of group 0 to 2.5787e-16.\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7249:\n",
      "train loss: 1.969269114921051e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7250:\n",
      "train loss: 2.427819775211066e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7251:\n",
      "train loss: 1.977586826088001e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7252:\n",
      "train loss: 2.4740090815290277e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7253:\n",
      "train loss: 1.9035347286715345e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7254:\n",
      "train loss: 2.114206183059548e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7255:\n",
      "train loss: 1.964791888955368e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7256:\n",
      "train loss: 2.121295773517722e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 7257:\n",
      "train loss: 2.745697385999562e-15\n",
      "Epoch 07259: reducing learning rate of group 0 to 2.4498e-16.\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7258:\n",
      "train loss: 1.9866458075842575e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7259:\n",
      "train loss: 2.7523908460613327e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7260:\n",
      "train loss: 2.620703196224049e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7261:\n",
      "train loss: 1.5858185454042625e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7262:\n",
      "train loss: 1.6611071069023973e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7263:\n",
      "train loss: 1.414207161257108e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7264:\n",
      "train loss: 1.8202134929699507e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7265:\n",
      "train loss: 2.2364733690831366e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7266:\n",
      "train loss: 1.986761303582185e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7267:\n",
      "train loss: 1.4830643266974128e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7268:\n",
      "train loss: 1.996628411521295e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7269:\n",
      "train loss: 2.132839539867658e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7270:\n",
      "train loss: 2.495150726390528e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 7271:\n",
      "train loss: 2.1717273302988674e-15\n",
      "Epoch 07273: reducing learning rate of group 0 to 2.3273e-16.\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7272:\n",
      "train loss: 1.485064740100696e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7273:\n",
      "train loss: 2.8299715683885096e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7274:\n",
      "train loss: 2.2787442452404117e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7275:\n",
      "train loss: 2.090720248068947e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7276:\n",
      "train loss: 2.1882778908021983e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7277:\n",
      "train loss: 2.51735751493652e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7278:\n",
      "train loss: 2.44300977539614e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7279:\n",
      "train loss: 2.1229121422681376e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 7280:\n",
      "train loss: 2.2155818222583594e-15\n",
      "Epoch 07282: reducing learning rate of group 0 to 2.2109e-16.\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7281:\n",
      "train loss: 2.176691660057405e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7282:\n",
      "train loss: 2.2745368064231097e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7283:\n",
      "train loss: 2.125995167442815e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7284:\n",
      "train loss: 2.64714380002748e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7285:\n",
      "train loss: 2.4397624393354707e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7286:\n",
      "train loss: 2.0035458997964825e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7287:\n",
      "train loss: 2.1605704467287578e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7288:\n",
      "train loss: 2.981468221640096e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7289:\n",
      "train loss: 1.3046341450167981e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7290:\n",
      "train loss: 1.706518248411163e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7291:\n",
      "train loss: 2.375624974564562e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7292:\n",
      "train loss: 1.6694707653849929e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7293:\n",
      "train loss: 2.087244065705659e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7294:\n",
      "train loss: 2.3476092829549807e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7295:\n",
      "train loss: 1.3019680557306048e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7296:\n",
      "train loss: 1.6484286095276065e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7297:\n",
      "train loss: 2.041819484777231e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7298:\n",
      "train loss: 1.2391819705847163e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7299:\n",
      "train loss: 2.174360911867911e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7300:\n",
      "train loss: 2.4548416224231633e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7301:\n",
      "train loss: 2.098836839073285e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7302:\n",
      "train loss: 2.0530221028235067e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7303:\n",
      "train loss: 2.093412360685736e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7304:\n",
      "train loss: 1.9385900832038077e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7305:\n",
      "train loss: 1.944316258881052e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 7306:\n",
      "train loss: 1.2972032348346217e-15\n",
      "Epoch 07308: reducing learning rate of group 0 to 2.1004e-16.\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7307:\n",
      "train loss: 1.5299898266462818e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7308:\n",
      "train loss: 1.8272920608376293e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7309:\n",
      "train loss: 1.2427772441848422e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7310:\n",
      "train loss: 1.2467126502146842e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7311:\n",
      "train loss: 2.5358010663338014e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7312:\n",
      "train loss: 2.043226163212554e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7313:\n",
      "train loss: 1.1746011574250736e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7314:\n",
      "train loss: 1.403676406550488e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7315:\n",
      "train loss: 1.1824881907648312e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7316:\n",
      "train loss: 1.2294537127926646e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7317:\n",
      "train loss: 2.048549697446172e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7318:\n",
      "train loss: 2.507781514388954e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7319:\n",
      "train loss: 1.7850574302703295e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7320:\n",
      "train loss: 1.7316700558197937e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 7321:\n",
      "train loss: 2.0962480477375904e-15\n",
      "Epoch 07323: reducing learning rate of group 0 to 1.9953e-16.\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7322:\n",
      "train loss: 1.2239303889993801e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7323:\n",
      "train loss: 2.836296208012968e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7324:\n",
      "train loss: 2.2478582878557965e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7325:\n",
      "train loss: 1.6619785993777352e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7326:\n",
      "train loss: 1.4980628169375388e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7327:\n",
      "train loss: 7.904194109077166e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7328:\n",
      "train loss: 2.426110846477026e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7329:\n",
      "train loss: 2.1359876507597013e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7330:\n",
      "train loss: 2.574576036667611e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7331:\n",
      "train loss: 2.4661883098707466e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7332:\n",
      "train loss: 9.289908658855595e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7333:\n",
      "train loss: 2.8814761448401105e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7334:\n",
      "train loss: 3.675127497523299e-15\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 7335:\n",
      "train loss: 1.956540059064123e-15\n",
      "Epoch 07337: reducing learning rate of group 0 to 1.8956e-16.\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7336:\n",
      "train loss: 2.1670600802099354e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7337:\n",
      "train loss: 1.4953287747109754e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7338:\n",
      "train loss: 1.5651549710713728e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7339:\n",
      "train loss: 1.4596162655888438e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7340:\n",
      "train loss: 2.124822065519611e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7341:\n",
      "train loss: 2.2854972991241347e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7342:\n",
      "train loss: 1.6992013271474583e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7343:\n",
      "train loss: 2.226322803796309e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 7344:\n",
      "train loss: 1.6591031961512254e-15\n",
      "Epoch 07346: reducing learning rate of group 0 to 1.8008e-16.\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7345:\n",
      "train loss: 1.3315158374175224e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7346:\n",
      "train loss: 1.4121359128552231e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7347:\n",
      "train loss: 9.887245046410705e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7348:\n",
      "train loss: 1.6392782542706867e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7349:\n",
      "train loss: 1.3843315873143858e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7350:\n",
      "train loss: 1.4917802092037898e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7351:\n",
      "train loss: 1.5192322475110516e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7352:\n",
      "train loss: 1.243741175634551e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 7353:\n",
      "train loss: 1.863730925090245e-15\n",
      "Epoch 07355: reducing learning rate of group 0 to 1.7108e-16.\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7354:\n",
      "train loss: 1.2067696681735966e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7355:\n",
      "train loss: 2.908476178465032e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7356:\n",
      "train loss: 2.328836332056683e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7357:\n",
      "train loss: 1.0360079705476576e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7358:\n",
      "train loss: 1.2505795840071968e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7359:\n",
      "train loss: 1.1266320759463315e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7360:\n",
      "train loss: 1.0626529360614513e-15\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7361:\n",
      "train loss: 9.94370244533718e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 7362:\n",
      "train loss: 1.3392645524567067e-15\n",
      "Epoch 07364: reducing learning rate of group 0 to 1.6252e-16.\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7363:\n",
      "train loss: 1.1976708044580325e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7364:\n",
      "train loss: 3.111012616984314e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7365:\n",
      "train loss: 3.1235959602445386e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7366:\n",
      "train loss: 1.4056173356551017e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7367:\n",
      "train loss: 1.2952259498855328e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7368:\n",
      "train loss: 1.7590065389501208e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7369:\n",
      "train loss: 1.397786806981062e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7370:\n",
      "train loss: 1.7533923184550456e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 7371:\n",
      "train loss: 1.0539155746383451e-15\n",
      "Epoch 07373: reducing learning rate of group 0 to 1.5440e-16.\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7372:\n",
      "train loss: 1.061346827387859e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7373:\n",
      "train loss: 1.5813303706617253e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7374:\n",
      "train loss: 2.043145798319095e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7375:\n",
      "train loss: 1.7405639989868027e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7376:\n",
      "train loss: 8.486950508534425e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7377:\n",
      "train loss: 2.7529452917071904e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7378:\n",
      "train loss: 2.611599970119975e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7379:\n",
      "train loss: 1.1304694071639728e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 7380:\n",
      "train loss: 1.6332308362001327e-15\n",
      "Epoch 07382: reducing learning rate of group 0 to 1.4668e-16.\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7381:\n",
      "train loss: 1.5870631843200563e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7382:\n",
      "train loss: 1.1549859869867902e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7383:\n",
      "train loss: 1.6428264360445571e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7384:\n",
      "train loss: 1.4957182211057307e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7385:\n",
      "train loss: 1.5358194643963751e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7386:\n",
      "train loss: 1.6683094133247347e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7387:\n",
      "train loss: 1.2337685228718728e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7388:\n",
      "train loss: 2.6260439574120764e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 7389:\n",
      "train loss: 3.358138522534383e-15\n",
      "Epoch 07391: reducing learning rate of group 0 to 1.3934e-16.\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7390:\n",
      "train loss: 1.9702179149027763e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7391:\n",
      "train loss: 2.2396193815895196e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7392:\n",
      "train loss: 2.763891384387982e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7393:\n",
      "train loss: 2.1966864975551694e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7394:\n",
      "train loss: 2.000190834752009e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7395:\n",
      "train loss: 1.9272609794593478e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7396:\n",
      "train loss: 1.0931120166212904e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7397:\n",
      "train loss: 2.260571330820395e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7398:\n",
      "train loss: 1.660876195208253e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7399:\n",
      "train loss: 7.202598865934392e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7400:\n",
      "train loss: 2.9008821854871057e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7401:\n",
      "train loss: 2.6091858050570126e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7402:\n",
      "train loss: 2.2652366187017735e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7403:\n",
      "train loss: 2.2677235009214643e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7404:\n",
      "train loss: 2.805197152621359e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7405:\n",
      "train loss: 1.785309592354718e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7406:\n",
      "train loss: 1.9768122563854053e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 7407:\n",
      "train loss: 2.2302632819067426e-15\n",
      "Epoch 07409: reducing learning rate of group 0 to 1.3238e-16.\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7408:\n",
      "train loss: 1.5825978019898347e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7409:\n",
      "train loss: 2.42099755870288e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7410:\n",
      "train loss: 2.905463954799547e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7411:\n",
      "train loss: 1.6543012952232461e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7412:\n",
      "train loss: 2.0329179082876637e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7413:\n",
      "train loss: 2.2489718890072852e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7414:\n",
      "train loss: 1.4516089798283262e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7415:\n",
      "train loss: 2.169352571283085e-15\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 7416:\n",
      "train loss: 2.3121395050830018e-15\n",
      "Epoch 07418: reducing learning rate of group 0 to 1.2576e-16.\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7417:\n",
      "train loss: 1.2215118741823098e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7418:\n",
      "train loss: 2.487299938373548e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7419:\n",
      "train loss: 3.0299260510790058e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7420:\n",
      "train loss: 1.6414557060373663e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7421:\n",
      "train loss: 1.7482664473029683e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7422:\n",
      "train loss: 1.914599238172858e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7423:\n",
      "train loss: 1.4854676203071176e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7424:\n",
      "train loss: 1.5908975203724245e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 7425:\n",
      "train loss: 1.5064867436828008e-15\n",
      "Epoch 07427: reducing learning rate of group 0 to 1.1947e-16.\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7426:\n",
      "train loss: 1.076110442622691e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7427:\n",
      "train loss: 1.7417646191608901e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7428:\n",
      "train loss: 2.0486400326299026e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7429:\n",
      "train loss: 1.2448434555855885e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7430:\n",
      "train loss: 1.0496701107961154e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7431:\n",
      "train loss: 1.931825711805917e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7432:\n",
      "train loss: 1.072147758525404e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7433:\n",
      "train loss: 1.1151075586687524e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 7434:\n",
      "train loss: 1.362174745026057e-15\n",
      "Epoch 07436: reducing learning rate of group 0 to 1.1350e-16.\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7435:\n",
      "train loss: 8.860760729653664e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7436:\n",
      "train loss: 1.2847475891588959e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7437:\n",
      "train loss: 1.3358350399984047e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7438:\n",
      "train loss: 1.2515823952601932e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7439:\n",
      "train loss: 1.6052876268254442e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7440:\n",
      "train loss: 1.5867390127734507e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7441:\n",
      "train loss: 1.877100554010767e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7442:\n",
      "train loss: 1.356435890209559e-15\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 7443:\n",
      "train loss: 1.6464446958999447e-15\n",
      "Epoch 07445: reducing learning rate of group 0 to 1.0782e-16.\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7444:\n",
      "train loss: 1.9989251129879582e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7445:\n",
      "train loss: 1.4447450982601138e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7446:\n",
      "train loss: 2.0084069744085034e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7447:\n",
      "train loss: 2.3274203987967794e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7448:\n",
      "train loss: 2.0073952078416288e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7449:\n",
      "train loss: 1.5010878573032333e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7450:\n",
      "train loss: 2.383623341277151e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7451:\n",
      "train loss: 1.5889878070888408e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7452:\n",
      "train loss: 1.4664943021109061e-15\n",
      "Epoch 07454: reducing learning rate of group 0 to 1.0243e-16.\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7453:\n",
      "train loss: 1.2891269453496378e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7454:\n",
      "train loss: 8.4504029380136635e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7455:\n",
      "train loss: 8.462095822476194e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7456:\n",
      "train loss: 1.5672013230425216e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7457:\n",
      "train loss: 1.365281143797489e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7458:\n",
      "train loss: 1.4232632077521738e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7459:\n",
      "train loss: 1.1501670485756857e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7460:\n",
      "train loss: 1.0044428028828643e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7461:\n",
      "train loss: 1.4528533047484503e-15\n",
      "Epoch 07463: reducing learning rate of group 0 to 9.7308e-17.\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7462:\n",
      "train loss: 1.3353720178657044e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7463:\n",
      "train loss: 9.037374899058684e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7464:\n",
      "train loss: 1.4597320291454203e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7465:\n",
      "train loss: 1.3935579063283302e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7466:\n",
      "train loss: 9.256919482204478e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7467:\n",
      "train loss: 6.764917073465739e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7468:\n",
      "train loss: 1.0629708015070869e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7469:\n",
      "train loss: 1.2574686651090333e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7470:\n",
      "train loss: 9.87002851474207e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7471:\n",
      "train loss: 2.489545166285639e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7472:\n",
      "train loss: 2.320984760732006e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7473:\n",
      "train loss: 1.665347675455195e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7474:\n",
      "train loss: 2.219954884078873e-15\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7475:\n",
      "train loss: 2.110693851458895e-15\n",
      "Epoch 07477: reducing learning rate of group 0 to 9.2442e-17.\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7476:\n",
      "train loss: 1.921855247344218e-15\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7477:\n",
      "train loss: 1.0539207506204512e-15\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7478:\n",
      "train loss: 1.2314390295110817e-15\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7479:\n",
      "train loss: 1.219181968811725e-15\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7480:\n",
      "train loss: 7.840949339282674e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7481:\n",
      "train loss: 5.971433094024202e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7482:\n",
      "train loss: 7.672671769318978e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7483:\n",
      "train loss: 8.421190209264804e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7484:\n",
      "train loss: 6.940113225013321e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7485:\n",
      "train loss: 1.0203341990008193e-15\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7486:\n",
      "train loss: 6.770894923605373e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7487:\n",
      "train loss: 6.453663012042895e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7488:\n",
      "train loss: 7.977060083803895e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7489:\n",
      "train loss: 8.379974223660771e-16\n",
      "Epoch 07491: reducing learning rate of group 0 to 8.7820e-17.\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7490:\n",
      "train loss: 9.68405492420869e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7491:\n",
      "train loss: 8.711334891741024e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7492:\n",
      "train loss: 8.803285476863593e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7493:\n",
      "train loss: 1.076474596097951e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7494:\n",
      "train loss: 1.2852269690540818e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7495:\n",
      "train loss: 1.3396221180026236e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7496:\n",
      "train loss: 1.181358030377367e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7497:\n",
      "train loss: 1.1940551167245066e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7498:\n",
      "train loss: 1.3594887703216888e-15\n",
      "Epoch 07500: reducing learning rate of group 0 to 8.3429e-17.\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7499:\n",
      "train loss: 9.017372636450105e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7500:\n",
      "train loss: 6.602697686083074e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7501:\n",
      "train loss: 1.0177638380869603e-15\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7502:\n",
      "train loss: 8.449718936361811e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7503:\n",
      "train loss: 8.81306833987831e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7504:\n",
      "train loss: 1.2299946682557826e-15\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7505:\n",
      "train loss: 9.617737632264004e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7506:\n",
      "train loss: 7.753387201425691e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7507:\n",
      "train loss: 6.520038018237825e-16\n",
      "Epoch 07509: reducing learning rate of group 0 to 7.9258e-17.\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7508:\n",
      "train loss: 1.2625230702383493e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7509:\n",
      "train loss: 9.534828557851826e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7510:\n",
      "train loss: 6.023136577174521e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7511:\n",
      "train loss: 1.6591201728255832e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7512:\n",
      "train loss: 1.6342398399919875e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7513:\n",
      "train loss: 9.936719437969754e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7514:\n",
      "train loss: 1.3545622195067886e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7515:\n",
      "train loss: 1.115568396108134e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7516:\n",
      "train loss: 8.407081056957916e-16\n",
      "Epoch 07518: reducing learning rate of group 0 to 7.5295e-17.\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7517:\n",
      "train loss: 8.7996017174055865e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7518:\n",
      "train loss: 1.2648668087867163e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7519:\n",
      "train loss: 1.597409228336409e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7520:\n",
      "train loss: 2.0740915462606797e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7521:\n",
      "train loss: 8.298711135757607e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7522:\n",
      "train loss: 9.151681986097811e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7523:\n",
      "train loss: 2.1176285361221337e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7524:\n",
      "train loss: 1.945589272533025e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7525:\n",
      "train loss: 8.933998344154648e-16\n",
      "Epoch 07527: reducing learning rate of group 0 to 7.1530e-17.\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7526:\n",
      "train loss: 8.065146124477153e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7527:\n",
      "train loss: 1.917807956620809e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7528:\n",
      "train loss: 2.8279200862927707e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7529:\n",
      "train loss: 2.5839098703454018e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7530:\n",
      "train loss: 1.1451985110895721e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7531:\n",
      "train loss: 1.0604829023173821e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7532:\n",
      "train loss: 1.7166110451643567e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7533:\n",
      "train loss: 2.1190484669356413e-15\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7534:\n",
      "train loss: 1.0911762744788442e-15\n",
      "Epoch 07536: reducing learning rate of group 0 to 6.7954e-17.\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7535:\n",
      "train loss: 1.0515942259987681e-15\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7536:\n",
      "train loss: 1.1981683992629773e-15\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7537:\n",
      "train loss: 7.841534642348147e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7538:\n",
      "train loss: 8.820017278422597e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7539:\n",
      "train loss: 9.194131094080943e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7540:\n",
      "train loss: 1.428306829642426e-15\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7541:\n",
      "train loss: 1.5747454205667252e-15\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7542:\n",
      "train loss: 1.3936453042823897e-15\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7543:\n",
      "train loss: 9.027780548190333e-16\n",
      "Epoch 07545: reducing learning rate of group 0 to 6.4556e-17.\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7544:\n",
      "train loss: 8.820347198653274e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7545:\n",
      "train loss: 1.128316239560937e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7546:\n",
      "train loss: 1.259311682286079e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7547:\n",
      "train loss: 1.1690579184999872e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7548:\n",
      "train loss: 1.3151819109746393e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7549:\n",
      "train loss: 1.4419757721175795e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7550:\n",
      "train loss: 1.2477882043791174e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7551:\n",
      "train loss: 7.692098885157183e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7552:\n",
      "train loss: 8.751310245430998e-16\n",
      "Epoch 07554: reducing learning rate of group 0 to 6.1328e-17.\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7553:\n",
      "train loss: 6.252612658104449e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7554:\n",
      "train loss: 9.510627563776523e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7555:\n",
      "train loss: 9.134770967799612e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7556:\n",
      "train loss: 9.88225438497475e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7557:\n",
      "train loss: 8.284513747670056e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7558:\n",
      "train loss: 1.4803252104417542e-15\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7559:\n",
      "train loss: 1.0929884364404806e-15\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7560:\n",
      "train loss: 9.638302993650225e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7561:\n",
      "train loss: 1.144689610865788e-15\n",
      "Epoch 07563: reducing learning rate of group 0 to 5.8262e-17.\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7562:\n",
      "train loss: 8.737568447795787e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7563:\n",
      "train loss: 7.109713413665136e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7564:\n",
      "train loss: 1.1490995750158501e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7565:\n",
      "train loss: 1.012160548471744e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7566:\n",
      "train loss: 9.563283865436556e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7567:\n",
      "train loss: 5.622346048489055e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7568:\n",
      "train loss: 6.671577554603524e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7569:\n",
      "train loss: 1.00860820075061e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7570:\n",
      "train loss: 1.0895857152827475e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7571:\n",
      "train loss: 6.920652213511181e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7572:\n",
      "train loss: 6.964560570513261e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7573:\n",
      "train loss: 1.5675195145467939e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7574:\n",
      "train loss: 1.2886333790751927e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7575:\n",
      "train loss: 1.185146719579328e-15\n",
      "Epoch 07577: reducing learning rate of group 0 to 5.5349e-17.\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7576:\n",
      "train loss: 8.306783751283999e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7577:\n",
      "train loss: 8.289231484133122e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7578:\n",
      "train loss: 9.286754140439635e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7579:\n",
      "train loss: 1.2864647699199775e-15\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7580:\n",
      "train loss: 8.325108756477647e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7581:\n",
      "train loss: 9.690102926905057e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7582:\n",
      "train loss: 6.635149203914777e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7583:\n",
      "train loss: 1.2533324613717589e-15\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7584:\n",
      "train loss: 7.399025119897896e-16\n",
      "Epoch 07586: reducing learning rate of group 0 to 5.2581e-17.\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7585:\n",
      "train loss: 8.934972277117109e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7586:\n",
      "train loss: 6.393288135002425e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7587:\n",
      "train loss: 4.359797925676749e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7588:\n",
      "train loss: 9.595088983909485e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7589:\n",
      "train loss: 7.840781462073193e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7590:\n",
      "train loss: 6.758280541343272e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7591:\n",
      "train loss: 7.107943067734532e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7592:\n",
      "train loss: 1.1725819840264696e-15\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7593:\n",
      "train loss: 8.306328871329623e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7594:\n",
      "train loss: 9.027131569697915e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7595:\n",
      "train loss: 7.028689744508947e-16\n",
      "Epoch 07597: reducing learning rate of group 0 to 4.9952e-17.\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7596:\n",
      "train loss: 5.651827733220299e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7597:\n",
      "train loss: 7.010056141172394e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7598:\n",
      "train loss: 6.776594503969828e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7599:\n",
      "train loss: 7.848861560983816e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7600:\n",
      "train loss: 6.045757627221168e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7601:\n",
      "train loss: 5.891116869782148e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7602:\n",
      "train loss: 5.719183513400764e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7603:\n",
      "train loss: 8.655592533103322e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7604:\n",
      "train loss: 8.123588978162613e-16\n",
      "Epoch 07606: reducing learning rate of group 0 to 4.7455e-17.\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7605:\n",
      "train loss: 5.777026651843186e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7606:\n",
      "train loss: 6.141567164863175e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7607:\n",
      "train loss: 6.2473184072634e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7608:\n",
      "train loss: 1.0927913421508416e-15\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7609:\n",
      "train loss: 7.517401176818338e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7610:\n",
      "train loss: 7.070454019220771e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7611:\n",
      "train loss: 5.574595256708025e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7612:\n",
      "train loss: 9.105989800544304e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7613:\n",
      "train loss: 4.38962943432065e-16\n",
      "Epoch 07615: reducing learning rate of group 0 to 4.5082e-17.\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7614:\n",
      "train loss: 7.530335783918766e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7615:\n",
      "train loss: 7.057606834157526e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7616:\n",
      "train loss: 6.94009692257142e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7617:\n",
      "train loss: 4.610384353418183e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7618:\n",
      "train loss: 4.921412455037028e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7619:\n",
      "train loss: 7.0088891537099195e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7620:\n",
      "train loss: 5.131740594079182e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7621:\n",
      "train loss: 5.597006571908928e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7622:\n",
      "train loss: 3.972110722166855e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7623:\n",
      "train loss: 6.732783812811127e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7624:\n",
      "train loss: 6.804202904404086e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7625:\n",
      "train loss: 5.52037613645348e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7626:\n",
      "train loss: 5.743639399227441e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7627:\n",
      "train loss: 5.569994341099939e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7628:\n",
      "train loss: 5.439853451535066e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7629:\n",
      "train loss: 5.361324013854436e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7630:\n",
      "train loss: 8.036914347124793e-16\n",
      "Epoch 07632: reducing learning rate of group 0 to 4.2828e-17.\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7631:\n",
      "train loss: 6.206467163367996e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7632:\n",
      "train loss: 6.16344731675176e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7633:\n",
      "train loss: 5.72633550985981e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7634:\n",
      "train loss: 6.842983930429976e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7635:\n",
      "train loss: 5.027412032497977e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7636:\n",
      "train loss: 5.758566977663473e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7637:\n",
      "train loss: 6.287580000268281e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7638:\n",
      "train loss: 1.210245786950555e-15\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7639:\n",
      "train loss: 8.410917178417045e-16\n",
      "Epoch 07641: reducing learning rate of group 0 to 4.0686e-17.\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7640:\n",
      "train loss: 7.762101083516534e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7641:\n",
      "train loss: 7.256192760943607e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7642:\n",
      "train loss: 8.664679225591356e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7643:\n",
      "train loss: 6.634918121878833e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7644:\n",
      "train loss: 5.107400321331714e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7645:\n",
      "train loss: 4.3509143294846553e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7646:\n",
      "train loss: 4.086022157854698e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7647:\n",
      "train loss: 4.990500297666258e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7648:\n",
      "train loss: 7.201431563175413e-16\n",
      "Epoch 07650: reducing learning rate of group 0 to 3.8652e-17.\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7649:\n",
      "train loss: 7.424191196645047e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7650:\n",
      "train loss: 7.814459786529739e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7651:\n",
      "train loss: 5.831851832540746e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7652:\n",
      "train loss: 6.470851362468294e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7653:\n",
      "train loss: 4.1907617237036663e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7654:\n",
      "train loss: 9.730487225259722e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7655:\n",
      "train loss: 1.15786656528112e-15\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7656:\n",
      "train loss: 7.149213677859764e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7657:\n",
      "train loss: 6.469360614493906e-16\n",
      "Epoch 07659: reducing learning rate of group 0 to 3.6719e-17.\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7658:\n",
      "train loss: 8.862169585405507e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7659:\n",
      "train loss: 5.889160282018568e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7660:\n",
      "train loss: 6.941297685423714e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7661:\n",
      "train loss: 6.394166213106686e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7662:\n",
      "train loss: 6.433445553818567e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7663:\n",
      "train loss: 7.201693442094206e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7664:\n",
      "train loss: 1.6597870149168314e-15\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7665:\n",
      "train loss: 9.427743795262472e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7666:\n",
      "train loss: 8.60870861903452e-16\n",
      "Epoch 07668: reducing learning rate of group 0 to 3.4883e-17.\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7667:\n",
      "train loss: 6.372115842120891e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7668:\n",
      "train loss: 6.416584819355335e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7669:\n",
      "train loss: 4.545118571688682e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7670:\n",
      "train loss: 7.754326154125125e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7671:\n",
      "train loss: 6.705501262594572e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7672:\n",
      "train loss: 5.208595556461917e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7673:\n",
      "train loss: 8.598035158765456e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7674:\n",
      "train loss: 9.477946598871453e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7675:\n",
      "train loss: 7.769072660980612e-16\n",
      "Epoch 07677: reducing learning rate of group 0 to 3.3139e-17.\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7676:\n",
      "train loss: 5.531133528363974e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7677:\n",
      "train loss: 5.833398655651972e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7678:\n",
      "train loss: 8.655497677576859e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7679:\n",
      "train loss: 5.392422125469048e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7680:\n",
      "train loss: 5.826620654465557e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7681:\n",
      "train loss: 4.59996405854956e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7682:\n",
      "train loss: 4.603444331337613e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7683:\n",
      "train loss: 5.49117999318872e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7684:\n",
      "train loss: 7.862675622783841e-16\n",
      "Epoch 07686: reducing learning rate of group 0 to 3.1482e-17.\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7685:\n",
      "train loss: 5.863410412268569e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7686:\n",
      "train loss: 4.4963889788155195e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7687:\n",
      "train loss: 4.685995825790961e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7688:\n",
      "train loss: 4.99828669564934e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7689:\n",
      "train loss: 5.615502845239714e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7690:\n",
      "train loss: 5.23872486895096e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7691:\n",
      "train loss: 6.020732832515867e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7692:\n",
      "train loss: 7.929416661823723e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7693:\n",
      "train loss: 4.543151317120975e-16\n",
      "Epoch 07695: reducing learning rate of group 0 to 2.9908e-17.\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7694:\n",
      "train loss: 5.040098193333279e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7695:\n",
      "train loss: 5.766868434816436e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7696:\n",
      "train loss: 4.818903458187878e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7697:\n",
      "train loss: 6.702848797797069e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7698:\n",
      "train loss: 5.027739229732932e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7699:\n",
      "train loss: 6.034343638603783e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7700:\n",
      "train loss: 5.212568451525686e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7701:\n",
      "train loss: 5.306041417253205e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7702:\n",
      "train loss: 5.676685372211497e-16\n",
      "Epoch 07704: reducing learning rate of group 0 to 2.8413e-17.\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7703:\n",
      "train loss: 4.1450516597558974e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7704:\n",
      "train loss: 4.983184842099851e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7705:\n",
      "train loss: 7.525639319384236e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7706:\n",
      "train loss: 5.101617737829246e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7707:\n",
      "train loss: 9.183018850444178e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7708:\n",
      "train loss: 7.15407542983834e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7709:\n",
      "train loss: 7.766710811047345e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7710:\n",
      "train loss: 4.2403452295859835e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7711:\n",
      "train loss: 4.3369252205508234e-16\n",
      "Epoch 07713: reducing learning rate of group 0 to 2.6992e-17.\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7712:\n",
      "train loss: 4.857845859351938e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7713:\n",
      "train loss: 4.175792745702236e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7714:\n",
      "train loss: 7.229445913974961e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7715:\n",
      "train loss: 7.613119167330702e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7716:\n",
      "train loss: 4.0494193881998486e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7717:\n",
      "train loss: 4.622082406118486e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7718:\n",
      "train loss: 5.152382746001118e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7719:\n",
      "train loss: 6.885593181248729e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7720:\n",
      "train loss: 1.330869479976585e-15\n",
      "Epoch 07722: reducing learning rate of group 0 to 2.5643e-17.\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7721:\n",
      "train loss: 1.0821198789177658e-15\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7722:\n",
      "train loss: 5.357284306445684e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7723:\n",
      "train loss: 6.162381368780719e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7724:\n",
      "train loss: 5.298724961278838e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7725:\n",
      "train loss: 6.176295235484408e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7726:\n",
      "train loss: 5.249874344134422e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7727:\n",
      "train loss: 3.9815577699592455e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7728:\n",
      "train loss: 3.655771212915784e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7729:\n",
      "train loss: 3.956346856984574e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7730:\n",
      "train loss: 3.8943038921293837e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7731:\n",
      "train loss: 3.4233114549568413e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7732:\n",
      "train loss: 3.312839558490456e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7733:\n",
      "train loss: 3.5578161420954593e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7734:\n",
      "train loss: 6.524189190917604e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7735:\n",
      "train loss: 7.273961422865112e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7736:\n",
      "train loss: 4.876047084134388e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7737:\n",
      "train loss: 3.3359219427557854e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7738:\n",
      "train loss: 3.0158728720413597e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7739:\n",
      "train loss: 5.037523939522078e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7740:\n",
      "train loss: 6.116283623349423e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7741:\n",
      "train loss: 7.115163421893026e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7742:\n",
      "train loss: 6.365775093181638e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7743:\n",
      "train loss: 4.1546503187469755e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7744:\n",
      "train loss: 5.012419487727502e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7745:\n",
      "train loss: 3.9223894067630853e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7746:\n",
      "train loss: 6.907297887161062e-16\n",
      "Epoch 07748: reducing learning rate of group 0 to 2.4360e-17.\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7747:\n",
      "train loss: 7.00135352693665e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7748:\n",
      "train loss: 5.096929490166267e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7749:\n",
      "train loss: 6.183126224884416e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7750:\n",
      "train loss: 8.282829775774009e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7751:\n",
      "train loss: 5.036369039174444e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7752:\n",
      "train loss: 6.151344692946635e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7753:\n",
      "train loss: 5.121171565454716e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7754:\n",
      "train loss: 4.599031352593072e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7755:\n",
      "train loss: 6.929554768654189e-16\n",
      "Epoch 07757: reducing learning rate of group 0 to 2.3142e-17.\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7756:\n",
      "train loss: 5.416011118790027e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7757:\n",
      "train loss: 4.631467201434708e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7758:\n",
      "train loss: 4.14614514775669e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7759:\n",
      "train loss: 3.378686245560027e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7760:\n",
      "train loss: 5.097258637712533e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7761:\n",
      "train loss: 5.496333388037055e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7762:\n",
      "train loss: 7.002335224721169e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7763:\n",
      "train loss: 1.0130330523559945e-15\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7764:\n",
      "train loss: 8.678565345836704e-16\n",
      "Epoch 07766: reducing learning rate of group 0 to 2.1985e-17.\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7765:\n",
      "train loss: 8.489479940436942e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7766:\n",
      "train loss: 5.410006930103303e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7767:\n",
      "train loss: 9.042416750779416e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7768:\n",
      "train loss: 1.1022316350488343e-15\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7769:\n",
      "train loss: 9.630553122467782e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7770:\n",
      "train loss: 9.368262156811516e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7771:\n",
      "train loss: 1.2454451092798466e-15\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7772:\n",
      "train loss: 7.221986498839467e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7773:\n",
      "train loss: 4.0492280626200697e-16\n",
      "Epoch 07775: reducing learning rate of group 0 to 2.0886e-17.\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7774:\n",
      "train loss: 3.4079773294811076e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7775:\n",
      "train loss: 6.992815259131669e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7776:\n",
      "train loss: 8.892661172399686e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7777:\n",
      "train loss: 1.0384129499981983e-15\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7778:\n",
      "train loss: 7.939420147500414e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7779:\n",
      "train loss: 3.8766758342026283e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7780:\n",
      "train loss: 4.061702961249192e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7781:\n",
      "train loss: 4.838135472872684e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7782:\n",
      "train loss: 3.839975784823143e-16\n",
      "Epoch 07784: reducing learning rate of group 0 to 1.9842e-17.\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7783:\n",
      "train loss: 7.622617061388402e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7784:\n",
      "train loss: 8.057331055716765e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7785:\n",
      "train loss: 8.477632395830436e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7786:\n",
      "train loss: 8.065952595862633e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7787:\n",
      "train loss: 4.686346353207838e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7788:\n",
      "train loss: 4.512967315943308e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7789:\n",
      "train loss: 4.22295386578737e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7790:\n",
      "train loss: 5.913898630338336e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7791:\n",
      "train loss: 6.775775772208972e-16\n",
      "Epoch 07793: reducing learning rate of group 0 to 1.8850e-17.\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7792:\n",
      "train loss: 6.268025731959886e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7793:\n",
      "train loss: 5.7088757325667305e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7794:\n",
      "train loss: 5.674896162365327e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7795:\n",
      "train loss: 4.880095804845128e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7796:\n",
      "train loss: 3.7484474424484187e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7797:\n",
      "train loss: 4.1576926719040427e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7798:\n",
      "train loss: 7.98215426464663e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7799:\n",
      "train loss: 5.261890106896584e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7800:\n",
      "train loss: 5.694866030529912e-16\n",
      "Epoch 07802: reducing learning rate of group 0 to 1.7907e-17.\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7801:\n",
      "train loss: 6.5052217517965545e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7802:\n",
      "train loss: 5.040756405556926e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7803:\n",
      "train loss: 5.160961317378568e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7804:\n",
      "train loss: 4.0732980831499556e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7805:\n",
      "train loss: 6.076865297848847e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7806:\n",
      "train loss: 5.048179878768538e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7807:\n",
      "train loss: 5.014828738565995e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7808:\n",
      "train loss: 5.014595077624306e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7809:\n",
      "train loss: 5.46670212820786e-16\n",
      "Epoch 07811: reducing learning rate of group 0 to 1.7012e-17.\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7810:\n",
      "train loss: 5.537573708433094e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7811:\n",
      "train loss: 3.894070792022257e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7812:\n",
      "train loss: 4.505891505429267e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7813:\n",
      "train loss: 4.175428469372613e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7814:\n",
      "train loss: 1.0043720942401104e-15\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7815:\n",
      "train loss: 6.349007837765072e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7816:\n",
      "train loss: 4.963779586760683e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7817:\n",
      "train loss: 4.928984861540656e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7818:\n",
      "train loss: 4.1882169881116616e-16\n",
      "Epoch 07820: reducing learning rate of group 0 to 1.6161e-17.\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7819:\n",
      "train loss: 3.3554110727903786e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7820:\n",
      "train loss: 5.148219259513535e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7821:\n",
      "train loss: 6.468304541631809e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7822:\n",
      "train loss: 3.2706838523885417e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7823:\n",
      "train loss: 3.572702525547306e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7824:\n",
      "train loss: 5.106436818724586e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7825:\n",
      "train loss: 3.8744238475402893e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7826:\n",
      "train loss: 3.6445151041847287e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7827:\n",
      "train loss: 3.7058874180199255e-16\n",
      "Epoch 07829: reducing learning rate of group 0 to 1.5353e-17.\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7828:\n",
      "train loss: 4.377142315334723e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7829:\n",
      "train loss: 3.956733037067859e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7830:\n",
      "train loss: 4.512760719432786e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7831:\n",
      "train loss: 4.183095689317069e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7832:\n",
      "train loss: 4.082882809221097e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7833:\n",
      "train loss: 5.263509009949913e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7834:\n",
      "train loss: 3.3733410803173396e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7835:\n",
      "train loss: 3.6460057434237793e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7836:\n",
      "train loss: 4.620438371550546e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7837:\n",
      "train loss: 2.9613389442771867e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7838:\n",
      "train loss: 4.589461463249314e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7839:\n",
      "train loss: 4.377788776567947e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7840:\n",
      "train loss: 3.700515013262729e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7841:\n",
      "train loss: 5.152542537950237e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7842:\n",
      "train loss: 4.3327088742216726e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7843:\n",
      "train loss: 3.6038856705199705e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7844:\n",
      "train loss: 3.54385371325915e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7845:\n",
      "train loss: 5.079259506009004e-16\n",
      "Epoch 07847: reducing learning rate of group 0 to 1.4585e-17.\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7846:\n",
      "train loss: 5.226822086610575e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7847:\n",
      "train loss: 5.350132010058966e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7848:\n",
      "train loss: 5.608400439761205e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7849:\n",
      "train loss: 5.054198395214843e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7850:\n",
      "train loss: 3.8132363689761905e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7851:\n",
      "train loss: 7.255940251562741e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7852:\n",
      "train loss: 4.741800274459871e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7853:\n",
      "train loss: 7.749835717408037e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7854:\n",
      "train loss: 5.273461575417763e-16\n",
      "Epoch 07856: reducing learning rate of group 0 to 1.3856e-17.\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7855:\n",
      "train loss: 3.6296446163490015e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7856:\n",
      "train loss: 3.676210164129196e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7857:\n",
      "train loss: 5.019087899380777e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7858:\n",
      "train loss: 6.533349956069121e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7859:\n",
      "train loss: 5.841462785022205e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7860:\n",
      "train loss: 6.453504215608822e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7861:\n",
      "train loss: 3.95398096311315e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7862:\n",
      "train loss: 3.1476407542561197e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7863:\n",
      "train loss: 3.6940641369079025e-16\n",
      "Epoch 07865: reducing learning rate of group 0 to 1.3163e-17.\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7864:\n",
      "train loss: 3.0199393498307842e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7865:\n",
      "train loss: 2.899734317926593e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7866:\n",
      "train loss: 3.007684466012591e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7867:\n",
      "train loss: 3.281413322870371e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7868:\n",
      "train loss: 3.9898617594050467e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7869:\n",
      "train loss: 3.61187238771894e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7870:\n",
      "train loss: 3.290787518021392e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7871:\n",
      "train loss: 6.002652621895571e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7872:\n",
      "train loss: 3.685658307482895e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7873:\n",
      "train loss: 4.61247504231323e-16\n",
      "Epoch 07875: reducing learning rate of group 0 to 1.2505e-17.\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7874:\n",
      "train loss: 3.2987138384176934e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7875:\n",
      "train loss: 3.399270539562864e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7876:\n",
      "train loss: 3.8035755621207485e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7877:\n",
      "train loss: 4.112489315919121e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7878:\n",
      "train loss: 3.4154334989130775e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7879:\n",
      "train loss: 3.3529730567445777e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7880:\n",
      "train loss: 3.747325799293103e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7881:\n",
      "train loss: 5.262153939824809e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7882:\n",
      "train loss: 5.864060615969551e-16\n",
      "Epoch 07884: reducing learning rate of group 0 to 1.1880e-17.\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7883:\n",
      "train loss: 5.710612671580882e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7884:\n",
      "train loss: 5.265954491373114e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7885:\n",
      "train loss: 5.276452789603251e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7886:\n",
      "train loss: 3.493833905790679e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7887:\n",
      "train loss: 4.1280429157596634e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7888:\n",
      "train loss: 4.445731480028395e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7889:\n",
      "train loss: 4.1213811233749113e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7890:\n",
      "train loss: 4.153048405691106e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7891:\n",
      "train loss: 4.016733569211042e-16\n",
      "Epoch 07893: reducing learning rate of group 0 to 1.1286e-17.\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7892:\n",
      "train loss: 4.3163973133511183e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7893:\n",
      "train loss: 2.855776599365954e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7894:\n",
      "train loss: 4.315941905432136e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7895:\n",
      "train loss: 4.168256732412595e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7896:\n",
      "train loss: 4.30556666251674e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7897:\n",
      "train loss: 4.79531971530891e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7898:\n",
      "train loss: 4.3353846160583383e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7899:\n",
      "train loss: 3.740283566750806e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7900:\n",
      "train loss: 3.7287368905409085e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7901:\n",
      "train loss: 4.278402562828299e-16\n",
      "Epoch 07903: reducing learning rate of group 0 to 1.0722e-17.\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7902:\n",
      "train loss: 5.625104687679122e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7903:\n",
      "train loss: 3.3269678838060863e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7904:\n",
      "train loss: 3.3885760156522816e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7905:\n",
      "train loss: 2.920983702793379e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7906:\n",
      "train loss: 3.0804015554356347e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7907:\n",
      "train loss: 4.963238398020025e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7908:\n",
      "train loss: 4.787382740635197e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7909:\n",
      "train loss: 3.1574199604046293e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7910:\n",
      "train loss: 3.240674770902903e-16\n",
      "Epoch 07912: reducing learning rate of group 0 to 1.0186e-17.\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7911:\n",
      "train loss: 3.165091563587666e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7912:\n",
      "train loss: 3.8645430693486435e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7913:\n",
      "train loss: 3.686482245123985e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7914:\n",
      "train loss: 6.39451337046752e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7915:\n",
      "train loss: 4.139751168399365e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7916:\n",
      "train loss: 4.748940233980512e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7917:\n",
      "train loss: 3.3552004649587016e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7918:\n",
      "train loss: 3.448532807352371e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7919:\n",
      "train loss: 3.235875174755004e-16\n",
      "Epoch 07921: reducing learning rate of group 0 to 9.6763e-18.\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7920:\n",
      "train loss: 3.24050443212479e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7921:\n",
      "train loss: 3.5599626067501136e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7922:\n",
      "train loss: 4.482150973485504e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7923:\n",
      "train loss: 3.3898408980135663e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7924:\n",
      "train loss: 3.137953519195905e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7925:\n",
      "train loss: 3.4912983028374266e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7926:\n",
      "train loss: 2.9966098612161224e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7927:\n",
      "train loss: 3.9818631008820317e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7928:\n",
      "train loss: 4.470354873682574e-16\n",
      "Epoch 07930: reducing learning rate of group 0 to 9.1925e-18.\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7929:\n",
      "train loss: 3.541068513430326e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7930:\n",
      "train loss: 4.490639930661446e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7931:\n",
      "train loss: 4.599849700178187e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7932:\n",
      "train loss: 4.242509950327864e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7933:\n",
      "train loss: 5.456905081253054e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7934:\n",
      "train loss: 4.408431692671642e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7935:\n",
      "train loss: 3.502706603799579e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7936:\n",
      "train loss: 3.1491642113277226e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7937:\n",
      "train loss: 4.0091975515528575e-16\n",
      "Epoch 07939: reducing learning rate of group 0 to 8.7329e-18.\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7938:\n",
      "train loss: 3.3436541258214915e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7939:\n",
      "train loss: 3.0588412427593135e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7940:\n",
      "train loss: 4.981488903116006e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7941:\n",
      "train loss: 4.880044526211338e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7942:\n",
      "train loss: 6.423284086253632e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7943:\n",
      "train loss: 5.05337157329658e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7944:\n",
      "train loss: 4.343139607279097e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7945:\n",
      "train loss: 3.3874507622593805e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7946:\n",
      "train loss: 2.809719376342678e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7947:\n",
      "train loss: 3.1765345430887094e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7948:\n",
      "train loss: 5.489861863635757e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7949:\n",
      "train loss: 4.1260394767350638e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7950:\n",
      "train loss: 6.4051272424280065e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7951:\n",
      "train loss: 3.80268893093044e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7952:\n",
      "train loss: 3.1445560241490754e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7953:\n",
      "train loss: 2.9801218438444393e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7954:\n",
      "train loss: 3.36469817466978e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7955:\n",
      "train loss: 2.2844075696142965e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7956:\n",
      "train loss: 2.932234177475813e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7957:\n",
      "train loss: 2.6939986787783077e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7958:\n",
      "train loss: 3.4337414153283374e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7959:\n",
      "train loss: 3.3467552253969574e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7960:\n",
      "train loss: 3.0553206307702294e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7961:\n",
      "train loss: 3.153069008028288e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7962:\n",
      "train loss: 3.983674254874433e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7963:\n",
      "train loss: 6.619091866117587e-16\n",
      "Epoch 07965: reducing learning rate of group 0 to 8.2962e-18.\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7964:\n",
      "train loss: 5.872844764911336e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7965:\n",
      "train loss: 7.498210077321701e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7966:\n",
      "train loss: 6.96532897262903e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7967:\n",
      "train loss: 6.51153876797404e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7968:\n",
      "train loss: 4.473726673789533e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7969:\n",
      "train loss: 2.545764198999361e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7970:\n",
      "train loss: 2.9882873616260905e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7971:\n",
      "train loss: 3.999108261080386e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7972:\n",
      "train loss: 3.7836268166544724e-16\n",
      "Epoch 07974: reducing learning rate of group 0 to 7.8814e-18.\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7973:\n",
      "train loss: 3.4664277730845746e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7974:\n",
      "train loss: 4.739444040976794e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7975:\n",
      "train loss: 5.561301662395533e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7976:\n",
      "train loss: 5.764055475005205e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7977:\n",
      "train loss: 3.6948227546937197e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7978:\n",
      "train loss: 3.356680762759224e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7979:\n",
      "train loss: 4.3507551792056165e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7980:\n",
      "train loss: 3.3315394114423784e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7981:\n",
      "train loss: 3.3432583438546504e-16\n",
      "Epoch 07983: reducing learning rate of group 0 to 7.4873e-18.\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7982:\n",
      "train loss: 2.9302438250032814e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7983:\n",
      "train loss: 3.566524442320952e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7984:\n",
      "train loss: 3.9916339211125905e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7985:\n",
      "train loss: 3.195797367978061e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7986:\n",
      "train loss: 3.8279880307812465e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7987:\n",
      "train loss: 3.1112790213659926e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7988:\n",
      "train loss: 2.9406669937221514e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7989:\n",
      "train loss: 3.235280390312736e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7990:\n",
      "train loss: 6.487067336116712e-16\n",
      "Epoch 07992: reducing learning rate of group 0 to 7.1130e-18.\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7991:\n",
      "train loss: 3.4129552295938103e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7992:\n",
      "train loss: 4.992201552859539e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7993:\n",
      "train loss: 6.658687182909699e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7994:\n",
      "train loss: 5.394804982536996e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7995:\n",
      "train loss: 4.953107647376274e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7996:\n",
      "train loss: 4.899100303351399e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7997:\n",
      "train loss: 3.2990169750436e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7998:\n",
      "train loss: 3.211429639245752e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7999:\n",
      "train loss: 2.903124758956146e-16\n",
      "Epoch 08001: reducing learning rate of group 0 to 6.7573e-18.\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8000:\n",
      "train loss: 3.7658197630152084e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8001:\n",
      "train loss: 2.667427049352094e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8002:\n",
      "train loss: 2.6048106410568096e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8003:\n",
      "train loss: 3.410544858188401e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8004:\n",
      "train loss: 3.2553746025119217e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8005:\n",
      "train loss: 3.189015393990637e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8006:\n",
      "train loss: 4.1850439404749783e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8007:\n",
      "train loss: 2.740573481066522e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 8008:\n",
      "train loss: 3.0974345487927943e-16\n",
      "Epoch 08010: reducing learning rate of group 0 to 6.4195e-18.\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8009:\n",
      "train loss: 3.6183195259415597e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8010:\n",
      "train loss: 2.9233845203673657e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8011:\n",
      "train loss: 3.363207069153748e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8012:\n",
      "train loss: 5.231494955919664e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8013:\n",
      "train loss: 3.3819926093871123e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8014:\n",
      "train loss: 4.455698466351236e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8015:\n",
      "train loss: 2.8798681812327536e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8016:\n",
      "train loss: 2.899823000173354e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 8017:\n",
      "train loss: 3.754726810915746e-16\n",
      "Epoch 08019: reducing learning rate of group 0 to 6.0985e-18.\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8018:\n",
      "train loss: 2.7301494540542596e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8019:\n",
      "train loss: 3.4113565917457736e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8020:\n",
      "train loss: 3.116969681654056e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8021:\n",
      "train loss: 3.38521914047394e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8022:\n",
      "train loss: 3.7769910750313717e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8023:\n",
      "train loss: 4.317298614200405e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8024:\n",
      "train loss: 3.7557898860019567e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8025:\n",
      "train loss: 2.9614934139434297e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 8026:\n",
      "train loss: 3.2633809441692608e-16\n",
      "Epoch 08028: reducing learning rate of group 0 to 5.7936e-18.\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8027:\n",
      "train loss: 3.229358548069463e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8028:\n",
      "train loss: 4.3546304078629383e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8029:\n",
      "train loss: 3.3117683223064686e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8030:\n",
      "train loss: 4.461879141993271e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8031:\n",
      "train loss: 4.97309918592825e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8032:\n",
      "train loss: 2.5478205597798813e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8033:\n",
      "train loss: 2.814589723372692e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8034:\n",
      "train loss: 3.4087052242574103e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 8035:\n",
      "train loss: 4.3532990389709794e-16\n",
      "Epoch 08037: reducing learning rate of group 0 to 5.5039e-18.\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8036:\n",
      "train loss: 4.070715440223989e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8037:\n",
      "train loss: 3.9174805613264813e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8038:\n",
      "train loss: 5.084244587896921e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8039:\n",
      "train loss: 5.36978500792745e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8040:\n",
      "train loss: 4.499405388384893e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8041:\n",
      "train loss: 3.815251584315428e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8042:\n",
      "train loss: 4.5734943696317e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8043:\n",
      "train loss: 3.9686035590948897e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 8044:\n",
      "train loss: 4.473444687448896e-16\n",
      "Epoch 08046: reducing learning rate of group 0 to 5.2287e-18.\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8045:\n",
      "train loss: 4.425021286778983e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8046:\n",
      "train loss: 4.2835380762208574e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8047:\n",
      "train loss: 5.4748020651984815e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8048:\n",
      "train loss: 4.698370972305795e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8049:\n",
      "train loss: 4.750095919043919e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8050:\n",
      "train loss: 4.837426964379451e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8051:\n",
      "train loss: 7.085694819885033e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8052:\n",
      "train loss: 8.784187497998938e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 8053:\n",
      "train loss: 5.057698305490877e-16\n",
      "Epoch 08055: reducing learning rate of group 0 to 4.9673e-18.\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8054:\n",
      "train loss: 2.8657182175423364e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8055:\n",
      "train loss: 3.57631103663739e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8056:\n",
      "train loss: 3.693422815601334e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8057:\n",
      "train loss: 3.255026531561872e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8058:\n",
      "train loss: 3.0452084593861817e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8059:\n",
      "train loss: 4.0401842383710513e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8060:\n",
      "train loss: 2.6442182779100446e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8061:\n",
      "train loss: 2.573632245034405e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 8062:\n",
      "train loss: 2.7035536648353386e-16\n",
      "Epoch 08064: reducing learning rate of group 0 to 4.7189e-18.\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8063:\n",
      "train loss: 2.3484019996816995e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8064:\n",
      "train loss: 2.3572548158324306e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8065:\n",
      "train loss: 4.99196000330628e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8066:\n",
      "train loss: 4.989710763836717e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8067:\n",
      "train loss: 4.176783011749924e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8068:\n",
      "train loss: 4.500831644199848e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8069:\n",
      "train loss: 4.617221115876973e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8070:\n",
      "train loss: 3.795030558687132e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8071:\n",
      "train loss: 3.3057647394469455e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8072:\n",
      "train loss: 2.1506855119947023e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8073:\n",
      "train loss: 2.2034377344798279e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8074:\n",
      "train loss: 2.538975768614402e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8075:\n",
      "train loss: 2.6357759193763055e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8076:\n",
      "train loss: 3.161815650171857e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8077:\n",
      "train loss: 3.1138743853890377e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8078:\n",
      "train loss: 2.6704576679653254e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8079:\n",
      "train loss: 2.943328729926358e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 8080:\n",
      "train loss: 2.890006058093064e-16\n",
      "Epoch 08082: reducing learning rate of group 0 to 4.4830e-18.\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8081:\n",
      "train loss: 3.384955462283857e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8082:\n",
      "train loss: 2.8005184586520144e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8083:\n",
      "train loss: 2.4991880551547494e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8084:\n",
      "train loss: 4.378093369961556e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8085:\n",
      "train loss: 4.434514186512207e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8086:\n",
      "train loss: 4.2801474762353395e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8087:\n",
      "train loss: 4.522705798874128e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8088:\n",
      "train loss: 3.807685540145931e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 8089:\n",
      "train loss: 3.7006729861418064e-16\n",
      "Epoch 08091: reducing learning rate of group 0 to 4.2588e-18.\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8090:\n",
      "train loss: 3.3989580056776945e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8091:\n",
      "train loss: 3.211109187653124e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8092:\n",
      "train loss: 2.7876171634959123e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8093:\n",
      "train loss: 3.1756213796611305e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8094:\n",
      "train loss: 2.943523325197324e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8095:\n",
      "train loss: 3.854359183482488e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8096:\n",
      "train loss: 3.5847459643364854e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8097:\n",
      "train loss: 2.829376132870552e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 8098:\n",
      "train loss: 2.3933688987523744e-16\n",
      "Epoch 08100: reducing learning rate of group 0 to 4.0459e-18.\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8099:\n",
      "train loss: 4.3822301760493457e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8100:\n",
      "train loss: 6.598149924360016e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8101:\n",
      "train loss: 4.0779741270082798e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8102:\n",
      "train loss: 3.8287351300960006e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8103:\n",
      "train loss: 4.91950900832496e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8104:\n",
      "train loss: 3.958473920164787e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8105:\n",
      "train loss: 4.981488007645401e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8106:\n",
      "train loss: 2.725023856786486e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 8107:\n",
      "train loss: 2.8780559329046864e-16\n",
      "Epoch 08109: reducing learning rate of group 0 to 3.8436e-18.\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8108:\n",
      "train loss: 2.7307909546874234e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8109:\n",
      "train loss: 2.764024999850644e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8110:\n",
      "train loss: 2.709958480048252e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8111:\n",
      "train loss: 2.490075222697288e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8112:\n",
      "train loss: 2.4607851884278613e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8113:\n",
      "train loss: 4.935715716952196e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8114:\n",
      "train loss: 4.085342500191267e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8115:\n",
      "train loss: 4.014617452497077e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 8116:\n",
      "train loss: 4.467654986204205e-16\n",
      "Epoch 08118: reducing learning rate of group 0 to 3.6514e-18.\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8117:\n",
      "train loss: 5.164325421318537e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8118:\n",
      "train loss: 4.909761914569419e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8119:\n",
      "train loss: 3.7467743659276477e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8120:\n",
      "train loss: 3.203263585405642e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8121:\n",
      "train loss: 3.024641760783035e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8122:\n",
      "train loss: 2.8882056421921267e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8123:\n",
      "train loss: 2.903638871022177e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8124:\n",
      "train loss: 4.224362270030505e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 8125:\n",
      "train loss: 4.322611262812836e-16\n",
      "Epoch 08127: reducing learning rate of group 0 to 3.4688e-18.\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8126:\n",
      "train loss: 4.256694690213268e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8127:\n",
      "train loss: 4.354799201147719e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8128:\n",
      "train loss: 3.944722898370103e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8129:\n",
      "train loss: 3.236952834059623e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8130:\n",
      "train loss: 3.144958890571306e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8131:\n",
      "train loss: 3.530695139549821e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8132:\n",
      "train loss: 4.3366605916881905e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8133:\n",
      "train loss: 3.6301954711474465e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 8134:\n",
      "train loss: 2.811966880133795e-16\n",
      "Epoch 08136: reducing learning rate of group 0 to 3.2954e-18.\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8135:\n",
      "train loss: 2.550965625113925e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8136:\n",
      "train loss: 3.026382478931767e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8137:\n",
      "train loss: 2.9150701944062783e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8138:\n",
      "train loss: 3.3972574442813543e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8139:\n",
      "train loss: 2.8986937971838127e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8140:\n",
      "train loss: 3.918469804380164e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8141:\n",
      "train loss: 2.958254884364041e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8142:\n",
      "train loss: 4.3587366509998423e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 8143:\n",
      "train loss: 4.500118309158398e-16\n",
      "Epoch 08145: reducing learning rate of group 0 to 3.1306e-18.\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8144:\n",
      "train loss: 3.6033402163535396e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8145:\n",
      "train loss: 4.139758918558617e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8146:\n",
      "train loss: 2.704963259359487e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8147:\n",
      "train loss: 3.5484526266726696e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8148:\n",
      "train loss: 3.549704689619998e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8149:\n",
      "train loss: 2.9574313028577697e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8150:\n",
      "train loss: 3.0560573604043656e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8151:\n",
      "train loss: 3.341007603949106e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 8152:\n",
      "train loss: 3.085184568419072e-16\n",
      "Epoch 08154: reducing learning rate of group 0 to 2.9741e-18.\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8153:\n",
      "train loss: 3.034485343258498e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8154:\n",
      "train loss: 3.321699800013323e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8155:\n",
      "train loss: 2.601994174127641e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8156:\n",
      "train loss: 2.550981918492117e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8157:\n",
      "train loss: 2.5948375404417515e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8158:\n",
      "train loss: 3.240277556855676e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8159:\n",
      "train loss: 2.6947377332707203e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8160:\n",
      "train loss: 3.5828907425430987e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 8161:\n",
      "train loss: 4.199481684983718e-16\n",
      "Epoch 08163: reducing learning rate of group 0 to 2.8254e-18.\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8162:\n",
      "train loss: 4.199481684983718e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8163:\n",
      "train loss: 4.199481684983718e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8164:\n",
      "train loss: 4.199481684983718e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8165:\n",
      "train loss: 3.558769960558129e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8166:\n",
      "train loss: 4.819325290334125e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8167:\n",
      "train loss: 4.0491841868374063e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8168:\n",
      "train loss: 4.1950776689173533e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8169:\n",
      "train loss: 3.6439194366251896e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 8170:\n",
      "train loss: 3.5339491111434005e-16\n",
      "Epoch 08172: reducing learning rate of group 0 to 2.6841e-18.\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8171:\n",
      "train loss: 2.4183259795340713e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8172:\n",
      "train loss: 2.4224076488999717e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8173:\n",
      "train loss: 2.385201039225374e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8174:\n",
      "train loss: 2.385201039225374e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8175:\n",
      "train loss: 2.597506109448658e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8176:\n",
      "train loss: 2.597506109448658e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8177:\n",
      "train loss: 2.597506109448658e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8178:\n",
      "train loss: 3.7109345532710956e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 8179:\n",
      "train loss: 5.823780904811298e-16\n",
      "Epoch 08181: reducing learning rate of group 0 to 2.5499e-18.\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8180:\n",
      "train loss: 3.008435116828245e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8181:\n",
      "train loss: 2.989318835506546e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8182:\n",
      "train loss: 2.994318179691348e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8183:\n",
      "train loss: 2.850844773029173e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8184:\n",
      "train loss: 2.850844773029173e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8185:\n",
      "train loss: 3.552531051457424e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8186:\n",
      "train loss: 3.552531051457424e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8187:\n",
      "train loss: 3.5097005599922625e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 8188:\n",
      "train loss: 3.5097005599922625e-16\n",
      "Epoch 08190: reducing learning rate of group 0 to 2.4224e-18.\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8189:\n",
      "train loss: 3.528540510246645e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8190:\n",
      "train loss: 3.6853299499714866e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8191:\n",
      "train loss: 2.656373576196416e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8192:\n",
      "train loss: 2.3260625970894196e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8193:\n",
      "train loss: 2.3260625970894196e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8194:\n",
      "train loss: 5.708069928102691e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8195:\n",
      "train loss: 6.271239225531776e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8196:\n",
      "train loss: 6.187901924411118e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 8197:\n",
      "train loss: 3.3593296041227703e-16\n",
      "Epoch 08199: reducing learning rate of group 0 to 2.3013e-18.\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8198:\n",
      "train loss: 2.6720960051967463e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8199:\n",
      "train loss: 3.0872311532627016e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8200:\n",
      "train loss: 3.0872311532627016e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8201:\n",
      "train loss: 3.1346857871276577e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8202:\n",
      "train loss: 2.68780978250898e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8203:\n",
      "train loss: 2.284651916714641e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8204:\n",
      "train loss: 2.6843783172265225e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8205:\n",
      "train loss: 2.632243672663725e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 8206:\n",
      "train loss: 2.6192889769162634e-16\n",
      "Epoch 08208: reducing learning rate of group 0 to 2.1862e-18.\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8207:\n",
      "train loss: 3.0786431013384026e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8208:\n",
      "train loss: 3.103382802347825e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8209:\n",
      "train loss: 3.6286397378179846e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8210:\n",
      "train loss: 3.6286397378179846e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8211:\n",
      "train loss: 3.4993950130230494e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8212:\n",
      "train loss: 3.4993950130230494e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8213:\n",
      "train loss: 4.462152387199447e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8214:\n",
      "train loss: 4.807624530243861e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 8215:\n",
      "train loss: 3.8227451966058907e-16\n",
      "Epoch 08217: reducing learning rate of group 0 to 2.0769e-18.\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8216:\n",
      "train loss: 3.8227451966058907e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8217:\n",
      "train loss: 2.6703769233989187e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8218:\n",
      "train loss: 3.2448938698100766e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8219:\n",
      "train loss: 3.068345672339034e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8220:\n",
      "train loss: 2.9672037694083015e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8221:\n",
      "train loss: 2.661165371152695e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8222:\n",
      "train loss: 3.16418978571738e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8223:\n",
      "train loss: 3.16418978571738e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 8224:\n",
      "train loss: 3.158935127137444e-16\n",
      "Epoch 08226: reducing learning rate of group 0 to 1.9731e-18.\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8225:\n",
      "train loss: 3.539858857043873e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8226:\n",
      "train loss: 4.530368178004884e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8227:\n",
      "train loss: 6.169157971668098e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8228:\n",
      "train loss: 6.169157971668098e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8229:\n",
      "train loss: 4.563613904027668e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8230:\n",
      "train loss: 3.262637678710444e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8231:\n",
      "train loss: 2.7541489495096315e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8232:\n",
      "train loss: 3.1057814868121547e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 8233:\n",
      "train loss: 2.7287972899477753e-16\n",
      "Epoch 08235: reducing learning rate of group 0 to 1.8744e-18.\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8234:\n",
      "train loss: 2.669391465002408e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8235:\n",
      "train loss: 2.595709171574342e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8236:\n",
      "train loss: 2.595709171574342e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8237:\n",
      "train loss: 5.437071188637464e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8238:\n",
      "train loss: 5.932264815230509e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8239:\n",
      "train loss: 5.895438903607767e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8240:\n",
      "train loss: 4.204460816625929e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8241:\n",
      "train loss: 4.204460816625929e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 8242:\n",
      "train loss: 4.204460816625929e-16\n",
      "Epoch 08244: reducing learning rate of group 0 to 1.7807e-18.\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8243:\n",
      "train loss: 3.54821441064094e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8244:\n",
      "train loss: 4.138909286885179e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8245:\n",
      "train loss: 4.116980129900234e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8246:\n",
      "train loss: 5.396414022826753e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8247:\n",
      "train loss: 5.494081419737305e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8248:\n",
      "train loss: 5.04081779328262e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8249:\n",
      "train loss: 5.280666832035795e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8250:\n",
      "train loss: 3.420521508666194e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 8251:\n",
      "train loss: 3.342509408867061e-16\n",
      "Epoch 08253: reducing learning rate of group 0 to 1.6917e-18.\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8252:\n",
      "train loss: 3.1474665262810583e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8253:\n",
      "train loss: 3.1760719615530485e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8254:\n",
      "train loss: 3.1687030430795797e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8255:\n",
      "train loss: 3.1599350254578267e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8256:\n",
      "train loss: 3.1985692453608293e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8257:\n",
      "train loss: 3.6235538371019775e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8258:\n",
      "train loss: 3.671356291909449e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8259:\n",
      "train loss: 3.762988574709322e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 8260:\n",
      "train loss: 3.8158941716710924e-16\n",
      "Epoch 08262: reducing learning rate of group 0 to 1.6071e-18.\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8261:\n",
      "train loss: 4.384437050354969e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8262:\n",
      "train loss: 4.568217497622495e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8263:\n",
      "train loss: 3.26347117634663e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8264:\n",
      "train loss: 3.875610177999855e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8265:\n",
      "train loss: 3.6420681162833797e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8266:\n",
      "train loss: 3.212095005809362e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8267:\n",
      "train loss: 3.2483696107376194e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8268:\n",
      "train loss: 2.873434290936324e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 8269:\n",
      "train loss: 3.066727903437829e-16\n",
      "Epoch 08271: reducing learning rate of group 0 to 1.5267e-18.\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8270:\n",
      "train loss: 3.097442658034797e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8271:\n",
      "train loss: 2.851562998360374e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8272:\n",
      "train loss: 2.851562998360374e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8273:\n",
      "train loss: 3.0256633102997414e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8274:\n",
      "train loss: 2.980226337134455e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8275:\n",
      "train loss: 2.8242216020256545e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8276:\n",
      "train loss: 2.803859325690071e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8277:\n",
      "train loss: 2.8205390740132435e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 8278:\n",
      "train loss: 2.8205390740132435e-16\n",
      "Epoch 08280: reducing learning rate of group 0 to 1.4504e-18.\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8279:\n",
      "train loss: 2.8205390740132435e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8280:\n",
      "train loss: 2.983370066354368e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8281:\n",
      "train loss: 3.1367803946908415e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8282:\n",
      "train loss: 2.6415465921136635e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8283:\n",
      "train loss: 2.6415465921136635e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8284:\n",
      "train loss: 2.60189403874025e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8285:\n",
      "train loss: 2.592472637144437e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8286:\n",
      "train loss: 2.592472637144437e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 8287:\n",
      "train loss: 2.592472637144437e-16\n",
      "Epoch 08289: reducing learning rate of group 0 to 1.3779e-18.\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8288:\n",
      "train loss: 2.592472637144437e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8289:\n",
      "train loss: 2.761660382650464e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8290:\n",
      "train loss: 2.6832768387142846e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8291:\n",
      "train loss: 5.573602699327982e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8292:\n",
      "train loss: 2.726016650860011e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8293:\n",
      "train loss: 5.573602699327982e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8294:\n",
      "train loss: 2.2669475839091473e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8295:\n",
      "train loss: 2.2669475839091473e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 8296:\n",
      "train loss: 4.130705000152179e-16\n",
      "Epoch 08298: reducing learning rate of group 0 to 1.3090e-18.\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8297:\n",
      "train loss: 4.130705000152179e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8298:\n",
      "train loss: 4.276327791171508e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8299:\n",
      "train loss: 4.276327791171508e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8300:\n",
      "train loss: 4.276327791171508e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8301:\n",
      "train loss: 4.260723923974378e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8302:\n",
      "train loss: 4.260723923974378e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8303:\n",
      "train loss: 3.2531007227390117e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8304:\n",
      "train loss: 3.2531007227390117e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 8305:\n",
      "train loss: 3.227890464427201e-16\n",
      "Epoch 08307: reducing learning rate of group 0 to 1.2435e-18.\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8306:\n",
      "train loss: 3.6476035557202195e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8307:\n",
      "train loss: 3.320897694445234e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8308:\n",
      "train loss: 3.320897694445234e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8309:\n",
      "train loss: 2.507044592102447e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8310:\n",
      "train loss: 2.507044592102447e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8311:\n",
      "train loss: 2.507044592102447e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8312:\n",
      "train loss: 2.507044592102447e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8313:\n",
      "train loss: 2.507044592102447e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 8314:\n",
      "train loss: 2.507044592102447e-16\n",
      "Epoch 08316: reducing learning rate of group 0 to 1.1813e-18.\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8315:\n",
      "train loss: 2.180413912895765e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8316:\n",
      "train loss: 2.560642099237327e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8317:\n",
      "train loss: 2.552446959880088e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8318:\n",
      "train loss: 3.3071973944998796e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8319:\n",
      "train loss: 3.3071973944998796e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8320:\n",
      "train loss: 3.282432390858781e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8321:\n",
      "train loss: 2.924966111118418e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8322:\n",
      "train loss: 2.924966111118418e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 8323:\n",
      "train loss: 2.66056618445717e-16\n",
      "Epoch 08325: reducing learning rate of group 0 to 1.1223e-18.\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8324:\n",
      "train loss: 3.607538433118006e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8325:\n",
      "train loss: 3.879104194935777e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8326:\n",
      "train loss: 3.879104194935777e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8327:\n",
      "train loss: 2.909769932468346e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8328:\n",
      "train loss: 2.909769932468346e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8329:\n",
      "train loss: 2.909769932468346e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8330:\n",
      "train loss: 2.7954292721303066e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8331:\n",
      "train loss: 2.7954292721303066e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 8332:\n",
      "train loss: 2.691367627821346e-16\n",
      "Epoch 08334: reducing learning rate of group 0 to 1.0662e-18.\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8333:\n",
      "train loss: 2.285057209464464e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8334:\n",
      "train loss: 2.7216262202037177e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8335:\n",
      "train loss: 2.719962524554173e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8336:\n",
      "train loss: 2.7068136124122633e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8337:\n",
      "train loss: 2.3780544277108638e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8338:\n",
      "train loss: 2.3780544277108638e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8339:\n",
      "train loss: 2.3780544277108638e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8340:\n",
      "train loss: 2.3780544277108638e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 8341:\n",
      "train loss: 2.9330650927474053e-16\n",
      "Epoch 08343: reducing learning rate of group 0 to 1.0129e-18.\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8342:\n",
      "train loss: 2.9330650927474053e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8343:\n",
      "train loss: 2.9330650927474053e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8344:\n",
      "train loss: 2.9330650927474053e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8345:\n",
      "train loss: 2.9330650927474053e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8346:\n",
      "train loss: 2.9330650927474053e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8347:\n",
      "train loss: 3.1461523764294607e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8348:\n",
      "train loss: 2.321231045328388e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8349:\n",
      "train loss: 3.445966926475985e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 8350:\n",
      "train loss: 3.445966926475985e-16\n",
      "Epoch 08352: reducing learning rate of group 0 to 9.6222e-19.\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8351:\n",
      "train loss: 3.2282008084485e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8352:\n",
      "train loss: 2.933810068223941e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8353:\n",
      "train loss: 2.933810068223941e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8354:\n",
      "train loss: 2.933810068223941e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8355:\n",
      "train loss: 2.933810068223941e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8356:\n",
      "train loss: 2.933810068223941e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8357:\n",
      "train loss: 3.1740343848600643e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8358:\n",
      "train loss: 3.1350217006032293e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 8359:\n",
      "train loss: 3.4033970209347343e-16\n",
      "Epoch 08361: reducing learning rate of group 0 to 9.1410e-19.\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8360:\n",
      "train loss: 3.4033970209347343e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8361:\n",
      "train loss: 3.4033970209347343e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8362:\n",
      "train loss: 3.041729061342846e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8363:\n",
      "train loss: 3.041729061342846e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8364:\n",
      "train loss: 3.127027622907188e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8365:\n",
      "train loss: 3.1401453655952296e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8366:\n",
      "train loss: 3.1401453655952296e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8367:\n",
      "train loss: 2.8595635419800903e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 8368:\n",
      "train loss: 2.893976787754838e-16\n",
      "Epoch 08370: reducing learning rate of group 0 to 8.6840e-19.\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8369:\n",
      "train loss: 2.893976787754838e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8370:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8371:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8372:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8373:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8374:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8375:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8376:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 8377:\n",
      "train loss: 3.279956423374975e-16\n",
      "Epoch 08379: reducing learning rate of group 0 to 8.2498e-19.\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8378:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8379:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8380:\n",
      "train loss: 3.279956423374975e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8381:\n",
      "train loss: 3.91128264009993e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8382:\n",
      "train loss: 3.2737807834342336e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8383:\n",
      "train loss: 3.421722664815882e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8384:\n",
      "train loss: 3.421722664815882e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8385:\n",
      "train loss: 3.8574584575120696e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 8386:\n",
      "train loss: 2.9576955468141887e-16\n",
      "Epoch 08388: reducing learning rate of group 0 to 7.8373e-19.\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8387:\n",
      "train loss: 2.9576955468141887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8388:\n",
      "train loss: 2.9576955468141887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8389:\n",
      "train loss: 2.9576955468141887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8390:\n",
      "train loss: 2.4229669180212086e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8391:\n",
      "train loss: 2.4229669180212086e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8392:\n",
      "train loss: 2.4229669180212086e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8393:\n",
      "train loss: 2.3402447797001613e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8394:\n",
      "train loss: 2.3402447797001613e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 8395:\n",
      "train loss: 2.3402447797001613e-16\n",
      "Epoch 08397: reducing learning rate of group 0 to 7.4454e-19.\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8396:\n",
      "train loss: 2.9693262874482344e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8397:\n",
      "train loss: 2.9693262874482344e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8398:\n",
      "train loss: 2.9693262874482344e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8399:\n",
      "train loss: 2.921560322592005e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8400:\n",
      "train loss: 2.921560322592005e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8401:\n",
      "train loss: 2.921560322592005e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8402:\n",
      "train loss: 2.921560322592005e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8403:\n",
      "train loss: 3.281815000448944e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 8404:\n",
      "train loss: 3.281815000448944e-16\n",
      "Epoch 08406: reducing learning rate of group 0 to 7.0732e-19.\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8405:\n",
      "train loss: 3.281815000448944e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8406:\n",
      "train loss: 3.884565303483911e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8407:\n",
      "train loss: 3.810702012423774e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8408:\n",
      "train loss: 2.616537725638498e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8409:\n",
      "train loss: 2.616537725638498e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8410:\n",
      "train loss: 2.616537725638498e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8411:\n",
      "train loss: 4.00983106318732e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8412:\n",
      "train loss: 4.00983106318732e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 8413:\n",
      "train loss: 4.00983106318732e-16\n",
      "Epoch 08415: reducing learning rate of group 0 to 6.7195e-19.\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8414:\n",
      "train loss: 3.731151768293578e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8415:\n",
      "train loss: 3.731151768293578e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8416:\n",
      "train loss: 3.5297801279206947e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8417:\n",
      "train loss: 3.5297801279206947e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8418:\n",
      "train loss: 3.527756314444864e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8419:\n",
      "train loss: 3.527756314444864e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8420:\n",
      "train loss: 3.0036571344237873e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8421:\n",
      "train loss: 3.0036571344237873e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 8422:\n",
      "train loss: 3.0036571344237873e-16\n",
      "Epoch 08424: reducing learning rate of group 0 to 6.3835e-19.\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8423:\n",
      "train loss: 3.0036571344237873e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8424:\n",
      "train loss: 3.0036571344237873e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8425:\n",
      "train loss: 3.0036571344237873e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8426:\n",
      "train loss: 2.455686568139049e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8427:\n",
      "train loss: 2.459898406448644e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8428:\n",
      "train loss: 2.459898406448644e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8429:\n",
      "train loss: 2.459898406448644e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8430:\n",
      "train loss: 2.459898406448644e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 8431:\n",
      "train loss: 2.459898406448644e-16\n",
      "Epoch 08433: reducing learning rate of group 0 to 6.0644e-19.\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8432:\n",
      "train loss: 2.720027932374344e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8433:\n",
      "train loss: 2.720027932374344e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8434:\n",
      "train loss: 2.7043165161257767e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8435:\n",
      "train loss: 2.67631807191736e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8436:\n",
      "train loss: 2.67631807191736e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8437:\n",
      "train loss: 2.67631807191736e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8438:\n",
      "train loss: 2.67631807191736e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8439:\n",
      "train loss: 2.3528363562546324e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 8440:\n",
      "train loss: 2.3184531314013254e-16\n",
      "Epoch 08442: reducing learning rate of group 0 to 5.7611e-19.\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8441:\n",
      "train loss: 2.3184531314013254e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8442:\n",
      "train loss: 2.3184531314013254e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8443:\n",
      "train loss: 2.263693750195638e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8444:\n",
      "train loss: 2.263693750195638e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8445:\n",
      "train loss: 2.263693750195638e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8446:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8447:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8448:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 8449:\n",
      "train loss: 3.3168085680759193e-16\n",
      "Epoch 08451: reducing learning rate of group 0 to 5.4731e-19.\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8450:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8451:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8452:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8453:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8454:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8455:\n",
      "train loss: 3.3168085680759193e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8456:\n",
      "train loss: 2.8783135927765075e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8457:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 8458:\n",
      "train loss: 2.766344235328715e-16\n",
      "Epoch 08460: reducing learning rate of group 0 to 5.1994e-19.\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8459:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8460:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8461:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8462:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8463:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8464:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8465:\n",
      "train loss: 2.766344235328715e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8466:\n",
      "train loss: 2.9280233984517877e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 8467:\n",
      "train loss: 2.9280233984517877e-16\n",
      "Epoch 08469: reducing learning rate of group 0 to 4.9395e-19.\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8468:\n",
      "train loss: 2.9280233984517877e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8469:\n",
      "train loss: 2.9280233984517877e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8470:\n",
      "train loss: 2.955641304924751e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8471:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8472:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8473:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8474:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8475:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 8476:\n",
      "train loss: 2.648155002140838e-16\n",
      "Epoch 08478: reducing learning rate of group 0 to 4.6925e-19.\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8477:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8478:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8479:\n",
      "train loss: 2.648155002140838e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8480:\n",
      "train loss: 2.4071736790204515e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8481:\n",
      "train loss: 2.4071736790204515e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8482:\n",
      "train loss: 2.939650372732954e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8483:\n",
      "train loss: 2.939650372732954e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8484:\n",
      "train loss: 2.939650372732954e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 8485:\n",
      "train loss: 2.939650372732954e-16\n",
      "Epoch 08487: reducing learning rate of group 0 to 4.4579e-19.\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8486:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8487:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8488:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8489:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8490:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8491:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8492:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8493:\n",
      "train loss: 2.428914578054294e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8494:\n",
      "train loss: 2.41474694144941e-16\n",
      "Epoch 08496: reducing learning rate of group 0 to 4.2350e-19.\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8495:\n",
      "train loss: 2.41474694144941e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8496:\n",
      "train loss: 2.410597380533107e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8497:\n",
      "train loss: 2.410597380533107e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8498:\n",
      "train loss: 2.410597380533107e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8499:\n",
      "train loss: 2.410597380533107e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8500:\n",
      "train loss: 2.449759472861897e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8501:\n",
      "train loss: 2.297021568165913e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8502:\n",
      "train loss: 2.4807935297783864e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8503:\n",
      "train loss: 2.4807935297783864e-16\n",
      "Epoch 08505: reducing learning rate of group 0 to 4.0232e-19.\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8504:\n",
      "train loss: 2.4807935297783864e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8505:\n",
      "train loss: 2.4807935297783864e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8506:\n",
      "train loss: 2.4807935297783864e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8507:\n",
      "train loss: 2.4316569683277676e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8508:\n",
      "train loss: 2.4316569683277676e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8509:\n",
      "train loss: 2.4316569683277676e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8510:\n",
      "train loss: 2.3546109918766736e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8511:\n",
      "train loss: 2.305518369662599e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8512:\n",
      "train loss: 2.305518369662599e-16\n",
      "Epoch 08514: reducing learning rate of group 0 to 3.8221e-19.\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8513:\n",
      "train loss: 2.6542283235903816e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8514:\n",
      "train loss: 2.6542283235903816e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8515:\n",
      "train loss: 2.6542283235903816e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8516:\n",
      "train loss: 2.6542283235903816e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8517:\n",
      "train loss: 2.86763482898248e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8518:\n",
      "train loss: 2.86763482898248e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8519:\n",
      "train loss: 2.76061050728676e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8520:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8521:\n",
      "train loss: 2.896586739083098e-16\n",
      "Epoch 08523: reducing learning rate of group 0 to 3.6310e-19.\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8522:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8523:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8524:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8525:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8526:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8527:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8528:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8529:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8530:\n",
      "train loss: 2.896586739083098e-16\n",
      "Epoch 08532: reducing learning rate of group 0 to 3.4494e-19.\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8531:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8532:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8533:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8534:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8535:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8536:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8537:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8538:\n",
      "train loss: 2.8453105476666714e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8539:\n",
      "train loss: 2.8453105476666714e-16\n",
      "Epoch 08541: reducing learning rate of group 0 to 3.2769e-19.\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8540:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8541:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8542:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8543:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8544:\n",
      "train loss: 2.896586739083098e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8545:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8546:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8547:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8548:\n",
      "train loss: 2.8925864700865546e-16\n",
      "Epoch 08550: reducing learning rate of group 0 to 3.1131e-19.\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8549:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8550:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8551:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8552:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8553:\n",
      "train loss: 2.8925864700865546e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8554:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8555:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8556:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8557:\n",
      "train loss: 2.839679166728568e-16\n",
      "Epoch 08559: reducing learning rate of group 0 to 2.9574e-19.\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8558:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8559:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8560:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8561:\n",
      "train loss: 2.839679166728568e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8562:\n",
      "train loss: 2.870341461645853e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8563:\n",
      "train loss: 2.870341461645853e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8564:\n",
      "train loss: 2.870341461645853e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8565:\n",
      "train loss: 2.870341461645853e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8566:\n",
      "train loss: 3.1607840743806194e-16\n",
      "Epoch 08568: reducing learning rate of group 0 to 2.8096e-19.\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8567:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8568:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8569:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8570:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8571:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8572:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8573:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8574:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8575:\n",
      "train loss: 3.05419444479422e-16\n",
      "Epoch 08577: reducing learning rate of group 0 to 2.6691e-19.\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8576:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8577:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8578:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8579:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8580:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8581:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8582:\n",
      "train loss: 3.05419444479422e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8583:\n",
      "train loss: 2.8536633288904723e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8584:\n",
      "train loss: 2.8536633288904723e-16\n",
      "Epoch 08586: reducing learning rate of group 0 to 2.5356e-19.\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8585:\n",
      "train loss: 2.8536633288904723e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8586:\n",
      "train loss: 2.8536633288904723e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8587:\n",
      "train loss: 2.8536633288904723e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8588:\n",
      "train loss: 2.8536633288904723e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8589:\n",
      "train loss: 3.1702119404166727e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8590:\n",
      "train loss: 2.9314014229015204e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8591:\n",
      "train loss: 2.9314014229015204e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8592:\n",
      "train loss: 2.9314014229015204e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8593:\n",
      "train loss: 2.9314014229015204e-16\n",
      "Epoch 08595: reducing learning rate of group 0 to 2.4088e-19.\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8594:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8595:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8596:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8597:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8598:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8599:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8600:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8601:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8602:\n",
      "train loss: 2.9410115453557094e-16\n",
      "Epoch 08604: reducing learning rate of group 0 to 2.2884e-19.\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8603:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8604:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8605:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8606:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8607:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8608:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8609:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8610:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8611:\n",
      "train loss: 2.9410115453557094e-16\n",
      "Epoch 08613: reducing learning rate of group 0 to 2.1740e-19.\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8612:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8613:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8614:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8615:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8616:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8617:\n",
      "train loss: 2.9410115453557094e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8618:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8619:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8620:\n",
      "train loss: 2.231753503802191e-16\n",
      "Epoch 08622: reducing learning rate of group 0 to 2.0653e-19.\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8621:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8622:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8623:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8624:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8625:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8626:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8627:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8628:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8629:\n",
      "train loss: 2.231753503802191e-16\n",
      "Epoch 08631: reducing learning rate of group 0 to 1.9620e-19.\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8630:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8631:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8632:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8633:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8634:\n",
      "train loss: 2.231753503802191e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8635:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8636:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8637:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8638:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08640: reducing learning rate of group 0 to 1.8639e-19.\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8639:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8640:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8641:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8642:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8643:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8644:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8645:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8646:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8647:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08649: reducing learning rate of group 0 to 1.7707e-19.\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8648:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8649:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8650:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8651:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8652:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8653:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8654:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8655:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8656:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08658: reducing learning rate of group 0 to 1.6822e-19.\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8657:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8658:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8659:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8660:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8661:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8662:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8663:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8664:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8665:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08667: reducing learning rate of group 0 to 1.5981e-19.\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8666:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8667:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8668:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8669:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8670:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8671:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8672:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8673:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8674:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08676: reducing learning rate of group 0 to 1.5182e-19.\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8675:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8676:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8677:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8678:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8679:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8680:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8681:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8682:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8683:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08685: reducing learning rate of group 0 to 1.4423e-19.\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8684:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8685:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8686:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8687:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8688:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8689:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8690:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8691:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8692:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08694: reducing learning rate of group 0 to 1.3702e-19.\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8693:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8694:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8695:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8696:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8697:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8698:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8699:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8700:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8701:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08703: reducing learning rate of group 0 to 1.3016e-19.\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8702:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8703:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8704:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8705:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8706:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8707:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8708:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8709:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8710:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08712: reducing learning rate of group 0 to 1.2366e-19.\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8711:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8712:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8713:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8714:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8715:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8716:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8717:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8718:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8719:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08721: reducing learning rate of group 0 to 1.1747e-19.\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8720:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8721:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8722:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8723:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8724:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8725:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8726:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8727:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8728:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08730: reducing learning rate of group 0 to 1.1160e-19.\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8729:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8730:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8731:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8732:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8733:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8734:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8735:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8736:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8737:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08739: reducing learning rate of group 0 to 1.0602e-19.\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8738:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8739:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8740:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8741:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8742:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8743:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8744:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8745:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8746:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08748: reducing learning rate of group 0 to 1.0072e-19.\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8747:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8748:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8749:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8750:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8751:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8752:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8753:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8754:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8755:\n",
      "train loss: 2.336340650256298e-16\n",
      "Epoch 08757: reducing learning rate of group 0 to 9.5683e-20.\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8756:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8757:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8758:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8759:\n",
      "train loss: 2.336340650256298e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8760:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8761:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8762:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8763:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8764:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08766: reducing learning rate of group 0 to 9.0899e-20.\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8765:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8766:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8767:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8768:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8769:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8770:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8771:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8772:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8773:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08775: reducing learning rate of group 0 to 8.6354e-20.\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8774:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8775:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8776:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8777:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8778:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8779:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8780:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8781:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8782:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08784: reducing learning rate of group 0 to 8.2036e-20.\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8783:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8784:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8785:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8786:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8787:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8788:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8789:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8790:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8791:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08793: reducing learning rate of group 0 to 7.7934e-20.\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8792:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8793:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8794:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8795:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8796:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8797:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8798:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8799:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8800:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08802: reducing learning rate of group 0 to 7.4038e-20.\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8801:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8802:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8803:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8804:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8805:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8806:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8807:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8808:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8809:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08811: reducing learning rate of group 0 to 7.0336e-20.\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8810:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8811:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8812:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8813:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8814:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8815:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8816:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8817:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8818:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08820: reducing learning rate of group 0 to 6.6819e-20.\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8819:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8820:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8821:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8822:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8823:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8824:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8825:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8826:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8827:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08829: reducing learning rate of group 0 to 6.3478e-20.\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8828:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8829:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8830:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8831:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8832:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8833:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8834:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8835:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8836:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08838: reducing learning rate of group 0 to 6.0304e-20.\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8837:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8838:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8839:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8840:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8841:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8842:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8843:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8844:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8845:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08847: reducing learning rate of group 0 to 5.7289e-20.\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8846:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8847:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8848:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8849:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8850:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8851:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8852:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8853:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8854:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08856: reducing learning rate of group 0 to 5.4424e-20.\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8855:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8856:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8857:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8858:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8859:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8860:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8861:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8862:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8863:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08865: reducing learning rate of group 0 to 5.1703e-20.\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8864:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8865:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8866:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8867:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8868:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8869:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8870:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8871:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8872:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08874: reducing learning rate of group 0 to 4.9118e-20.\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8873:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8874:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8875:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8876:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8877:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8878:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8879:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8880:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8881:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08883: reducing learning rate of group 0 to 4.6662e-20.\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8882:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8883:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8884:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8885:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8886:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8887:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8888:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8889:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8890:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08892: reducing learning rate of group 0 to 4.4329e-20.\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8891:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8892:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8893:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8894:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8895:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8896:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8897:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8898:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8899:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08901: reducing learning rate of group 0 to 4.2113e-20.\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8900:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8901:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8902:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8903:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8904:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8905:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8906:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8907:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8908:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08910: reducing learning rate of group 0 to 4.0007e-20.\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8909:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8910:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8911:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8912:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8913:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8914:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8915:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8916:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8917:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08919: reducing learning rate of group 0 to 3.8007e-20.\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8918:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8919:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8920:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8921:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8922:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8923:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8924:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8925:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8926:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08928: reducing learning rate of group 0 to 3.6106e-20.\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8927:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8928:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8929:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8930:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8931:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8932:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8933:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8934:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8935:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08937: reducing learning rate of group 0 to 3.4301e-20.\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8936:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8937:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8938:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8939:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8940:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8941:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8942:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8943:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8944:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08946: reducing learning rate of group 0 to 3.2586e-20.\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8945:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8946:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8947:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8948:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8949:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8950:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8951:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8952:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8953:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08955: reducing learning rate of group 0 to 3.0957e-20.\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8954:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8955:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8956:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8957:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8958:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8959:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8960:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8961:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8962:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08964: reducing learning rate of group 0 to 2.9409e-20.\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8963:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8964:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8965:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8966:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8967:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8968:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8969:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8970:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8971:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08973: reducing learning rate of group 0 to 2.7938e-20.\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8972:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8973:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8974:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8975:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8976:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8977:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8978:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8979:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8980:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08982: reducing learning rate of group 0 to 2.6541e-20.\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8981:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8982:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8983:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8984:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8985:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8986:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8987:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8988:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8989:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 08991: reducing learning rate of group 0 to 2.5214e-20.\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8990:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8991:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8992:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8993:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8994:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8995:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8996:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8997:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8998:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09000: reducing learning rate of group 0 to 2.3954e-20.\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8999:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9000:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9001:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9002:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9003:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9004:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9005:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9006:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 9007:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09009: reducing learning rate of group 0 to 2.2756e-20.\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9008:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9009:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9010:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9011:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9012:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9013:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9014:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9015:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 9016:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09018: reducing learning rate of group 0 to 2.1618e-20.\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9017:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9018:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9019:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9020:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9021:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9022:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9023:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9024:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 9025:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09027: reducing learning rate of group 0 to 2.0537e-20.\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9026:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9027:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9028:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9029:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9030:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9031:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9032:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9033:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 9034:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09036: reducing learning rate of group 0 to 1.9510e-20.\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9035:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9036:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9037:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9038:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9039:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9040:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9041:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9042:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 9043:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09045: reducing learning rate of group 0 to 1.8535e-20.\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9044:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9045:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9046:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9047:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9048:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9049:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9050:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9051:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 9052:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09054: reducing learning rate of group 0 to 1.7608e-20.\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9053:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9054:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9055:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9056:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9057:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9058:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9059:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9060:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 9061:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09063: reducing learning rate of group 0 to 1.6728e-20.\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9062:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9063:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9064:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9065:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9066:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9067:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9068:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9069:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 9070:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09072: reducing learning rate of group 0 to 1.5891e-20.\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9071:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9072:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9073:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9074:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9075:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9076:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9077:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9078:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 9079:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09081: reducing learning rate of group 0 to 1.5097e-20.\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9080:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9081:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9082:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9083:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9084:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9085:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9086:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9087:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 9088:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09090: reducing learning rate of group 0 to 1.4342e-20.\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9089:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9090:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9091:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9092:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9093:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9094:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9095:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9096:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 9097:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09099: reducing learning rate of group 0 to 1.3625e-20.\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9098:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9099:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9100:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9101:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9102:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9103:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9104:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9105:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 9106:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09108: reducing learning rate of group 0 to 1.2944e-20.\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9107:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9108:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9109:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9110:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9111:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9112:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9113:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9114:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 9115:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09117: reducing learning rate of group 0 to 1.2296e-20.\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9116:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9117:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9118:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9119:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9120:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9121:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9122:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9123:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 9124:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09126: reducing learning rate of group 0 to 1.1682e-20.\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9125:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9126:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9127:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9128:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9129:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9130:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9131:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9132:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 9133:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09135: reducing learning rate of group 0 to 1.1098e-20.\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9134:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9135:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9136:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9137:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9138:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9139:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9140:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9141:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 9142:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09144: reducing learning rate of group 0 to 1.0543e-20.\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9143:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9144:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9145:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9146:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9147:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9148:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9149:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9150:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 9151:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09153: reducing learning rate of group 0 to 1.0016e-20.\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9152:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9153:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9154:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9155:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9156:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9157:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9158:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9159:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 9160:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09162: reducing learning rate of group 0 to 9.5147e-21.\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9161:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9162:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9163:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9164:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9165:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9166:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9167:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9168:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 9169:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09171: reducing learning rate of group 0 to 9.0390e-21.\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9170:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9171:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9172:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9173:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9174:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9175:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9176:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9177:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 9178:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09180: reducing learning rate of group 0 to 8.5870e-21.\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9179:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9180:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9181:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9182:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9183:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9184:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9185:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9186:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 9187:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09189: reducing learning rate of group 0 to 8.1577e-21.\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9188:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9189:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9190:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9191:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9192:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9193:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9194:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9195:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 9196:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09198: reducing learning rate of group 0 to 7.7498e-21.\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9197:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9198:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9199:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9200:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9201:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9202:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9203:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9204:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 9205:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09207: reducing learning rate of group 0 to 7.3623e-21.\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9206:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9207:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9208:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9209:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9210:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9211:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9212:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9213:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 9214:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09216: reducing learning rate of group 0 to 6.9942e-21.\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9215:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9216:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9217:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9218:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9219:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9220:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9221:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9222:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 9223:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09225: reducing learning rate of group 0 to 6.6445e-21.\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9224:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9225:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9226:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9227:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9228:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9229:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9230:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9231:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 9232:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09234: reducing learning rate of group 0 to 6.3123e-21.\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9233:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9234:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9235:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9236:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9237:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9238:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9239:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9240:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 9241:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09243: reducing learning rate of group 0 to 5.9967e-21.\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9242:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9243:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9244:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9245:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9246:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9247:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9248:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9249:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 9250:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09252: reducing learning rate of group 0 to 5.6968e-21.\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9251:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9252:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9253:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9254:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9255:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9256:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9257:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9258:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 9259:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09261: reducing learning rate of group 0 to 5.4120e-21.\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9260:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9261:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9262:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9263:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9264:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9265:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9266:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9267:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 9268:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09270: reducing learning rate of group 0 to 5.1414e-21.\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9269:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9270:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9271:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9272:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9273:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9274:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9275:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9276:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 9277:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09279: reducing learning rate of group 0 to 4.8843e-21.\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9278:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9279:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9280:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9281:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9282:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9283:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9284:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9285:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 9286:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09288: reducing learning rate of group 0 to 4.6401e-21.\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9287:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9288:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9289:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9290:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9291:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9292:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9293:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9294:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 9295:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09297: reducing learning rate of group 0 to 4.4081e-21.\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9296:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9297:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9298:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9299:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9300:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9301:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9302:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9303:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 9304:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09306: reducing learning rate of group 0 to 4.1877e-21.\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9305:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9306:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9307:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9308:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9309:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9310:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9311:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9312:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 9313:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09315: reducing learning rate of group 0 to 3.9783e-21.\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9314:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9315:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9316:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9317:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9318:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9319:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9320:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9321:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 9322:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09324: reducing learning rate of group 0 to 3.7794e-21.\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9323:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9324:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9325:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9326:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9327:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9328:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9329:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9330:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 9331:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09333: reducing learning rate of group 0 to 3.5904e-21.\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9332:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9333:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9334:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9335:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9336:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9337:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9338:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9339:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 9340:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09342: reducing learning rate of group 0 to 3.4109e-21.\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9341:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9342:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9343:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9344:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9345:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9346:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9347:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9348:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 9349:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09351: reducing learning rate of group 0 to 3.2404e-21.\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9350:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9351:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9352:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9353:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9354:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9355:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9356:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9357:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 9358:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09360: reducing learning rate of group 0 to 3.0783e-21.\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9359:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9360:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9361:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9362:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9363:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9364:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9365:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9366:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 9367:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09369: reducing learning rate of group 0 to 2.9244e-21.\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9368:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9369:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9370:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9371:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9372:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9373:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9374:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9375:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 9376:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09378: reducing learning rate of group 0 to 2.7782e-21.\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9377:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9378:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9379:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9380:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9381:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9382:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9383:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9384:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 9385:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09387: reducing learning rate of group 0 to 2.6393e-21.\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9386:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9387:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9388:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9389:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9390:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9391:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9392:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9393:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 9394:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09396: reducing learning rate of group 0 to 2.5073e-21.\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9395:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9396:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9397:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9398:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9399:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9400:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9401:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9402:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 9403:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09405: reducing learning rate of group 0 to 2.3820e-21.\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9404:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9405:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9406:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9407:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9408:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9409:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9410:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9411:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 9412:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09414: reducing learning rate of group 0 to 2.2629e-21.\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9413:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9414:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9415:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9416:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9417:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9418:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9419:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9420:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 9421:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09423: reducing learning rate of group 0 to 2.1497e-21.\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9422:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9423:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9424:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9425:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9426:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9427:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9428:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9429:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 9430:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09432: reducing learning rate of group 0 to 2.0422e-21.\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9431:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9432:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9433:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9434:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9435:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9436:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9437:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9438:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 9439:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09441: reducing learning rate of group 0 to 1.9401e-21.\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9440:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9441:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9442:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9443:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9444:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9445:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9446:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9447:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 9448:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09450: reducing learning rate of group 0 to 1.8431e-21.\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9449:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9450:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9451:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9452:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9453:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9454:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9455:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9456:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 9457:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09459: reducing learning rate of group 0 to 1.7510e-21.\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9458:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9459:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9460:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9461:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9462:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9463:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9464:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9465:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 9466:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09468: reducing learning rate of group 0 to 1.6634e-21.\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9467:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9468:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9469:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9470:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9471:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9472:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9473:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9474:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 9475:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09477: reducing learning rate of group 0 to 1.5802e-21.\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9476:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9477:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9478:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9479:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9480:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9481:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9482:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9483:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 9484:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09486: reducing learning rate of group 0 to 1.5012e-21.\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9485:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9486:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9487:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9488:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9489:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9490:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9491:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9492:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9493:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09495: reducing learning rate of group 0 to 1.4262e-21.\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9494:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9495:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9496:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9497:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9498:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9499:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9500:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9501:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9502:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09504: reducing learning rate of group 0 to 1.3549e-21.\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9503:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9504:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9505:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9506:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9507:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9508:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9509:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9510:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9511:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09513: reducing learning rate of group 0 to 1.2871e-21.\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9512:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9513:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9514:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9515:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9516:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9517:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9518:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9519:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9520:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09522: reducing learning rate of group 0 to 1.2228e-21.\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9521:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9522:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9523:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9524:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9525:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9526:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9527:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9528:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9529:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09531: reducing learning rate of group 0 to 1.1616e-21.\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9530:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9531:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9532:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9533:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9534:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9535:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9536:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9537:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9538:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09540: reducing learning rate of group 0 to 1.1035e-21.\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9539:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9540:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9541:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9542:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9543:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9544:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9545:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9546:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9547:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09549: reducing learning rate of group 0 to 1.0484e-21.\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9548:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9549:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9550:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9551:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9552:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9553:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9554:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9555:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9556:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09558: reducing learning rate of group 0 to 9.9594e-22.\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9557:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9558:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9559:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9560:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9561:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9562:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9563:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9564:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9565:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09567: reducing learning rate of group 0 to 9.4615e-22.\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9566:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9567:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9568:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9569:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9570:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9571:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9572:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9573:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9574:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09576: reducing learning rate of group 0 to 8.9884e-22.\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9575:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9576:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9577:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9578:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9579:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9580:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9581:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9582:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9583:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09585: reducing learning rate of group 0 to 8.5390e-22.\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9584:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9585:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9586:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9587:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9588:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9589:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9590:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9591:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9592:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09594: reducing learning rate of group 0 to 8.1120e-22.\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9593:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9594:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9595:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9596:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9597:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9598:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9599:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9600:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9601:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09603: reducing learning rate of group 0 to 7.7064e-22.\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9602:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9603:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9604:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9605:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9606:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9607:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9608:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9609:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9610:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09612: reducing learning rate of group 0 to 7.3211e-22.\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9611:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9612:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9613:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9614:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9615:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9616:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9617:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9618:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9619:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09621: reducing learning rate of group 0 to 6.9551e-22.\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9620:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9621:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9622:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9623:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9624:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9625:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9626:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9627:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9628:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09630: reducing learning rate of group 0 to 6.6073e-22.\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9629:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9630:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9631:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9632:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9633:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9634:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9635:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9636:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9637:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09639: reducing learning rate of group 0 to 6.2769e-22.\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9638:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9639:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9640:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9641:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9642:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9643:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9644:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9645:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9646:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09648: reducing learning rate of group 0 to 5.9631e-22.\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9647:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9648:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9649:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9650:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9651:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9652:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9653:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9654:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9655:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09657: reducing learning rate of group 0 to 5.6649e-22.\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9656:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9657:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9658:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9659:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9660:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9661:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9662:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9663:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9664:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09666: reducing learning rate of group 0 to 5.3817e-22.\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9665:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9666:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9667:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9668:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9669:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9670:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9671:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9672:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9673:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09675: reducing learning rate of group 0 to 5.1126e-22.\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9674:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9675:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9676:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9677:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9678:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9679:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9680:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9681:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9682:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09684: reducing learning rate of group 0 to 4.8570e-22.\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9683:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9684:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9685:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9686:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9687:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9688:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9689:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9690:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9691:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09693: reducing learning rate of group 0 to 4.6141e-22.\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9692:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9693:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9694:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9695:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9696:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9697:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9698:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9699:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9700:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09702: reducing learning rate of group 0 to 4.3834e-22.\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9701:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9702:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9703:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9704:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9705:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9706:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9707:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9708:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9709:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09711: reducing learning rate of group 0 to 4.1642e-22.\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9710:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9711:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9712:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9713:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9714:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9715:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9716:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9717:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9718:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09720: reducing learning rate of group 0 to 3.9560e-22.\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9719:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9720:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9721:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9722:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9723:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9724:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9725:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9726:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9727:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09729: reducing learning rate of group 0 to 3.7582e-22.\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9728:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9729:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9730:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9731:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9732:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9733:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9734:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9735:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9736:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09738: reducing learning rate of group 0 to 3.5703e-22.\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9737:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9738:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9739:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9740:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9741:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9742:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9743:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9744:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9745:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09747: reducing learning rate of group 0 to 3.3918e-22.\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9746:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9747:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9748:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9749:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9750:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9751:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9752:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9753:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9754:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09756: reducing learning rate of group 0 to 3.2222e-22.\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9755:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9756:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9757:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9758:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9759:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9760:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9761:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9762:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9763:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09765: reducing learning rate of group 0 to 3.0611e-22.\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9764:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9765:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9766:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9767:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9768:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9769:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9770:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9771:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9772:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09774: reducing learning rate of group 0 to 2.9080e-22.\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9773:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9774:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9775:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9776:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9777:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9778:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9779:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9780:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9781:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09783: reducing learning rate of group 0 to 2.7626e-22.\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9782:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9783:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9784:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9785:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9786:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9787:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9788:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9789:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9790:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09792: reducing learning rate of group 0 to 2.6245e-22.\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9791:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9792:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9793:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9794:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9795:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9796:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9797:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9798:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9799:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09801: reducing learning rate of group 0 to 2.4933e-22.\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9800:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9801:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9802:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9803:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9804:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9805:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9806:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9807:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9808:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09810: reducing learning rate of group 0 to 2.3686e-22.\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9809:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9810:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9811:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9812:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9813:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9814:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9815:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9816:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9817:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09819: reducing learning rate of group 0 to 2.2502e-22.\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9818:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9819:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9820:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9821:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9822:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9823:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9824:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9825:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9826:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09828: reducing learning rate of group 0 to 2.1377e-22.\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9827:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9828:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9829:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9830:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9831:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9832:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9833:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9834:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9835:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09837: reducing learning rate of group 0 to 2.0308e-22.\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9836:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9837:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9838:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9839:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9840:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9841:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9842:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9843:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9844:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09846: reducing learning rate of group 0 to 1.9293e-22.\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9845:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9846:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9847:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9848:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9849:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9850:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9851:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9852:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9853:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09855: reducing learning rate of group 0 to 1.8328e-22.\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9854:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9855:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9856:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9857:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9858:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9859:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9860:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9861:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9862:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09864: reducing learning rate of group 0 to 1.7412e-22.\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9863:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9864:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9865:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9866:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9867:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9868:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9869:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9870:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9871:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09873: reducing learning rate of group 0 to 1.6541e-22.\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9872:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9873:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9874:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9875:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9876:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9877:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9878:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9879:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9880:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09882: reducing learning rate of group 0 to 1.5714e-22.\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9881:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9882:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9883:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9884:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9885:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9886:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9887:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9888:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9889:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09891: reducing learning rate of group 0 to 1.4928e-22.\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9890:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9891:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9892:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9893:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9894:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9895:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9896:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9897:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9898:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09900: reducing learning rate of group 0 to 1.4182e-22.\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9899:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9900:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9901:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9902:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9903:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9904:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9905:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9906:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9907:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09909: reducing learning rate of group 0 to 1.3473e-22.\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9908:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9909:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9910:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9911:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9912:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9913:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9914:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9915:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9916:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09918: reducing learning rate of group 0 to 1.2799e-22.\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9917:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9918:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9919:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9920:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9921:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9922:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9923:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9924:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9925:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09927: reducing learning rate of group 0 to 1.2159e-22.\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9926:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9927:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9928:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9929:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9930:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9931:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9932:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9933:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9934:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09936: reducing learning rate of group 0 to 1.1551e-22.\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9935:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9936:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9937:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9938:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9939:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9940:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9941:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9942:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9943:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09945: reducing learning rate of group 0 to 1.0974e-22.\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9944:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9945:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9946:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9947:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9948:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9949:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9950:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9951:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9952:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09954: reducing learning rate of group 0 to 1.0425e-22.\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9953:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9954:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9955:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9956:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9957:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9958:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9959:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9960:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9961:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09963: reducing learning rate of group 0 to 9.9037e-23.\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9962:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9963:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9964:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9965:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9966:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9967:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9968:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9969:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9970:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09972: reducing learning rate of group 0 to 9.4085e-23.\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9971:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9972:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9973:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9974:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9975:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9976:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9977:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9978:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9979:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09981: reducing learning rate of group 0 to 8.9381e-23.\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9980:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9981:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9982:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9983:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9984:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9985:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9986:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9987:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9988:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09990: reducing learning rate of group 0 to 8.4912e-23.\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9989:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9990:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9991:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9992:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9993:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9994:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9995:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9996:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9997:\n",
      "train loss: 2.442445147454092e-16\n",
      "Epoch 09999: reducing learning rate of group 0 to 8.0666e-23.\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9998:\n",
      "train loss: 2.442445147454092e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9999:\n",
      "train loss: 2.442445147454092e-16\n"
     ]
    }
   ],
   "source": [
    "Test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
