{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData():\n",
    "    def __init__(self, data_path, nsub=3, length=5):\n",
    "        self.data = torch.load(data_path)\n",
    "        self.nsub = nsub\n",
    "        self.length = length\n",
    "\n",
    "    def getData(self,):\n",
    "        rho = torch.clone(self.data['rho'])\n",
    "        ans = torch.clone(self.data['ans'])\n",
    "        output = torch.clone(self.data['output_a'])\n",
    "        \n",
    "        return (rho, ans, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __int__(self, ):\n",
    "        super(CustomLoss,self).__init__()\n",
    "    def forward(self, a, b):\n",
    "        loss = torch.norm(a-b,p = 'fro')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Predictor,self).__init__()\n",
    "        self.input = nn.Linear(input_dim,output_dim)\n",
    "        self.hidden =  nn.ModuleList([ nn.Sequential( nn.Linear(output_dim, output_dim) ) for i in range(4) ])\n",
    "        self.pred = nn.Linear(output_dim,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x0 = self.input(x)\n",
    "        x = x0\n",
    "        x = x / torch.norm(x)\n",
    "        for layer in self.hidden:\n",
    "            x = self.tanh(layer(x) + x0)\n",
    "            x = x / torch.norm(x)\n",
    "        x = self.pred(x)\n",
    "        x = self.pred(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__() # necessary\n",
    "        self.conv = nn.Conv2d(cin, cout, (3, 3), padding=1)\n",
    "        # self.bn = nn.BatchNorm2d(cout)\n",
    "        # self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,nsub,length):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(1,64),\n",
    "            ConvBlock(64,64),\n",
    "            ConvBlock(64,64),\n",
    "            ConvBlock(64,64),\n",
    "            ConvBlock(64,64),\n",
    "        )\n",
    "\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            Predictor((length**nsub)**2,length*nsub),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # features = self.features(x)\n",
    "        y = self.regression(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.01\n",
      "Epoch 000:\n",
      "train loss: 0.9999210476883259\n",
      "lr: 0.01\n",
      "Epoch 001:\n",
      "train loss: 0.9999839479524231\n",
      "lr: 0.01\n",
      "Epoch 002:\n",
      "train loss: 0.9997117832660655\n",
      "lr: 0.01\n",
      "Epoch 003:\n",
      "train loss: 0.9985071008292374\n",
      "lr: 0.01\n",
      "Epoch 004:\n",
      "train loss: 0.9948215549363022\n",
      "lr: 0.01\n",
      "Epoch 005:\n",
      "train loss: 0.9853308059684515\n",
      "lr: 0.01\n",
      "Epoch 006:\n",
      "train loss: 0.9629633253540503\n",
      "lr: 0.01\n",
      "Epoch 007:\n",
      "train loss: 0.9132382628628949\n",
      "lr: 0.01\n",
      "Epoch 008:\n",
      "train loss: 0.8203938037205513\n",
      "lr: 0.01\n",
      "Epoch 009:\n",
      "train loss: 0.7421777555414035\n",
      "lr: 0.01\n",
      "Epoch 010:\n",
      "train loss: 0.6485674064336671\n",
      "lr: 0.01\n",
      "Epoch 011:\n",
      "train loss: 0.48510640058614646\n",
      "lr: 0.01\n",
      "Epoch 012:\n",
      "train loss: 0.3379050462308718\n",
      "lr: 0.01\n",
      "Epoch 013:\n",
      "train loss: 0.26063687269941477\n",
      "lr: 0.01\n",
      "Epoch 014:\n",
      "train loss: 0.19920713674140525\n",
      "lr: 0.01\n",
      "Epoch 015:\n",
      "train loss: 0.19000004992278202\n",
      "lr: 0.01\n",
      "Epoch 016:\n",
      "train loss: 0.19839083862496706\n",
      "lr: 0.01\n",
      "Epoch 017:\n",
      "train loss: 0.21579638788262323\n",
      "lr: 0.01\n",
      "Epoch 018:\n",
      "train loss: 0.2259403935147001\n",
      "lr: 0.01\n",
      "Epoch 019:\n",
      "train loss: 0.21758252639041284\n",
      "lr: 0.01\n",
      "Epoch 020:\n",
      "train loss: 0.18912500561164922\n",
      "lr: 0.01\n",
      "Epoch 021:\n",
      "train loss: 0.15159893998566504\n",
      "lr: 0.01\n",
      "Epoch 022:\n",
      "train loss: 0.11040291556756562\n",
      "lr: 0.01\n",
      "Epoch 023:\n",
      "train loss: 0.07483201107124027\n",
      "lr: 0.01\n",
      "Epoch 024:\n",
      "train loss: 0.14235590146393584\n",
      "lr: 0.01\n",
      "Epoch 025:\n",
      "train loss: 0.13470951132257578\n",
      "lr: 0.01\n",
      "Epoch 026:\n",
      "train loss: 0.15854616422992035\n",
      "lr: 0.01\n",
      "Epoch 027:\n",
      "train loss: 0.13758958350430003\n",
      "lr: 0.01\n",
      "Epoch 028:\n",
      "train loss: 0.13730364500240597\n",
      "lr: 0.01\n",
      "Epoch 029:\n",
      "train loss: 0.10397763195966289\n",
      "lr: 0.01\n",
      "Epoch 030:\n",
      "train loss: 0.08770838321281645\n",
      "lr: 0.01\n",
      "Epoch 031:\n",
      "train loss: 0.09430717160754855\n",
      "lr: 0.01\n",
      "Epoch 032:\n",
      "train loss: 0.056016227621057756\n",
      "lr: 0.01\n",
      "Epoch 033:\n",
      "train loss: 0.05354731293482705\n",
      "lr: 0.01\n",
      "Epoch 034:\n",
      "train loss: 0.0668796528661961\n",
      "lr: 0.01\n",
      "Epoch 035:\n",
      "train loss: 0.07385567506428051\n",
      "lr: 0.01\n",
      "Epoch 036:\n",
      "train loss: 0.0636517547661416\n",
      "lr: 0.01\n",
      "Epoch 037:\n",
      "train loss: 0.04493362047469113\n",
      "lr: 0.01\n",
      "Epoch 038:\n",
      "train loss: 0.026773133654976793\n",
      "lr: 0.01\n",
      "Epoch 039:\n",
      "train loss: 0.06437995327966527\n",
      "lr: 0.01\n",
      "Epoch 040:\n",
      "train loss: 0.08790883271859307\n",
      "lr: 0.01\n",
      "Epoch 041:\n",
      "train loss: 0.06105885719864013\n",
      "lr: 0.01\n",
      "Epoch 042:\n",
      "train loss: 0.07649984192263395\n",
      "lr: 0.01\n",
      "Epoch 043:\n",
      "train loss: 0.059662701441396966\n",
      "lr: 0.01\n",
      "Epoch 044:\n",
      "train loss: 0.036893513452428764\n",
      "lr: 0.01\n",
      "Epoch 045:\n",
      "train loss: 0.022128628909697152\n",
      "lr: 0.01\n",
      "Epoch 046:\n",
      "train loss: 0.0748522712543837\n",
      "lr: 0.01\n",
      "Epoch 047:\n",
      "train loss: 0.05646132294632057\n",
      "lr: 0.01\n",
      "Epoch 048:\n",
      "train loss: 0.05487688129221437\n",
      "lr: 0.01\n",
      "Epoch 049:\n",
      "train loss: 0.06936749909575683\n",
      "lr: 0.01\n",
      "Epoch 050:\n",
      "train loss: 0.037447470645780655\n",
      "lr: 0.01\n",
      "Epoch 051:\n",
      "train loss: 0.021466753278549217\n",
      "lr: 0.01\n",
      "Epoch 052:\n",
      "train loss: 0.06796021632985659\n",
      "lr: 0.01\n",
      "Epoch 053:\n",
      "train loss: 0.06351271297294094\n",
      "lr: 0.01\n",
      "Epoch 054:\n",
      "train loss: 0.044348094539892594\n",
      "lr: 0.01\n",
      "Epoch 055:\n",
      "train loss: 0.09072280776372309\n",
      "lr: 0.01\n",
      "Epoch 056:\n",
      "train loss: 0.035502567783963435\n",
      "lr: 0.01\n",
      "Epoch 057:\n",
      "train loss: 0.06214960186714767\n",
      "lr: 0.01\n",
      "Epoch 058:\n",
      "train loss: 0.04547928683855973\n",
      "lr: 0.01\n",
      "Epoch 059:\n",
      "train loss: 0.03931545880893233\n",
      "Epoch 00061: reducing learning rate of group 0 to 9.5000e-03.\n",
      "lr: 0.0095\n",
      "Epoch 060:\n",
      "train loss: 0.041259336461470716\n",
      "lr: 0.0095\n",
      "Epoch 061:\n",
      "train loss: 0.045335466093484114\n",
      "lr: 0.0095\n",
      "Epoch 062:\n",
      "train loss: 0.034220665315578255\n",
      "lr: 0.0095\n",
      "Epoch 063:\n",
      "train loss: 0.03757067493583531\n",
      "lr: 0.0095\n",
      "Epoch 064:\n",
      "train loss: 0.046234275429015204\n",
      "lr: 0.0095\n",
      "Epoch 065:\n",
      "train loss: 0.03620763782131888\n",
      "lr: 0.0095\n",
      "Epoch 066:\n",
      "train loss: 0.029697591249678086\n",
      "lr: 0.0095\n",
      "Epoch 067:\n",
      "train loss: 0.04356315263290526\n",
      "lr: 0.0095\n",
      "Epoch 068:\n",
      "train loss: 0.032493553005042385\n",
      "Epoch 00070: reducing learning rate of group 0 to 9.0250e-03.\n",
      "lr: 0.009025\n",
      "Epoch 069:\n",
      "train loss: 0.036304985624259524\n",
      "lr: 0.009025\n",
      "Epoch 070:\n",
      "train loss: 0.04012644070101351\n",
      "lr: 0.009025\n",
      "Epoch 071:\n",
      "train loss: 0.03196223883761276\n",
      "lr: 0.009025\n",
      "Epoch 072:\n",
      "train loss: 0.02607712347231839\n",
      "lr: 0.009025\n",
      "Epoch 073:\n",
      "train loss: 0.04540049964211867\n",
      "lr: 0.009025\n",
      "Epoch 074:\n",
      "train loss: 0.017161030687852873\n",
      "lr: 0.009025\n",
      "Epoch 075:\n",
      "train loss: 0.029863252163428222\n",
      "lr: 0.009025\n",
      "Epoch 076:\n",
      "train loss: 0.05497578110503507\n",
      "lr: 0.009025\n",
      "Epoch 077:\n",
      "train loss: 0.01770093040837232\n",
      "lr: 0.009025\n",
      "Epoch 078:\n",
      "train loss: 0.0450872161539844\n",
      "lr: 0.009025\n",
      "Epoch 079:\n",
      "train loss: 0.04208780405595172\n",
      "lr: 0.009025\n",
      "Epoch 080:\n",
      "train loss: 0.02007062287722318\n",
      "lr: 0.009025\n",
      "Epoch 081:\n",
      "train loss: 0.04520185117607374\n",
      "lr: 0.009025\n",
      "Epoch 082:\n",
      "train loss: 0.04907587745988539\n",
      "Epoch 00084: reducing learning rate of group 0 to 8.5737e-03.\n",
      "lr: 0.00857375\n",
      "Epoch 083:\n",
      "train loss: 0.039392700281736096\n",
      "lr: 0.00857375\n",
      "Epoch 084:\n",
      "train loss: 0.04643352091185337\n",
      "lr: 0.00857375\n",
      "Epoch 085:\n",
      "train loss: 0.028852706526341917\n",
      "lr: 0.00857375\n",
      "Epoch 086:\n",
      "train loss: 0.034297428427119324\n",
      "lr: 0.00857375\n",
      "Epoch 087:\n",
      "train loss: 0.043030644293794135\n",
      "lr: 0.00857375\n",
      "Epoch 088:\n",
      "train loss: 0.021368158319831982\n",
      "lr: 0.00857375\n",
      "Epoch 089:\n",
      "train loss: 0.01865641432861648\n",
      "lr: 0.00857375\n",
      "Epoch 090:\n",
      "train loss: 0.050383439129683585\n",
      "lr: 0.00857375\n",
      "Epoch 091:\n",
      "train loss: 0.027617451656622737\n",
      "Epoch 00093: reducing learning rate of group 0 to 8.1451e-03.\n",
      "lr: 0.0081450625\n",
      "Epoch 092:\n",
      "train loss: 0.04164025432387765\n",
      "lr: 0.0081450625\n",
      "Epoch 093:\n",
      "train loss: 0.03943912408685008\n",
      "lr: 0.0081450625\n",
      "Epoch 094:\n",
      "train loss: 0.01954858229462491\n",
      "lr: 0.0081450625\n",
      "Epoch 095:\n",
      "train loss: 0.04316774251212776\n",
      "lr: 0.0081450625\n",
      "Epoch 096:\n",
      "train loss: 0.02941529621895181\n",
      "lr: 0.0081450625\n",
      "Epoch 097:\n",
      "train loss: 0.01729337267280171\n",
      "lr: 0.0081450625\n",
      "Epoch 098:\n",
      "train loss: 0.01573047357763218\n",
      "lr: 0.0081450625\n",
      "Epoch 099:\n",
      "train loss: 0.06602594500900989\n",
      "lr: 0.0081450625\n",
      "Epoch 100:\n",
      "train loss: 0.030660313263487254\n",
      "lr: 0.0081450625\n",
      "Epoch 101:\n",
      "train loss: 0.09155084225413364\n",
      "lr: 0.0081450625\n",
      "Epoch 102:\n",
      "train loss: 0.10038508555160813\n",
      "lr: 0.0081450625\n",
      "Epoch 103:\n",
      "train loss: 0.022225886074874558\n",
      "lr: 0.0081450625\n",
      "Epoch 104:\n",
      "train loss: 0.156739667760832\n",
      "lr: 0.0081450625\n",
      "Epoch 105:\n",
      "train loss: 0.19220007481970966\n",
      "lr: 0.0081450625\n",
      "Epoch 106:\n",
      "train loss: 0.08877204110223567\n",
      "Epoch 00108: reducing learning rate of group 0 to 7.7378e-03.\n",
      "lr: 0.007737809374999999\n",
      "Epoch 107:\n",
      "train loss: 0.10110827776973133\n",
      "lr: 0.007737809374999999\n",
      "Epoch 108:\n",
      "train loss: 0.17101226969857025\n",
      "lr: 0.007737809374999999\n",
      "Epoch 109:\n",
      "train loss: 0.16094134797487628\n",
      "lr: 0.007737809374999999\n",
      "Epoch 110:\n",
      "train loss: 0.07986308749190377\n",
      "lr: 0.007737809374999999\n",
      "Epoch 111:\n",
      "train loss: 0.09050067658716553\n",
      "lr: 0.007737809374999999\n",
      "Epoch 112:\n",
      "train loss: 0.14544416435634114\n",
      "lr: 0.007737809374999999\n",
      "Epoch 113:\n",
      "train loss: 0.07841347541776712\n",
      "lr: 0.007737809374999999\n",
      "Epoch 114:\n",
      "train loss: 0.07515367967506648\n",
      "lr: 0.007737809374999999\n",
      "Epoch 115:\n",
      "train loss: 0.12231562336965413\n",
      "Epoch 00117: reducing learning rate of group 0 to 7.3509e-03.\n",
      "lr: 0.007350918906249998\n",
      "Epoch 116:\n",
      "train loss: 0.09096802549761236\n",
      "lr: 0.007350918906249998\n",
      "Epoch 117:\n",
      "train loss: 0.02136764011190877\n",
      "lr: 0.007350918906249998\n",
      "Epoch 118:\n",
      "train loss: 0.03464566412835134\n",
      "lr: 0.007350918906249998\n",
      "Epoch 119:\n",
      "train loss: 0.04178671910678811\n",
      "lr: 0.007350918906249998\n",
      "Epoch 120:\n",
      "train loss: 0.02972114832509098\n",
      "lr: 0.007350918906249998\n",
      "Epoch 121:\n",
      "train loss: 0.06463296042177892\n",
      "lr: 0.007350918906249998\n",
      "Epoch 122:\n",
      "train loss: 0.05632750041473081\n",
      "lr: 0.007350918906249998\n",
      "Epoch 123:\n",
      "train loss: 0.0405222430033221\n",
      "lr: 0.007350918906249998\n",
      "Epoch 124:\n",
      "train loss: 0.0458685440641083\n",
      "Epoch 00126: reducing learning rate of group 0 to 6.9834e-03.\n",
      "lr: 0.006983372960937498\n",
      "Epoch 125:\n",
      "train loss: 0.02916187995886081\n",
      "lr: 0.006983372960937498\n",
      "Epoch 126:\n",
      "train loss: 0.010995991085171459\n",
      "lr: 0.006983372960937498\n",
      "Epoch 127:\n",
      "train loss: 0.08005204006303615\n",
      "lr: 0.006983372960937498\n",
      "Epoch 128:\n",
      "train loss: 0.09055952388453511\n",
      "lr: 0.006983372960937498\n",
      "Epoch 129:\n",
      "train loss: 0.033283946564788884\n",
      "lr: 0.006983372960937498\n",
      "Epoch 130:\n",
      "train loss: 0.10040949389449742\n",
      "lr: 0.006983372960937498\n",
      "Epoch 131:\n",
      "train loss: 0.1331189951753391\n",
      "lr: 0.006983372960937498\n",
      "Epoch 132:\n",
      "train loss: 0.06739991030930725\n",
      "lr: 0.006983372960937498\n",
      "Epoch 133:\n",
      "train loss: 0.06937271514405909\n",
      "lr: 0.006983372960937498\n",
      "Epoch 134:\n",
      "train loss: 0.11498241730673275\n",
      "Epoch 00136: reducing learning rate of group 0 to 6.6342e-03.\n",
      "lr: 0.006634204312890623\n",
      "Epoch 135:\n",
      "train loss: 0.09352321555124986\n",
      "lr: 0.006634204312890623\n",
      "Epoch 136:\n",
      "train loss: 0.00968786965409688\n",
      "lr: 0.006634204312890623\n",
      "Epoch 137:\n",
      "train loss: 0.12453036266078338\n",
      "lr: 0.006634204312890623\n",
      "Epoch 138:\n",
      "train loss: 0.16039541099744617\n",
      "lr: 0.006634204312890623\n",
      "Epoch 139:\n",
      "train loss: 0.10274664202135896\n",
      "lr: 0.006634204312890623\n",
      "Epoch 140:\n",
      "train loss: 0.042292438399668904\n",
      "lr: 0.006634204312890623\n",
      "Epoch 141:\n",
      "train loss: 0.08887822958557454\n",
      "lr: 0.006634204312890623\n",
      "Epoch 142:\n",
      "train loss: 0.07991014233262246\n",
      "lr: 0.006634204312890623\n",
      "Epoch 143:\n",
      "train loss: 0.032391916080887484\n",
      "lr: 0.006634204312890623\n",
      "Epoch 144:\n",
      "train loss: 0.07655357853395865\n",
      "Epoch 00146: reducing learning rate of group 0 to 6.3025e-03.\n",
      "lr: 0.006302494097246091\n",
      "Epoch 145:\n",
      "train loss: 0.07158581039558934\n",
      "lr: 0.006302494097246091\n",
      "Epoch 146:\n",
      "train loss: 0.030026883940004273\n",
      "lr: 0.006302494097246091\n",
      "Epoch 147:\n",
      "train loss: 0.0554432965318237\n",
      "lr: 0.006302494097246091\n",
      "Epoch 148:\n",
      "train loss: 0.03242760487301631\n",
      "lr: 0.006302494097246091\n",
      "Epoch 149:\n",
      "train loss: 0.057446355952989206\n",
      "lr: 0.006302494097246091\n",
      "Epoch 150:\n",
      "train loss: 0.06732170945724343\n",
      "lr: 0.006302494097246091\n",
      "Epoch 151:\n",
      "train loss: 0.014308790547343233\n",
      "lr: 0.006302494097246091\n",
      "Epoch 152:\n",
      "train loss: 0.06328551726612729\n",
      "lr: 0.006302494097246091\n",
      "Epoch 153:\n",
      "train loss: 0.06957164752100363\n",
      "Epoch 00155: reducing learning rate of group 0 to 5.9874e-03.\n",
      "lr: 0.005987369392383786\n",
      "Epoch 154:\n",
      "train loss: 0.04614002630811731\n",
      "lr: 0.005987369392383786\n",
      "Epoch 155:\n",
      "train loss: 0.05726236921266508\n",
      "lr: 0.005987369392383786\n",
      "Epoch 156:\n",
      "train loss: 0.03745440535191419\n",
      "lr: 0.005987369392383786\n",
      "Epoch 157:\n",
      "train loss: 0.0474656950953086\n",
      "lr: 0.005987369392383786\n",
      "Epoch 158:\n",
      "train loss: 0.06529252660449078\n",
      "lr: 0.005987369392383786\n",
      "Epoch 159:\n",
      "train loss: 0.03747196450395433\n",
      "lr: 0.005987369392383786\n",
      "Epoch 160:\n",
      "train loss: 0.04840857141695318\n",
      "lr: 0.005987369392383786\n",
      "Epoch 161:\n",
      "train loss: 0.04763543678345366\n",
      "lr: 0.005987369392383786\n",
      "Epoch 162:\n",
      "train loss: 0.03464873204974453\n",
      "Epoch 00164: reducing learning rate of group 0 to 5.6880e-03.\n",
      "lr: 0.005688000922764597\n",
      "Epoch 163:\n",
      "train loss: 0.04544283756689962\n",
      "lr: 0.005688000922764597\n",
      "Epoch 164:\n",
      "train loss: 0.013372027382295999\n",
      "lr: 0.005688000922764597\n",
      "Epoch 165:\n",
      "train loss: 0.05409286281757419\n",
      "lr: 0.005688000922764597\n",
      "Epoch 166:\n",
      "train loss: 0.04831547694484743\n",
      "lr: 0.005688000922764597\n",
      "Epoch 167:\n",
      "train loss: 0.032646117419255376\n",
      "lr: 0.005688000922764597\n",
      "Epoch 168:\n",
      "train loss: 0.041414222756603856\n",
      "lr: 0.005688000922764597\n",
      "Epoch 169:\n",
      "train loss: 0.01516727278767369\n",
      "lr: 0.005688000922764597\n",
      "Epoch 170:\n",
      "train loss: 0.03750700979051323\n",
      "lr: 0.005688000922764597\n",
      "Epoch 171:\n",
      "train loss: 0.01920208020276949\n",
      "Epoch 00173: reducing learning rate of group 0 to 5.4036e-03.\n",
      "lr: 0.005403600876626367\n",
      "Epoch 172:\n",
      "train loss: 0.04371279920827022\n",
      "lr: 0.005403600876626367\n",
      "Epoch 173:\n",
      "train loss: 0.03280014686854379\n",
      "lr: 0.005403600876626367\n",
      "Epoch 174:\n",
      "train loss: 0.03151733665807816\n",
      "lr: 0.005403600876626367\n",
      "Epoch 175:\n",
      "train loss: 0.031730833657319386\n",
      "lr: 0.005403600876626367\n",
      "Epoch 176:\n",
      "train loss: 0.028878718666043114\n",
      "lr: 0.005403600876626367\n",
      "Epoch 177:\n",
      "train loss: 0.026538304401882845\n",
      "lr: 0.005403600876626367\n",
      "Epoch 178:\n",
      "train loss: 0.029056374274918438\n",
      "lr: 0.005403600876626367\n",
      "Epoch 179:\n",
      "train loss: 0.022252439777219152\n",
      "lr: 0.005403600876626367\n",
      "Epoch 180:\n",
      "train loss: 0.039869102678615834\n",
      "Epoch 00182: reducing learning rate of group 0 to 5.1334e-03.\n",
      "lr: 0.005133420832795048\n",
      "Epoch 181:\n",
      "train loss: 0.03898276973311822\n",
      "lr: 0.005133420832795048\n",
      "Epoch 182:\n",
      "train loss: 0.015519416755955381\n",
      "lr: 0.005133420832795048\n",
      "Epoch 183:\n",
      "train loss: 0.009629144917782\n",
      "lr: 0.005133420832795048\n",
      "Epoch 184:\n",
      "train loss: 0.04414906613279625\n",
      "lr: 0.005133420832795048\n",
      "Epoch 185:\n",
      "train loss: 0.04141051107118002\n",
      "lr: 0.005133420832795048\n",
      "Epoch 186:\n",
      "train loss: 0.01516255908765683\n",
      "lr: 0.005133420832795048\n",
      "Epoch 187:\n",
      "train loss: 0.017036957157994897\n",
      "lr: 0.005133420832795048\n",
      "Epoch 188:\n",
      "train loss: 0.03314345873318232\n",
      "lr: 0.005133420832795048\n",
      "Epoch 189:\n",
      "train loss: 0.028299796788356243\n",
      "lr: 0.005133420832795048\n",
      "Epoch 190:\n",
      "train loss: 0.028531943245993844\n",
      "lr: 0.005133420832795048\n",
      "Epoch 191:\n",
      "train loss: 0.02366006831585979\n",
      "Epoch 00193: reducing learning rate of group 0 to 4.8767e-03.\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 192:\n",
      "train loss: 0.03406768182490597\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 193:\n",
      "train loss: 0.03499842728145284\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 194:\n",
      "train loss: 0.014585112090937808\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 195:\n",
      "train loss: 0.011900951447315055\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 196:\n",
      "train loss: 0.03780229309249792\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 197:\n",
      "train loss: 0.03525764139706313\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 198:\n",
      "train loss: 0.01904171284896819\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 199:\n",
      "train loss: 0.018821835756897377\n",
      "lr: 0.0048767497911552955\n",
      "Epoch 200:\n",
      "train loss: 0.029422133049078154\n",
      "Epoch 00202: reducing learning rate of group 0 to 4.6329e-03.\n",
      "lr: 0.00463291230159753\n",
      "Epoch 201:\n",
      "train loss: 0.02666156260149807\n",
      "lr: 0.00463291230159753\n",
      "Epoch 202:\n",
      "train loss: 0.026701839062980005\n",
      "lr: 0.00463291230159753\n",
      "Epoch 203:\n",
      "train loss: 0.02165719396212312\n",
      "lr: 0.00463291230159753\n",
      "Epoch 204:\n",
      "train loss: 0.02891502519221265\n",
      "lr: 0.00463291230159753\n",
      "Epoch 205:\n",
      "train loss: 0.029587347214193514\n",
      "lr: 0.00463291230159753\n",
      "Epoch 206:\n",
      "train loss: 0.019489001874616756\n",
      "lr: 0.00463291230159753\n",
      "Epoch 207:\n",
      "train loss: 0.016758458971784188\n",
      "lr: 0.00463291230159753\n",
      "Epoch 208:\n",
      "train loss: 0.030002692052085316\n",
      "lr: 0.00463291230159753\n",
      "Epoch 209:\n",
      "train loss: 0.028134806795022938\n",
      "Epoch 00211: reducing learning rate of group 0 to 4.4013e-03.\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 210:\n",
      "train loss: 0.02320310568015209\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 211:\n",
      "train loss: 0.020570616031835614\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 212:\n",
      "train loss: 0.024097971344100702\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 213:\n",
      "train loss: 0.022140852427775498\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 214:\n",
      "train loss: 0.025367823672193427\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 215:\n",
      "train loss: 0.02075427759189877\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 216:\n",
      "train loss: 0.026464450556229636\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 217:\n",
      "train loss: 0.026084794661661678\n",
      "lr: 0.0044012666865176535\n",
      "Epoch 218:\n",
      "train loss: 0.020439192903271255\n",
      "Epoch 00220: reducing learning rate of group 0 to 4.1812e-03.\n",
      "lr: 0.004181203352191771\n",
      "Epoch 219:\n",
      "train loss: 0.017519217741661226\n",
      "lr: 0.004181203352191771\n",
      "Epoch 220:\n",
      "train loss: 0.027229403572384497\n",
      "lr: 0.004181203352191771\n",
      "Epoch 221:\n",
      "train loss: 0.025223755929407326\n",
      "lr: 0.004181203352191771\n",
      "Epoch 222:\n",
      "train loss: 0.020587310370283603\n",
      "lr: 0.004181203352191771\n",
      "Epoch 223:\n",
      "train loss: 0.018801698774423378\n",
      "lr: 0.004181203352191771\n",
      "Epoch 224:\n",
      "train loss: 0.023046656970407857\n",
      "lr: 0.004181203352191771\n",
      "Epoch 225:\n",
      "train loss: 0.020546049178027354\n",
      "lr: 0.004181203352191771\n",
      "Epoch 226:\n",
      "train loss: 0.024424727465133297\n",
      "lr: 0.004181203352191771\n",
      "Epoch 227:\n",
      "train loss: 0.020671267159080707\n",
      "Epoch 00229: reducing learning rate of group 0 to 3.9721e-03.\n",
      "lr: 0.003972143184582182\n",
      "Epoch 228:\n",
      "train loss: 0.02381753800969778\n",
      "lr: 0.003972143184582182\n",
      "Epoch 229:\n",
      "train loss: 0.02322389538281117\n",
      "lr: 0.003972143184582182\n",
      "Epoch 230:\n",
      "train loss: 0.018045251652998046\n",
      "lr: 0.003972143184582182\n",
      "Epoch 231:\n",
      "train loss: 0.01507823762877022\n",
      "lr: 0.003972143184582182\n",
      "Epoch 232:\n",
      "train loss: 0.025778651888506202\n",
      "lr: 0.003972143184582182\n",
      "Epoch 233:\n",
      "train loss: 0.024498590116517685\n",
      "lr: 0.003972143184582182\n",
      "Epoch 234:\n",
      "train loss: 0.017681365754319608\n",
      "lr: 0.003972143184582182\n",
      "Epoch 235:\n",
      "train loss: 0.015734845392758632\n",
      "lr: 0.003972143184582182\n",
      "Epoch 236:\n",
      "train loss: 0.024009454278911324\n",
      "Epoch 00238: reducing learning rate of group 0 to 3.7735e-03.\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 237:\n",
      "train loss: 0.022100550961910933\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 238:\n",
      "train loss: 0.020268578924187996\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 239:\n",
      "train loss: 0.017545292005164623\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 240:\n",
      "train loss: 0.021281768270389374\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 241:\n",
      "train loss: 0.020174781963147072\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 242:\n",
      "train loss: 0.019188225619787558\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 243:\n",
      "train loss: 0.015984592363689404\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 244:\n",
      "train loss: 0.023171681190254447\n",
      "lr: 0.0037735360253530726\n",
      "Epoch 245:\n",
      "train loss: 0.022404140369146657\n",
      "Epoch 00247: reducing learning rate of group 0 to 3.5849e-03.\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 246:\n",
      "train loss: 0.016814572629731933\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 247:\n",
      "train loss: 0.014389165424134365\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 248:\n",
      "train loss: 0.02180867790057534\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 249:\n",
      "train loss: 0.020466566138883473\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 250:\n",
      "train loss: 0.017252208075186213\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 251:\n",
      "train loss: 0.014901798455441675\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 252:\n",
      "train loss: 0.021391302379348557\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 253:\n",
      "train loss: 0.020215089268824284\n",
      "lr: 0.0035848592240854188\n",
      "Epoch 254:\n",
      "train loss: 0.0172045328648165\n",
      "Epoch 00256: reducing learning rate of group 0 to 3.4056e-03.\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 255:\n",
      "train loss: 0.014614120775159514\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 256:\n",
      "train loss: 0.021853293356174516\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 257:\n",
      "train loss: 0.02095787375295512\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 258:\n",
      "train loss: 0.014709450503252416\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 259:\n",
      "train loss: 0.013080553618850764\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 260:\n",
      "train loss: 0.020451624971075874\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 261:\n",
      "train loss: 0.01895060193869306\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 262:\n",
      "train loss: 0.016898686632402418\n",
      "lr: 0.0034056162628811476\n",
      "Epoch 263:\n",
      "train loss: 0.014424168590541376\n",
      "Epoch 00265: reducing learning rate of group 0 to 3.2353e-03.\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 264:\n",
      "train loss: 0.020124591777936778\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 265:\n",
      "train loss: 0.01942231419739697\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 266:\n",
      "train loss: 0.014253628223860034\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 267:\n",
      "train loss: 0.012339660022172488\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 268:\n",
      "train loss: 0.019689160346011824\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 269:\n",
      "train loss: 0.018649518324233415\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 270:\n",
      "train loss: 0.015273897553953504\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 271:\n",
      "train loss: 0.013076658860303746\n",
      "lr: 0.0032353354497370902\n",
      "Epoch 272:\n",
      "train loss: 0.01924216879072674\n",
      "Epoch 00274: reducing learning rate of group 0 to 3.0736e-03.\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 273:\n",
      "train loss: 0.01848210373045693\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 274:\n",
      "train loss: 0.015121127189943216\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 275:\n",
      "train loss: 0.012846584824448868\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 276:\n",
      "train loss: 0.017853767960115185\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 277:\n",
      "train loss: 0.017129283447502332\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 278:\n",
      "train loss: 0.01477781836418497\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 279:\n",
      "train loss: 0.012434882578982096\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 280:\n",
      "train loss: 0.01832286643649127\n",
      "lr: 0.0030735686772502355\n",
      "Epoch 281:\n",
      "train loss: 0.017653740838811275\n",
      "Epoch 00283: reducing learning rate of group 0 to 2.9199e-03.\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 282:\n",
      "train loss: 0.014238162160501755\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 283:\n",
      "train loss: 0.012182671076391007\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 284:\n",
      "train loss: 0.01672351780110318\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 285:\n",
      "train loss: 0.015872465084915244\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 286:\n",
      "train loss: 0.0143154398856329\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 287:\n",
      "train loss: 0.01195286373201741\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 288:\n",
      "train loss: 0.017453564826995623\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 289:\n",
      "train loss: 0.016971732163537617\n",
      "lr: 0.0029198902433877237\n",
      "Epoch 290:\n",
      "train loss: 0.013051033171264234\n",
      "Epoch 00292: reducing learning rate of group 0 to 2.7739e-03.\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 291:\n",
      "train loss: 0.011139357251485104\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 292:\n",
      "train loss: 0.017664790523740405\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 293:\n",
      "train loss: 0.016860335332049114\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 294:\n",
      "train loss: 0.012243697325703358\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 295:\n",
      "train loss: 0.011003998820203734\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 296:\n",
      "train loss: 0.01572412906386685\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 297:\n",
      "train loss: 0.01462752794971388\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 298:\n",
      "train loss: 0.014026010211262877\n",
      "lr: 0.0027738957312183374\n",
      "Epoch 299:\n",
      "train loss: 0.011515552620117563\n",
      "Epoch 00301: reducing learning rate of group 0 to 2.6352e-03.\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 300:\n",
      "train loss: 0.01667920857355369\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 301:\n",
      "train loss: 0.01660264932612442\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 302:\n",
      "train loss: 0.01055072030339515\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 303:\n",
      "train loss: 0.009511461459658485\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 304:\n",
      "train loss: 0.015576463692111141\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 305:\n",
      "train loss: 0.014532807231842429\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 306:\n",
      "train loss: 0.012887389186973854\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 307:\n",
      "train loss: 0.010693466239981053\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 308:\n",
      "train loss: 0.01574104495869531\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 309:\n",
      "train loss: 0.015634929040504477\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 310:\n",
      "train loss: 0.011330947463015398\n",
      "lr: 0.0026352009446574203\n",
      "Epoch 311:\n",
      "train loss: 0.00960140622571503\n",
      "Epoch 00313: reducing learning rate of group 0 to 2.5034e-03.\n",
      "lr: 0.002503440897424549\n",
      "Epoch 312:\n",
      "train loss: 0.016103466743393673\n",
      "lr: 0.002503440897424549\n",
      "Epoch 313:\n",
      "train loss: 0.015555411048546162\n",
      "lr: 0.002503440897424549\n",
      "Epoch 314:\n",
      "train loss: 0.010874143657258447\n",
      "lr: 0.002503440897424549\n",
      "Epoch 315:\n",
      "train loss: 0.009893900935738026\n",
      "lr: 0.002503440897424549\n",
      "Epoch 316:\n",
      "train loss: 0.013732946405647855\n",
      "lr: 0.002503440897424549\n",
      "Epoch 317:\n",
      "train loss: 0.01290203109877703\n",
      "lr: 0.002503440897424549\n",
      "Epoch 318:\n",
      "train loss: 0.012725817133238388\n",
      "lr: 0.002503440897424549\n",
      "Epoch 319:\n",
      "train loss: 0.009878781997214188\n",
      "lr: 0.002503440897424549\n",
      "Epoch 320:\n",
      "train loss: 0.015852105205608317\n",
      "Epoch 00322: reducing learning rate of group 0 to 2.3783e-03.\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 321:\n",
      "train loss: 0.01635536968914952\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 322:\n",
      "train loss: 0.00948134713557868\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 323:\n",
      "train loss: 0.009101687110291464\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 324:\n",
      "train loss: 0.012487734486675636\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 325:\n",
      "train loss: 0.011325902021346658\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 326:\n",
      "train loss: 0.012814118031580838\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 327:\n",
      "train loss: 0.009426674372195545\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 328:\n",
      "train loss: 0.015772338973483613\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 329:\n",
      "train loss: 0.01688917227206004\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 330:\n",
      "train loss: 0.007796570130301613\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 331:\n",
      "train loss: 0.009370172071964791\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 332:\n",
      "train loss: 0.010512107886737728\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 333:\n",
      "train loss: 0.008917972629563647\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 334:\n",
      "train loss: 0.012882506160921523\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 335:\n",
      "train loss: 0.006841705362293398\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 336:\n",
      "train loss: 0.0211142973454285\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 337:\n",
      "train loss: 0.024178458443925712\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 338:\n",
      "train loss: 0.0070298753202641985\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 339:\n",
      "train loss: 0.028020203840126295\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 340:\n",
      "train loss: 0.0343402601239642\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 341:\n",
      "train loss: 0.017540145172436346\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 342:\n",
      "train loss: 0.02246640651626663\n",
      "lr: 0.0023782688525533216\n",
      "Epoch 343:\n",
      "train loss: 0.033352567301421936\n",
      "Epoch 00345: reducing learning rate of group 0 to 2.2594e-03.\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 344:\n",
      "train loss: 0.02244955906838108\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 345:\n",
      "train loss: 0.009536970172105377\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 346:\n",
      "train loss: 0.015710488267070354\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 347:\n",
      "train loss: 0.0038920858055290133\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 348:\n",
      "train loss: 0.010702504900307136\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 349:\n",
      "train loss: 0.006575460409768839\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 350:\n",
      "train loss: 0.007948316065951186\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 351:\n",
      "train loss: 0.007613257284052595\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 352:\n",
      "train loss: 0.005144070360296361\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 353:\n",
      "train loss: 0.00570235342597129\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 354:\n",
      "train loss: 0.011188741709338505\n",
      "lr: 0.0022593554099256553\n",
      "Epoch 355:\n",
      "train loss: 0.00757182825017558\n",
      "Epoch 00357: reducing learning rate of group 0 to 2.1464e-03.\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 356:\n",
      "train loss: 0.01464863432234691\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 357:\n",
      "train loss: 0.009879982508177778\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 358:\n",
      "train loss: 0.014835330384881695\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 359:\n",
      "train loss: 0.01727354791075951\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 360:\n",
      "train loss: 0.004438249245300138\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 361:\n",
      "train loss: 0.013326689626615354\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 362:\n",
      "train loss: 0.0076619726822717464\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 363:\n",
      "train loss: 0.015766262806234912\n",
      "lr: 0.0021463876394293723\n",
      "Epoch 364:\n",
      "train loss: 0.0127596222668279\n",
      "Epoch 00366: reducing learning rate of group 0 to 2.0391e-03.\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 365:\n",
      "train loss: 0.009504567819135367\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 366:\n",
      "train loss: 0.009527073922805952\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 367:\n",
      "train loss: 0.010583729879685842\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 368:\n",
      "train loss: 0.008674751669830045\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 369:\n",
      "train loss: 0.011810623023653237\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 370:\n",
      "train loss: 0.010907963233261373\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 371:\n",
      "train loss: 0.010176831066903216\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 372:\n",
      "train loss: 0.009444005818887765\n",
      "lr: 0.0020390682574579037\n",
      "Epoch 373:\n",
      "train loss: 0.010034113657331879\n",
      "Epoch 00375: reducing learning rate of group 0 to 1.9371e-03.\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 374:\n",
      "train loss: 0.008301153853180148\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 375:\n",
      "train loss: 0.012740516841294399\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 376:\n",
      "train loss: 0.012002346548737372\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 377:\n",
      "train loss: 0.006744327082416277\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 378:\n",
      "train loss: 0.005327866363756858\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 379:\n",
      "train loss: 0.013969392298472143\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 380:\n",
      "train loss: 0.012936082429022071\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 381:\n",
      "train loss: 0.0062702721775585975\n",
      "lr: 0.0019371148445850085\n",
      "Epoch 382:\n",
      "train loss: 0.00524574262374246\n",
      "Epoch 00384: reducing learning rate of group 0 to 1.8403e-03.\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 383:\n",
      "train loss: 0.013885819524846405\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 384:\n",
      "train loss: 0.012971456702589867\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 385:\n",
      "train loss: 0.0053238872640910765\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 386:\n",
      "train loss: 0.0045611821824475\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 387:\n",
      "train loss: 0.013417975640875516\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 388:\n",
      "train loss: 0.012539167203307925\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 389:\n",
      "train loss: 0.005901001939972631\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 390:\n",
      "train loss: 0.005226675708320074\n",
      "lr: 0.0018402591023557579\n",
      "Epoch 391:\n",
      "train loss: 0.012634147399576932\n",
      "Epoch 00393: reducing learning rate of group 0 to 1.7482e-03.\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 392:\n",
      "train loss: 0.01176063008463657\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 393:\n",
      "train loss: 0.0067472577968014655\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 394:\n",
      "train loss: 0.005944711193316001\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 395:\n",
      "train loss: 0.011116216799096754\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 396:\n",
      "train loss: 0.01046313809267092\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 397:\n",
      "train loss: 0.007026715968247048\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 398:\n",
      "train loss: 0.005997716433684272\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 399:\n",
      "train loss: 0.011209811320472192\n",
      "lr: 0.0017482461472379698\n",
      "Epoch 400:\n",
      "train loss: 0.010743518646406406\n",
      "Epoch 00402: reducing learning rate of group 0 to 1.6608e-03.\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 401:\n",
      "train loss: 0.006712465245142221\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 402:\n",
      "train loss: 0.005768689849932785\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 403:\n",
      "train loss: 0.010412129876491268\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 404:\n",
      "train loss: 0.00991932411299892\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 405:\n",
      "train loss: 0.006745505870360052\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 406:\n",
      "train loss: 0.0057792820080544574\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 407:\n",
      "train loss: 0.010415066810370403\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 408:\n",
      "train loss: 0.009996401998112471\n",
      "lr: 0.0016608338398760713\n",
      "Epoch 409:\n",
      "train loss: 0.0067040950741761906\n",
      "Epoch 00411: reducing learning rate of group 0 to 1.5778e-03.\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 410:\n",
      "train loss: 0.005786961825734298\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 411:\n",
      "train loss: 0.010283153766043862\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 412:\n",
      "train loss: 0.009859663751795941\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 413:\n",
      "train loss: 0.006240362125865332\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 414:\n",
      "train loss: 0.005654492579529354\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 415:\n",
      "train loss: 0.009248935344540942\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 416:\n",
      "train loss: 0.008681486404356514\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 417:\n",
      "train loss: 0.007237319329953347\n",
      "lr: 0.0015777921478822676\n",
      "Epoch 418:\n",
      "train loss: 0.005961644713933229\n",
      "Epoch 00420: reducing learning rate of group 0 to 1.4989e-03.\n",
      "lr: 0.001498902540488154\n",
      "Epoch 419:\n",
      "train loss: 0.009665242242515209\n",
      "lr: 0.001498902540488154\n",
      "Epoch 420:\n",
      "train loss: 0.009609668127325229\n",
      "lr: 0.001498902540488154\n",
      "Epoch 421:\n",
      "train loss: 0.005603549205769514\n",
      "lr: 0.001498902540488154\n",
      "Epoch 422:\n",
      "train loss: 0.005202949838637176\n",
      "lr: 0.001498902540488154\n",
      "Epoch 423:\n",
      "train loss: 0.008670477836062621\n",
      "lr: 0.001498902540488154\n",
      "Epoch 424:\n",
      "train loss: 0.008071238457336677\n",
      "lr: 0.001498902540488154\n",
      "Epoch 425:\n",
      "train loss: 0.00708776016994011\n",
      "lr: 0.001498902540488154\n",
      "Epoch 426:\n",
      "train loss: 0.005698143452127318\n",
      "lr: 0.001498902540488154\n",
      "Epoch 427:\n",
      "train loss: 0.009278089266376539\n",
      "Epoch 00429: reducing learning rate of group 0 to 1.4240e-03.\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 428:\n",
      "train loss: 0.009413036946036955\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 429:\n",
      "train loss: 0.0056458047561188255\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 430:\n",
      "train loss: 0.005067974289408668\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 431:\n",
      "train loss: 0.008175926342862759\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 432:\n",
      "train loss: 0.00772393100543878\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 433:\n",
      "train loss: 0.006711745289034215\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 434:\n",
      "train loss: 0.00541344828818868\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 435:\n",
      "train loss: 0.008669963846678485\n",
      "lr: 0.0014239574134637463\n",
      "Epoch 436:\n",
      "train loss: 0.008743615178880199\n",
      "Epoch 00438: reducing learning rate of group 0 to 1.3528e-03.\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 437:\n",
      "train loss: 0.005662836805101037\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 438:\n",
      "train loss: 0.0050427864479661915\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 439:\n",
      "train loss: 0.007565422262534123\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 440:\n",
      "train loss: 0.007161293732179343\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 441:\n",
      "train loss: 0.006437448534926766\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 442:\n",
      "train loss: 0.005097125780702415\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 443:\n",
      "train loss: 0.00839690000191862\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 444:\n",
      "train loss: 0.00852598249517312\n",
      "lr: 0.0013527595427905588\n",
      "Epoch 445:\n",
      "train loss: 0.005163663010607218\n",
      "Epoch 00447: reducing learning rate of group 0 to 1.2851e-03.\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 446:\n",
      "train loss: 0.004767251521844698\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 447:\n",
      "train loss: 0.00763481251935764\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 448:\n",
      "train loss: 0.007108405039095716\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 449:\n",
      "train loss: 0.006053162237292933\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 450:\n",
      "train loss: 0.005000358982459494\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 451:\n",
      "train loss: 0.007586770694645601\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 452:\n",
      "train loss: 0.007596060960438933\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 453:\n",
      "train loss: 0.005310461893353398\n",
      "lr: 0.0012851215656510308\n",
      "Epoch 454:\n",
      "train loss: 0.004509344369002842\n",
      "Epoch 00456: reducing learning rate of group 0 to 1.2209e-03.\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 455:\n",
      "train loss: 0.007714562088565694\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 456:\n",
      "train loss: 0.0074618997880546364\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 457:\n",
      "train loss: 0.005184770879448219\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 458:\n",
      "train loss: 0.0046715733886538045\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 459:\n",
      "train loss: 0.006661724706024839\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 460:\n",
      "train loss: 0.006343263385586233\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 461:\n",
      "train loss: 0.005845581045899688\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 462:\n",
      "train loss: 0.004511817761671624\n",
      "lr: 0.0012208654873684791\n",
      "Epoch 463:\n",
      "train loss: 0.007743654361261479\n",
      "Epoch 00465: reducing learning rate of group 0 to 1.1598e-03.\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 464:\n",
      "train loss: 0.00794127740261944\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 465:\n",
      "train loss: 0.004507800117664691\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 466:\n",
      "train loss: 0.0043915464558136694\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 467:\n",
      "train loss: 0.005891637724285922\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 468:\n",
      "train loss: 0.005346200393678852\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 469:\n",
      "train loss: 0.006027906213720461\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 470:\n",
      "train loss: 0.004316191852820517\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 471:\n",
      "train loss: 0.007866517454327357\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 472:\n",
      "train loss: 0.008433150879633025\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 473:\n",
      "train loss: 0.0034478433317263945\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 474:\n",
      "train loss: 0.004686794636028631\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 475:\n",
      "train loss: 0.004546934505294827\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 476:\n",
      "train loss: 0.003819870930358488\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 477:\n",
      "train loss: 0.005724675544804434\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 478:\n",
      "train loss: 0.002626785135394753\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 479:\n",
      "train loss: 0.00983999195755714\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 480:\n",
      "train loss: 0.009776950927879374\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 481:\n",
      "train loss: 0.0013006145691360474\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 482:\n",
      "train loss: 0.003012505380571095\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 483:\n",
      "train loss: 0.00563520733188609\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 484:\n",
      "train loss: 0.0026515580053576068\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 485:\n",
      "train loss: 0.009600885964230681\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 486:\n",
      "train loss: 0.009514212923082785\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 487:\n",
      "train loss: 0.003741618820680736\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 488:\n",
      "train loss: 0.006546129430007823\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 489:\n",
      "train loss: 0.0012753116885152783\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 490:\n",
      "train loss: 0.00587130014721936\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 491:\n",
      "train loss: 0.0018002835939724482\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 492:\n",
      "train loss: 0.005919849746185334\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 493:\n",
      "train loss: 0.002499696676046244\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 494:\n",
      "train loss: 0.006813769158087216\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 495:\n",
      "train loss: 0.0025866544503094506\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 496:\n",
      "train loss: 0.011718998080498585\n",
      "lr: 0.0011598222130000551\n",
      "Epoch 497:\n",
      "train loss: 0.01372052929372342\n",
      "Epoch 00499: reducing learning rate of group 0 to 1.1018e-03.\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 498:\n",
      "train loss: 0.0051131684642182805\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 499:\n",
      "train loss: 0.013467687826768063\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 500:\n",
      "train loss: 0.018607708197451903\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 501:\n",
      "train loss: 0.01264166526497979\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 502:\n",
      "train loss: 0.003778104937833019\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 503:\n",
      "train loss: 0.008545545423039678\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 504:\n",
      "train loss: 0.003204503554555271\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 505:\n",
      "train loss: 0.011820128417519647\n",
      "lr: 0.0011018311023500522\n",
      "Epoch 506:\n",
      "train loss: 0.01476982787731468\n",
      "Epoch 00508: reducing learning rate of group 0 to 1.0467e-03.\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 507:\n",
      "train loss: 0.007018221960578851\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 508:\n",
      "train loss: 0.010345366995230223\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 509:\n",
      "train loss: 0.015177312087209837\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 510:\n",
      "train loss: 0.010047845257976959\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 511:\n",
      "train loss: 0.004532477717704846\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 512:\n",
      "train loss: 0.007919194772133877\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 513:\n",
      "train loss: 0.0013794689314732783\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 514:\n",
      "train loss: 0.013273579059262321\n",
      "lr: 0.0010467395472325495\n",
      "Epoch 515:\n",
      "train loss: 0.016733789851564065\n",
      "Epoch 00517: reducing learning rate of group 0 to 9.9440e-04.\n",
      "lr: 0.000994402569870922\n",
      "Epoch 516:\n",
      "train loss: 0.0105356863814361\n",
      "lr: 0.000994402569870922\n",
      "Epoch 517:\n",
      "train loss: 0.005550134499693572\n",
      "lr: 0.000994402569870922\n",
      "Epoch 518:\n",
      "train loss: 0.009657619541200497\n",
      "lr: 0.000994402569870922\n",
      "Epoch 519:\n",
      "train loss: 0.004625137818437452\n",
      "lr: 0.000994402569870922\n",
      "Epoch 520:\n",
      "train loss: 0.009243160850122632\n",
      "lr: 0.000994402569870922\n",
      "Epoch 521:\n",
      "train loss: 0.012217933098631983\n",
      "lr: 0.000994402569870922\n",
      "Epoch 522:\n",
      "train loss: 0.006084275600587603\n",
      "lr: 0.000994402569870922\n",
      "Epoch 523:\n",
      "train loss: 0.008906125303638246\n",
      "lr: 0.000994402569870922\n",
      "Epoch 524:\n",
      "train loss: 0.01268102139804398\n",
      "Epoch 00526: reducing learning rate of group 0 to 9.4468e-04.\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 525:\n",
      "train loss: 0.006713399658663439\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 526:\n",
      "train loss: 0.008021126379624932\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 527:\n",
      "train loss: 0.011821443254936906\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 528:\n",
      "train loss: 0.006705422607232988\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 529:\n",
      "train loss: 0.006753615104552792\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 530:\n",
      "train loss: 0.00986912773758189\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 531:\n",
      "train loss: 0.0038755954225397817\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 532:\n",
      "train loss: 0.01026193089534461\n",
      "lr: 0.0009446824413773759\n",
      "Epoch 533:\n",
      "train loss: 0.014118143101933607\n",
      "Epoch 00535: reducing learning rate of group 0 to 8.9745e-04.\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 534:\n",
      "train loss: 0.009032726418274118\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 535:\n",
      "train loss: 0.004397640639433473\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 536:\n",
      "train loss: 0.0074909876692125735\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 537:\n",
      "train loss: 0.0019892230680623193\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 538:\n",
      "train loss: 0.01100836500373827\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 539:\n",
      "train loss: 0.014345429225049049\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 540:\n",
      "train loss: 0.009222551179956077\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 541:\n",
      "train loss: 0.003820124751392127\n",
      "lr: 0.0008974483193085071\n",
      "Epoch 542:\n",
      "train loss: 0.007211672443366683\n",
      "Epoch 00544: reducing learning rate of group 0 to 8.5258e-04.\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 543:\n",
      "train loss: 0.0020832056068824214\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 544:\n",
      "train loss: 0.010441310961038315\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 545:\n",
      "train loss: 0.013182947318561296\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 546:\n",
      "train loss: 0.007985079303954222\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 547:\n",
      "train loss: 0.004838104643870187\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 548:\n",
      "train loss: 0.008280837280652686\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 549:\n",
      "train loss: 0.003680400562957178\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 550:\n",
      "train loss: 0.008355301510344679\n",
      "lr: 0.0008525759033430817\n",
      "Epoch 551:\n",
      "train loss: 0.011100709252484664\n",
      "Epoch 00553: reducing learning rate of group 0 to 8.0995e-04.\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 552:\n",
      "train loss: 0.005862590219418086\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 553:\n",
      "train loss: 0.006836087700567543\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 554:\n",
      "train loss: 0.01009229151987471\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 555:\n",
      "train loss: 0.005404179714313233\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 556:\n",
      "train loss: 0.006332134688410183\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 557:\n",
      "train loss: 0.009409552051577114\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 558:\n",
      "train loss: 0.004806348718576212\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 559:\n",
      "train loss: 0.0068540908399303\n",
      "lr: 0.0008099471081759276\n",
      "Epoch 560:\n",
      "train loss: 0.009765454066904978\n",
      "Epoch 00562: reducing learning rate of group 0 to 7.6945e-04.\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 561:\n",
      "train loss: 0.004792840360302734\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 562:\n",
      "train loss: 0.0072017502606733395\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 563:\n",
      "train loss: 0.01030925797379756\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 564:\n",
      "train loss: 0.006120169758239483\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 565:\n",
      "train loss: 0.004793741856678481\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 566:\n",
      "train loss: 0.0074570810889054385\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 567:\n",
      "train loss: 0.002667766896180612\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 568:\n",
      "train loss: 0.00872915294955854\n",
      "lr: 0.0007694497527671312\n",
      "Epoch 569:\n",
      "train loss: 0.011910762188450325\n",
      "Epoch 00571: reducing learning rate of group 0 to 7.3098e-04.\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 570:\n",
      "train loss: 0.007798155833921922\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 571:\n",
      "train loss: 0.002981868807026917\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 572:\n",
      "train loss: 0.005445565011419149\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 573:\n",
      "train loss: 0.0008750173255010387\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 574:\n",
      "train loss: 0.009787508924651902\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 575:\n",
      "train loss: 0.012692859490909137\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 576:\n",
      "train loss: 0.008738805299921841\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 577:\n",
      "train loss: 0.0018483109216213887\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 578:\n",
      "train loss: 0.0051480311271632755\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 579:\n",
      "train loss: 0.0026396520060772907\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 580:\n",
      "train loss: 0.005507742994762641\n",
      "lr: 0.0007309772651287747\n",
      "Epoch 581:\n",
      "train loss: 0.005301892621256301\n",
      "Epoch 00583: reducing learning rate of group 0 to 6.9443e-04.\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 582:\n",
      "train loss: 0.0028376382579591206\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 583:\n",
      "train loss: 0.0037339024047331046\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 584:\n",
      "train loss: 0.0020056911499339272\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 585:\n",
      "train loss: 0.002169156333184793\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 586:\n",
      "train loss: 0.003314606559956394\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 587:\n",
      "train loss: 0.0024831772658702243\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 588:\n",
      "train loss: 0.003940600930655424\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 589:\n",
      "train loss: 0.0030853032040952756\n",
      "lr: 0.0006944284018723359\n",
      "Epoch 590:\n",
      "train loss: 0.004262094235834426\n",
      "Epoch 00592: reducing learning rate of group 0 to 6.5971e-04.\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 591:\n",
      "train loss: 0.003915249572548221\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 592:\n",
      "train loss: 0.0028382498115681718\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 593:\n",
      "train loss: 0.0027799333373380157\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 594:\n",
      "train loss: 0.003109442586130635\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 595:\n",
      "train loss: 0.0023527722131548336\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 596:\n",
      "train loss: 0.004296365421490754\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 597:\n",
      "train loss: 0.003984575359122368\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 598:\n",
      "train loss: 0.0024754052083111044\n",
      "lr: 0.0006597069817787191\n",
      "Epoch 599:\n",
      "train loss: 0.0022726397179251992\n",
      "Epoch 00601: reducing learning rate of group 0 to 6.2672e-04.\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 600:\n",
      "train loss: 0.003911142784406738\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 601:\n",
      "train loss: 0.0034947683894646403\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 602:\n",
      "train loss: 0.0026664877213470207\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 603:\n",
      "train loss: 0.002351626496107605\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 604:\n",
      "train loss: 0.003735952349775346\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 605:\n",
      "train loss: 0.003424245239941191\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 606:\n",
      "train loss: 0.0025705395215624644\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 607:\n",
      "train loss: 0.00220913962323758\n",
      "lr: 0.0006267216326897832\n",
      "Epoch 608:\n",
      "train loss: 0.0038569409352260473\n",
      "Epoch 00610: reducing learning rate of group 0 to 5.9539e-04.\n",
      "lr: 0.000595385551055294\n",
      "Epoch 609:\n",
      "train loss: 0.0035086648951792986\n",
      "lr: 0.000595385551055294\n",
      "Epoch 610:\n",
      "train loss: 0.0026060107236478614\n",
      "lr: 0.000595385551055294\n",
      "Epoch 611:\n",
      "train loss: 0.002336241111748037\n",
      "lr: 0.000595385551055294\n",
      "Epoch 612:\n",
      "train loss: 0.0033797053620614483\n",
      "lr: 0.000595385551055294\n",
      "Epoch 613:\n",
      "train loss: 0.0030832709381592117\n",
      "lr: 0.000595385551055294\n",
      "Epoch 614:\n",
      "train loss: 0.002655536508555419\n",
      "lr: 0.000595385551055294\n",
      "Epoch 615:\n",
      "train loss: 0.002300862145322294\n",
      "lr: 0.000595385551055294\n",
      "Epoch 616:\n",
      "train loss: 0.003516698965637188\n",
      "lr: 0.000595385551055294\n",
      "Epoch 617:\n",
      "train loss: 0.0032757859675406086\n",
      "Epoch 00619: reducing learning rate of group 0 to 5.6562e-04.\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 618:\n",
      "train loss: 0.0023947348327040888\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 619:\n",
      "train loss: 0.0020168244902864303\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 620:\n",
      "train loss: 0.0035040733757943894\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 621:\n",
      "train loss: 0.0032756669050083755\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 622:\n",
      "train loss: 0.002133802001232378\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 623:\n",
      "train loss: 0.0017912576934403718\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 624:\n",
      "train loss: 0.0037174829961696687\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 625:\n",
      "train loss: 0.003501850548083733\n",
      "lr: 0.0005656162735025292\n",
      "Epoch 626:\n",
      "train loss: 0.0018770138904550956\n",
      "Epoch 00628: reducing learning rate of group 0 to 5.3734e-04.\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 627:\n",
      "train loss: 0.0015150101667492377\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 628:\n",
      "train loss: 0.003993431891645827\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 629:\n",
      "train loss: 0.0037766345613992583\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 630:\n",
      "train loss: 0.0013376531782495545\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 631:\n",
      "train loss: 0.0010097569925072894\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 632:\n",
      "train loss: 0.004214561161185263\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 633:\n",
      "train loss: 0.004008857142201855\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 634:\n",
      "train loss: 0.0010954337355980218\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 635:\n",
      "train loss: 0.0007932570548210858\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 636:\n",
      "train loss: 0.00430642026181224\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 637:\n",
      "train loss: 0.003983313464893874\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 638:\n",
      "train loss: 0.0012639107928458387\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 639:\n",
      "train loss: 0.0011665932170302979\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 640:\n",
      "train loss: 0.0037197838678019166\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 641:\n",
      "train loss: 0.0031685969324789145\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 642:\n",
      "train loss: 0.0022470793718302943\n",
      "lr: 0.0005373354598274027\n",
      "Epoch 643:\n",
      "train loss: 0.0021914182682565117\n",
      "Epoch 00645: reducing learning rate of group 0 to 5.1047e-04.\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 644:\n",
      "train loss: 0.002784245152687787\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 645:\n",
      "train loss: 0.002355490020809353\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 646:\n",
      "train loss: 0.0026827293003070176\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 647:\n",
      "train loss: 0.002515852087293694\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 648:\n",
      "train loss: 0.0023272838294132047\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 649:\n",
      "train loss: 0.0020209087000390906\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 650:\n",
      "train loss: 0.0029194509158468897\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 651:\n",
      "train loss: 0.0026775858066644723\n",
      "lr: 0.0005104686868360325\n",
      "Epoch 652:\n",
      "train loss: 0.0022288890439692735\n",
      "Epoch 00654: reducing learning rate of group 0 to 4.8495e-04.\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 653:\n",
      "train loss: 0.001975792404458044\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 654:\n",
      "train loss: 0.0029200849191143748\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 655:\n",
      "train loss: 0.0026508569759131303\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 656:\n",
      "train loss: 0.002038386212176636\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 657:\n",
      "train loss: 0.00183030347607964\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 658:\n",
      "train loss: 0.002786718917116227\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 659:\n",
      "train loss: 0.0024885776661849944\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 660:\n",
      "train loss: 0.0022266810344922523\n",
      "lr: 0.0004849452524942309\n",
      "Epoch 661:\n",
      "train loss: 0.0020442598220470048\n",
      "Epoch 00663: reducing learning rate of group 0 to 4.6070e-04.\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 662:\n",
      "train loss: 0.002545441299510697\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 663:\n",
      "train loss: 0.002231646403575167\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 664:\n",
      "train loss: 0.0022549916115183997\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 665:\n",
      "train loss: 0.0020904816434236004\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 666:\n",
      "train loss: 0.0022603220780380373\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 667:\n",
      "train loss: 0.0019565652260506807\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 668:\n",
      "train loss: 0.0025300156622053813\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 669:\n",
      "train loss: 0.0023720180305347458\n",
      "lr: 0.00046069798986951934\n",
      "Epoch 670:\n",
      "train loss: 0.001967136231701576\n",
      "Epoch 00672: reducing learning rate of group 0 to 4.3766e-04.\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 671:\n",
      "train loss: 0.0016578911182355543\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 672:\n",
      "train loss: 0.002829225236874575\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 673:\n",
      "train loss: 0.002683015605594254\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 674:\n",
      "train loss: 0.0014303344769435278\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 675:\n",
      "train loss: 0.0011354426141448777\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 676:\n",
      "train loss: 0.0031218073230609156\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 677:\n",
      "train loss: 0.0029761635881082544\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 678:\n",
      "train loss: 0.0011332193088319101\n",
      "lr: 0.0004376630903760434\n",
      "Epoch 679:\n",
      "train loss: 0.0008407329598182999\n",
      "Epoch 00681: reducing learning rate of group 0 to 4.1578e-04.\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 680:\n",
      "train loss: 0.003407553420859182\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 681:\n",
      "train loss: 0.0032576726850374483\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 682:\n",
      "train loss: 0.0006452271422820166\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 683:\n",
      "train loss: 0.0003761783802463007\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 684:\n",
      "train loss: 0.0036288179865586537\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 685:\n",
      "train loss: 0.0034650208108772773\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 686:\n",
      "train loss: 0.0004827520311201674\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 687:\n",
      "train loss: 0.0006594893666158036\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 688:\n",
      "train loss: 0.0020188399334027294\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 689:\n",
      "train loss: 0.0009458767757008536\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 690:\n",
      "train loss: 0.0027155966316900542\n",
      "lr: 0.00041577993585724117\n",
      "Epoch 691:\n",
      "train loss: 0.0021711486536156945\n",
      "Epoch 00693: reducing learning rate of group 0 to 3.9499e-04.\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 692:\n",
      "train loss: 0.0023341307554356457\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 693:\n",
      "train loss: 0.0021523780812055587\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 694:\n",
      "train loss: 0.0017474726466433148\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 695:\n",
      "train loss: 0.0018202650161822222\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 696:\n",
      "train loss: 0.0014619856978673122\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 697:\n",
      "train loss: 0.0010734767012407628\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 698:\n",
      "train loss: 0.0023407948120669187\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 699:\n",
      "train loss: 0.0016927791863145455\n",
      "lr: 0.0003949909390643791\n",
      "Epoch 700:\n",
      "train loss: 0.00251772330526132\n",
      "Epoch 00702: reducing learning rate of group 0 to 3.7524e-04.\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 701:\n",
      "train loss: 0.0024922509435234366\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 702:\n",
      "train loss: 0.0011985010252129086\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 703:\n",
      "train loss: 0.0010967260007428176\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 704:\n",
      "train loss: 0.00230188511974801\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 705:\n",
      "train loss: 0.002037107802455417\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 706:\n",
      "train loss: 0.0016457488373251825\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 707:\n",
      "train loss: 0.0014658652804115779\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 708:\n",
      "train loss: 0.002142020869574761\n",
      "lr: 0.0003752413921111601\n",
      "Epoch 709:\n",
      "train loss: 0.001949763391392417\n",
      "Epoch 00711: reducing learning rate of group 0 to 3.5648e-04.\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 710:\n",
      "train loss: 0.0015810513138064068\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 711:\n",
      "train loss: 0.0013824137413270857\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 712:\n",
      "train loss: 0.0020008258122485487\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 713:\n",
      "train loss: 0.0017556444170530392\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 714:\n",
      "train loss: 0.0017393457319658644\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 715:\n",
      "train loss: 0.0016250285937803193\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 716:\n",
      "train loss: 0.0017049822235179733\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 717:\n",
      "train loss: 0.0014895850802401553\n",
      "lr: 0.00035647932250560207\n",
      "Epoch 718:\n",
      "train loss: 0.001927978605277666\n",
      "Epoch 00720: reducing learning rate of group 0 to 3.3866e-04.\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 719:\n",
      "train loss: 0.0017334402466620967\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 720:\n",
      "train loss: 0.0017095248129904972\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 721:\n",
      "train loss: 0.0015676788331503591\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 722:\n",
      "train loss: 0.0016266058739836055\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 723:\n",
      "train loss: 0.001431685668806226\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 724:\n",
      "train loss: 0.001815559936980834\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 725:\n",
      "train loss: 0.0016556779208884793\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 726:\n",
      "train loss: 0.0015748381484404928\n",
      "lr: 0.00033865535638032194\n",
      "Epoch 727:\n",
      "train loss: 0.001392039695498085\n",
      "Epoch 00729: reducing learning rate of group 0 to 3.2172e-04.\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 728:\n",
      "train loss: 0.0018494087958987636\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 729:\n",
      "train loss: 0.0017028490063177215\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 730:\n",
      "train loss: 0.001338737879555457\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 731:\n",
      "train loss: 0.001146728220655597\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 732:\n",
      "train loss: 0.0019495168360067533\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 733:\n",
      "train loss: 0.0018122130194942645\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 734:\n",
      "train loss: 0.0012270211080884364\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 735:\n",
      "train loss: 0.001041570127994419\n",
      "lr: 0.00032172258856130585\n",
      "Epoch 736:\n",
      "train loss: 0.0020429789382898932\n",
      "Epoch 00738: reducing learning rate of group 0 to 3.0564e-04.\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 737:\n",
      "train loss: 0.0019046662481163427\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 738:\n",
      "train loss: 0.0011398169989736415\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 739:\n",
      "train loss: 0.0009662894476743701\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 740:\n",
      "train loss: 0.0019631868360930997\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 741:\n",
      "train loss: 0.0018326015684988382\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 742:\n",
      "train loss: 0.0010512040813491531\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 743:\n",
      "train loss: 0.0008727723484739831\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 744:\n",
      "train loss: 0.0020567189513915076\n",
      "lr: 0.0003056364591332405\n",
      "Epoch 745:\n",
      "train loss: 0.0019247060161489248\n",
      "Epoch 00747: reducing learning rate of group 0 to 2.9035e-04.\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 746:\n",
      "train loss: 0.0009620359560120582\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 747:\n",
      "train loss: 0.0007886856907411119\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 748:\n",
      "train loss: 0.0019912815849348685\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 749:\n",
      "train loss: 0.0018679045751652586\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 750:\n",
      "train loss: 0.000869928952293986\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 751:\n",
      "train loss: 0.0007022648536389262\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 752:\n",
      "train loss: 0.002076095334997417\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 753:\n",
      "train loss: 0.0019505237993052945\n",
      "lr: 0.0002903546361765785\n",
      "Epoch 754:\n",
      "train loss: 0.0007879094571970464\n",
      "Epoch 00756: reducing learning rate of group 0 to 2.7584e-04.\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 755:\n",
      "train loss: 0.0006225288321696584\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 756:\n",
      "train loss: 0.002155102311010214\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 757:\n",
      "train loss: 0.002038603367824786\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 758:\n",
      "train loss: 0.0005582297386395616\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 759:\n",
      "train loss: 0.0004008376819162163\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 760:\n",
      "train loss: 0.0022262131693837482\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 761:\n",
      "train loss: 0.002100238541506412\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 762:\n",
      "train loss: 0.0005111365094636367\n",
      "lr: 0.00027583690436774953\n",
      "Epoch 763:\n",
      "train loss: 0.0004060901635068142\n",
      "Epoch 00765: reducing learning rate of group 0 to 2.6205e-04.\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 764:\n",
      "train loss: 0.0020843639921950827\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 765:\n",
      "train loss: 0.0018306397002813978\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 766:\n",
      "train loss: 0.0007870997923653111\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 767:\n",
      "train loss: 0.0008011942301325381\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 768:\n",
      "train loss: 0.0015376911512392328\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 769:\n",
      "train loss: 0.0012652009926594078\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 770:\n",
      "train loss: 0.0013339236487215098\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 771:\n",
      "train loss: 0.0013099772120045495\n",
      "lr: 0.00026204505914936203\n",
      "Epoch 772:\n",
      "train loss: 0.0010899743327178984\n",
      "Epoch 00774: reducing learning rate of group 0 to 2.4894e-04.\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 773:\n",
      "train loss: 0.0008743519658483336\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 774:\n",
      "train loss: 0.0016850907158521846\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 775:\n",
      "train loss: 0.001613613982925029\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 776:\n",
      "train loss: 0.0007003141014437365\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 777:\n",
      "train loss: 0.0005398221153750792\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 778:\n",
      "train loss: 0.0018344812311284421\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 779:\n",
      "train loss: 0.0017139997221848969\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 780:\n",
      "train loss: 0.000665006262445742\n",
      "lr: 0.00024894280619189394\n",
      "Epoch 781:\n",
      "train loss: 0.0005828515868604048\n",
      "Epoch 00783: reducing learning rate of group 0 to 2.3650e-04.\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 782:\n",
      "train loss: 0.0017111543643464038\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 783:\n",
      "train loss: 0.001511411013850089\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 784:\n",
      "train loss: 0.0007974630350635022\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 785:\n",
      "train loss: 0.0007440941275627737\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 786:\n",
      "train loss: 0.001434583889584063\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 787:\n",
      "train loss: 0.0012566509984117801\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 788:\n",
      "train loss: 0.001042987264003726\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 789:\n",
      "train loss: 0.0009779577209937669\n",
      "lr: 0.00023649566588229923\n",
      "Epoch 790:\n",
      "train loss: 0.0012136189676342584\n",
      "Epoch 00792: reducing learning rate of group 0 to 2.2467e-04.\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 791:\n",
      "train loss: 0.0010497374072705834\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 792:\n",
      "train loss: 0.0012314160716434585\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 793:\n",
      "train loss: 0.001157922523132585\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 794:\n",
      "train loss: 0.0009338354008928773\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 795:\n",
      "train loss: 0.0007870715316170277\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 796:\n",
      "train loss: 0.0013718441315931514\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 797:\n",
      "train loss: 0.0012916670393222649\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 798:\n",
      "train loss: 0.0008027618198358373\n",
      "lr: 0.00022467088258818426\n",
      "Epoch 799:\n",
      "train loss: 0.0006614193312538715\n",
      "Epoch 00801: reducing learning rate of group 0 to 2.1344e-04.\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 800:\n",
      "train loss: 0.0014900796657596038\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 801:\n",
      "train loss: 0.0014039455420165835\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 802:\n",
      "train loss: 0.0005915328544959178\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 803:\n",
      "train loss: 0.0004650870905819907\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 804:\n",
      "train loss: 0.0015700948460650194\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 805:\n",
      "train loss: 0.0014820159014025077\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 806:\n",
      "train loss: 0.0005166080317975044\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 807:\n",
      "train loss: 0.0003927886434203051\n",
      "lr: 0.00021343733845877503\n",
      "Epoch 808:\n",
      "train loss: 0.0016392738592487758\n",
      "Epoch 00810: reducing learning rate of group 0 to 2.0277e-04.\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 809:\n",
      "train loss: 0.001549775303332006\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 810:\n",
      "train loss: 0.00045051956280552633\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 811:\n",
      "train loss: 0.0003391898297705616\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 812:\n",
      "train loss: 0.0015778842609217103\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 813:\n",
      "train loss: 0.0014803772929363314\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 814:\n",
      "train loss: 0.0004328695236399\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 815:\n",
      "train loss: 0.00034081112201796225\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 816:\n",
      "train loss: 0.0015480800693556936\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 817:\n",
      "train loss: 0.0014244446194194413\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 818:\n",
      "train loss: 0.0005146381530643185\n",
      "lr: 0.00020276547153583627\n",
      "Epoch 819:\n",
      "train loss: 0.0004461309201352762\n",
      "Epoch 00821: reducing learning rate of group 0 to 1.9263e-04.\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 820:\n",
      "train loss: 0.0014281259455713351\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 821:\n",
      "train loss: 0.0012913381221419966\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 822:\n",
      "train loss: 0.0005582629197884042\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 823:\n",
      "train loss: 0.0004937064355799132\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 824:\n",
      "train loss: 0.0012931721175225847\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 825:\n",
      "train loss: 0.0011728966357361064\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 826:\n",
      "train loss: 0.000668966144595206\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 827:\n",
      "train loss: 0.0005965900452531453\n",
      "lr: 0.00019262719795904446\n",
      "Epoch 828:\n",
      "train loss: 0.0011960070165162875\n",
      "Epoch 00830: reducing learning rate of group 0 to 1.8300e-04.\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 829:\n",
      "train loss: 0.0010793814121143183\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 830:\n",
      "train loss: 0.0007577119176663734\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 831:\n",
      "train loss: 0.0006851214472156133\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 832:\n",
      "train loss: 0.0010215911573594229\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 833:\n",
      "train loss: 0.0009137286482856983\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 834:\n",
      "train loss: 0.0008278089168320343\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 835:\n",
      "train loss: 0.0007513037592507119\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 836:\n",
      "train loss: 0.0009606013676853117\n",
      "lr: 0.00018299583806109224\n",
      "Epoch 837:\n",
      "train loss: 0.0008577844030513318\n",
      "Epoch 00839: reducing learning rate of group 0 to 1.7385e-04.\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 838:\n",
      "train loss: 0.0008769502114927051\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 839:\n",
      "train loss: 0.0007945757694808259\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 840:\n",
      "train loss: 0.0008379831292258858\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 841:\n",
      "train loss: 0.0007466026341969428\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 842:\n",
      "train loss: 0.0008947928706124444\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 843:\n",
      "train loss: 0.0008104338741337341\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 844:\n",
      "train loss: 0.0008273900244227306\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 845:\n",
      "train loss: 0.0007423320784973895\n",
      "lr: 0.0001738460461580376\n",
      "Epoch 846:\n",
      "train loss: 0.0008926152466836819\n",
      "Epoch 00848: reducing learning rate of group 0 to 1.6515e-04.\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 847:\n",
      "train loss: 0.0008023425715544556\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 848:\n",
      "train loss: 0.0008401093717392907\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 849:\n",
      "train loss: 0.0007641689628130019\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 850:\n",
      "train loss: 0.0007843324387828286\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 851:\n",
      "train loss: 0.0006950631497916953\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 852:\n",
      "train loss: 0.0008671923270784797\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 853:\n",
      "train loss: 0.0007936840707673954\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 854:\n",
      "train loss: 0.000752438576762117\n",
      "lr: 0.00016515374385013573\n",
      "Epoch 855:\n",
      "train loss: 0.0006617832988938349\n",
      "Epoch 00857: reducing learning rate of group 0 to 1.5690e-04.\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 856:\n",
      "train loss: 0.0009008533380565559\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 857:\n",
      "train loss: 0.0008282056374440296\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 858:\n",
      "train loss: 0.000639334383850767\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 859:\n",
      "train loss: 0.0005531474090933922\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 860:\n",
      "train loss: 0.0009305175692084024\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 861:\n",
      "train loss: 0.0008612014656745799\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 862:\n",
      "train loss: 0.0006060194770125179\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 863:\n",
      "train loss: 0.0005203053626724392\n",
      "lr: 0.00015689605665762893\n",
      "Epoch 864:\n",
      "train loss: 0.0009622911309699626\n",
      "Epoch 00866: reducing learning rate of group 0 to 1.4905e-04.\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 865:\n",
      "train loss: 0.0008925577216912486\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 866:\n",
      "train loss: 0.0005743832462169347\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 867:\n",
      "train loss: 0.0004933110798870326\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 868:\n",
      "train loss: 0.0009143646862290388\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 869:\n",
      "train loss: 0.0008477703066720172\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 870:\n",
      "train loss: 0.0005456560021798746\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 871:\n",
      "train loss: 0.00046492110801289743\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 872:\n",
      "train loss: 0.0009419469303090083\n",
      "lr: 0.00014905125382474747\n",
      "Epoch 873:\n",
      "train loss: 0.0008752010998319779\n",
      "Epoch 00875: reducing learning rate of group 0 to 1.4160e-04.\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 874:\n",
      "train loss: 0.0005177875047022448\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 875:\n",
      "train loss: 0.0004371623560638691\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 876:\n",
      "train loss: 0.0008988514745907532\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 877:\n",
      "train loss: 0.0008353945968718991\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 878:\n",
      "train loss: 0.00048756428291096875\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 879:\n",
      "train loss: 0.0004110750472360602\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 880:\n",
      "train loss: 0.0009243724832214225\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 881:\n",
      "train loss: 0.0008609424280163232\n",
      "lr: 0.0001415986911335101\n",
      "Epoch 882:\n",
      "train loss: 0.0004614927693356526\n",
      "Epoch 00884: reducing learning rate of group 0 to 1.3452e-04.\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 883:\n",
      "train loss: 0.0003850204484329724\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 884:\n",
      "train loss: 0.0009499267985669996\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 885:\n",
      "train loss: 0.0008897381449782434\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 886:\n",
      "train loss: 0.0003660258010646177\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 887:\n",
      "train loss: 0.0002935445285414549\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 888:\n",
      "train loss: 0.0009738735890951783\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 889:\n",
      "train loss: 0.0009135216744380402\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 890:\n",
      "train loss: 0.00034205900829643263\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 891:\n",
      "train loss: 0.0002700644269032531\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 892:\n",
      "train loss: 0.0009963357153069683\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 893:\n",
      "train loss: 0.0009355796875276714\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 894:\n",
      "train loss: 0.0003202505251824449\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 895:\n",
      "train loss: 0.0002495160207977026\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 896:\n",
      "train loss: 0.0010149861113339634\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 897:\n",
      "train loss: 0.0009530971375990585\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 898:\n",
      "train loss: 0.00030423596061690143\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 899:\n",
      "train loss: 0.00023698972641010378\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 900:\n",
      "train loss: 0.0010231471692096453\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 901:\n",
      "train loss: 0.0009581014549902698\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 902:\n",
      "train loss: 0.00030403437459735656\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 903:\n",
      "train loss: 0.00024560342822551505\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 904:\n",
      "train loss: 0.0010041955519782648\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 905:\n",
      "train loss: 0.0009311066446019271\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 906:\n",
      "train loss: 0.0003426387211976281\n",
      "lr: 0.00013451875657683458\n",
      "Epoch 907:\n",
      "train loss: 0.00029970073424225203\n",
      "Epoch 00909: reducing learning rate of group 0 to 1.2779e-04.\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 908:\n",
      "train loss: 0.0009312100003787391\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 909:\n",
      "train loss: 0.0008428192677518817\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 910:\n",
      "train loss: 0.00038688087579186187\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 911:\n",
      "train loss: 0.00036606844930095836\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 912:\n",
      "train loss: 0.0007780976673971395\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 913:\n",
      "train loss: 0.0006722450078249137\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 914:\n",
      "train loss: 0.00057472040587776\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 915:\n",
      "train loss: 0.0005572679413026925\n",
      "lr: 0.00012779281874799285\n",
      "Epoch 916:\n",
      "train loss: 0.0005880162421352424\n",
      "Epoch 00918: reducing learning rate of group 0 to 1.2140e-04.\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 917:\n",
      "train loss: 0.0004768273460102856\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 918:\n",
      "train loss: 0.0007657298895134857\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 919:\n",
      "train loss: 0.0007448085965138667\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 920:\n",
      "train loss: 0.0003607743462422013\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 921:\n",
      "train loss: 0.00027464261425743226\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 922:\n",
      "train loss: 0.000882424894769241\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 923:\n",
      "train loss: 0.000843463733266948\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 924:\n",
      "train loss: 0.00029219147510291504\n",
      "lr: 0.0001214031778105932\n",
      "Epoch 925:\n",
      "train loss: 0.0002569547126780139\n",
      "Epoch 00927: reducing learning rate of group 0 to 1.1533e-04.\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 926:\n",
      "train loss: 0.000843281878975795\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 927:\n",
      "train loss: 0.0007578031936571603\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 928:\n",
      "train loss: 0.00037316243396758953\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 929:\n",
      "train loss: 0.000381876581694394\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 930:\n",
      "train loss: 0.0006113276739886849\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 931:\n",
      "train loss: 0.0004853846168416176\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 932:\n",
      "train loss: 0.0006654370091997277\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 933:\n",
      "train loss: 0.0006642033212364765\n",
      "lr: 0.00011533301892006353\n",
      "Epoch 934:\n",
      "train loss: 0.0003568390039916566\n",
      "Epoch 00936: reducing learning rate of group 0 to 1.0957e-04.\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 935:\n",
      "train loss: 0.00024646488022507057\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 936:\n",
      "train loss: 0.0008734803432763223\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 937:\n",
      "train loss: 0.0008546231854141592\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 938:\n",
      "train loss: 0.00015675198145740016\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 939:\n",
      "train loss: 0.00017370382120041763\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 940:\n",
      "train loss: 0.0007587927322094211\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 941:\n",
      "train loss: 0.0006257767616347616\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 942:\n",
      "train loss: 0.00047747182467609795\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 943:\n",
      "train loss: 0.0004869120637556491\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 944:\n",
      "train loss: 0.00047374522967700254\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 945:\n",
      "train loss: 0.0003712704919982035\n",
      "lr: 0.00010956636797406035\n",
      "Epoch 946:\n",
      "train loss: 0.0006929069693141924\n",
      "Epoch 00948: reducing learning rate of group 0 to 1.0409e-04.\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 947:\n",
      "train loss: 0.0006651792640666062\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 948:\n",
      "train loss: 0.00033326043063246436\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 949:\n",
      "train loss: 0.00026098429936978685\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 950:\n",
      "train loss: 0.0007284338833107289\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 951:\n",
      "train loss: 0.000689509016615604\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 952:\n",
      "train loss: 0.00027347660660466183\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 953:\n",
      "train loss: 0.00021424522703165257\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 954:\n",
      "train loss: 0.0007649257002400649\n",
      "lr: 0.00010408804957535733\n",
      "Epoch 955:\n",
      "train loss: 0.0007192254578194801\n",
      "Epoch 00957: reducing learning rate of group 0 to 9.8884e-05.\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 956:\n",
      "train loss: 0.000252140266684049\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 957:\n",
      "train loss: 0.00020409997680501864\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 958:\n",
      "train loss: 0.0007149045999489804\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 959:\n",
      "train loss: 0.0006625511456162614\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 960:\n",
      "train loss: 0.0002711518187494515\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 961:\n",
      "train loss: 0.000238229061797064\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 962:\n",
      "train loss: 0.0006653022169849315\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 963:\n",
      "train loss: 0.0006000047635480863\n",
      "lr: 9.888364709658946e-05\n",
      "Epoch 964:\n",
      "train loss: 0.0003474437688786037\n",
      "Epoch 00966: reducing learning rate of group 0 to 9.3939e-05.\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 965:\n",
      "train loss: 0.0003252750109508508\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 966:\n",
      "train loss: 0.0005652659006104562\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 967:\n",
      "train loss: 0.0004910112680756603\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 968:\n",
      "train loss: 0.0004179630195332167\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 969:\n",
      "train loss: 0.0004025474892937463\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 970:\n",
      "train loss: 0.00044085474343827586\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 971:\n",
      "train loss: 0.0003618043640391399\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 972:\n",
      "train loss: 0.0005463964257795694\n",
      "lr: 9.393946474175998e-05\n",
      "Epoch 973:\n",
      "train loss: 0.0005300528957104551\n",
      "Epoch 00975: reducing learning rate of group 0 to 8.9242e-05.\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 974:\n",
      "train loss: 0.000320720250909989\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 975:\n",
      "train loss: 0.00025036536604285803\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 976:\n",
      "train loss: 0.0006016429305274805\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 977:\n",
      "train loss: 0.0005774838565239753\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 978:\n",
      "train loss: 0.0002437687779839693\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 979:\n",
      "train loss: 0.000196950106622927\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 980:\n",
      "train loss: 0.0006315339623205956\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 981:\n",
      "train loss: 0.0005871981350416351\n",
      "lr: 8.924249150467197e-05\n",
      "Epoch 982:\n",
      "train loss: 0.00025895633997438764\n",
      "Epoch 00984: reducing learning rate of group 0 to 8.4780e-05.\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 983:\n",
      "train loss: 0.0002409910482979838\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 984:\n",
      "train loss: 0.0005541444349262457\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 985:\n",
      "train loss: 0.00048444184531794683\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 986:\n",
      "train loss: 0.0003449559677207796\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 987:\n",
      "train loss: 0.00034160383476172394\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 988:\n",
      "train loss: 0.00040207424480613715\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 989:\n",
      "train loss: 0.0003174794331348947\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 990:\n",
      "train loss: 0.0005146655134791961\n",
      "lr: 8.478036692943837e-05\n",
      "Epoch 991:\n",
      "train loss: 0.0005088058973267181\n",
      "Epoch 00993: reducing learning rate of group 0 to 8.0541e-05.\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 992:\n",
      "train loss: 0.00025103615031495655\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 993:\n",
      "train loss: 0.00018224547171186216\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 994:\n",
      "train loss: 0.0005898296017719656\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 995:\n",
      "train loss: 0.0005711183732632261\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 996:\n",
      "train loss: 0.00017506793855705043\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 997:\n",
      "train loss: 0.00015603506636881786\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 998:\n",
      "train loss: 0.0005669577209014312\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 999:\n",
      "train loss: 0.000508528137337772\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1000:\n",
      "train loss: 0.0002790652996187081\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1001:\n",
      "train loss: 0.00028100598270282566\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1002:\n",
      "train loss: 0.00041459895828288443\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1003:\n",
      "train loss: 0.0003313818426762035\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1004:\n",
      "train loss: 0.0004644004600888954\n",
      "lr: 8.054134858296645e-05\n",
      "Epoch 1005:\n",
      "train loss: 0.0004590480019135999\n",
      "Epoch 01007: reducing learning rate of group 0 to 7.6514e-05.\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1006:\n",
      "train loss: 0.00025572533316427524\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1007:\n",
      "train loss: 0.00018300374028551244\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1008:\n",
      "train loss: 0.0005548206092077782\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1009:\n",
      "train loss: 0.0005396422601238324\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1010:\n",
      "train loss: 0.00016102604078738395\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1011:\n",
      "train loss: 0.00012931835262382303\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1012:\n",
      "train loss: 0.0005718906939517299\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1013:\n",
      "train loss: 0.0005281890583299138\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1014:\n",
      "train loss: 0.00020667248924192473\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1015:\n",
      "train loss: 0.00020698750272620697\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1016:\n",
      "train loss: 0.0004579530571507937\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1017:\n",
      "train loss: 0.0003859615109047838\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1018:\n",
      "train loss: 0.0003693223485991518\n",
      "lr: 7.651428115381812e-05\n",
      "Epoch 1019:\n",
      "train loss: 0.0003651732570798778\n",
      "Epoch 01021: reducing learning rate of group 0 to 7.2689e-05.\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1020:\n",
      "train loss: 0.00030727825991649075\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1021:\n",
      "train loss: 0.00023395629521912178\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1022:\n",
      "train loss: 0.0004729476401129318\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1023:\n",
      "train loss: 0.00046131362355429287\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1024:\n",
      "train loss: 0.00019707375643165392\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1025:\n",
      "train loss: 0.00014811152424571062\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1026:\n",
      "train loss: 0.0005370321460606848\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1027:\n",
      "train loss: 0.0005108882477551452\n",
      "lr: 7.268856709612721e-05\n",
      "Epoch 1028:\n",
      "train loss: 0.00017208430527681107\n",
      "Epoch 01030: reducing learning rate of group 0 to 6.9054e-05.\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1029:\n",
      "train loss: 0.00016369805953551754\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1030:\n",
      "train loss: 0.0004774502854875524\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1031:\n",
      "train loss: 0.0004191699817779987\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1032:\n",
      "train loss: 0.0002626338044995831\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1033:\n",
      "train loss: 0.0002675832039552509\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1034:\n",
      "train loss: 0.0003255216845846548\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1035:\n",
      "train loss: 0.0002505458614190056\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1036:\n",
      "train loss: 0.00043004969004875967\n",
      "lr: 6.905413874132085e-05\n",
      "Epoch 1037:\n",
      "train loss: 0.00042496597507934666\n",
      "Epoch 01039: reducing learning rate of group 0 to 6.5601e-05.\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1038:\n",
      "train loss: 0.00019227119649400313\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1039:\n",
      "train loss: 0.00013834744117861638\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1040:\n",
      "train loss: 0.0004840069440677926\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1041:\n",
      "train loss: 0.0004641066303516305\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1042:\n",
      "train loss: 0.0001488471690266606\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1043:\n",
      "train loss: 0.00014200931071773956\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1044:\n",
      "train loss: 0.00043559511494080345\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1045:\n",
      "train loss: 0.00038076088715997337\n",
      "lr: 6.56014318042548e-05\n",
      "Epoch 1046:\n",
      "train loss: 0.0002632247567567092\n",
      "Epoch 01048: reducing learning rate of group 0 to 6.2321e-05.\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1047:\n",
      "train loss: 0.0002612808294139723\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1048:\n",
      "train loss: 0.000310745362469896\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1049:\n",
      "train loss: 0.00025069990906270804\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1050:\n",
      "train loss: 0.000357662624190755\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1051:\n",
      "train loss: 0.00034899843032932924\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1052:\n",
      "train loss: 0.00021175588867476723\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1053:\n",
      "train loss: 0.00016519076738309417\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1054:\n",
      "train loss: 0.00042419324798945775\n",
      "lr: 6.232136021404205e-05\n",
      "Epoch 1055:\n",
      "train loss: 0.0004035518996083764\n",
      "Epoch 01057: reducing learning rate of group 0 to 5.9205e-05.\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1056:\n",
      "train loss: 0.00017902713234810942\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1057:\n",
      "train loss: 0.00016172362242792613\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1058:\n",
      "train loss: 0.00036788930157673894\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1059:\n",
      "train loss: 0.00032432150531112034\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1060:\n",
      "train loss: 0.00024980618032996503\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1061:\n",
      "train loss: 0.00024181916127458025\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1062:\n",
      "train loss: 0.0002812545151435788\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1063:\n",
      "train loss: 0.00022885130824101807\n",
      "lr: 5.920529220333995e-05\n",
      "Epoch 1064:\n",
      "train loss: 0.00034340915295285923\n",
      "Epoch 01066: reducing learning rate of group 0 to 5.6245e-05.\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1065:\n",
      "train loss: 0.0003315835820186536\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1066:\n",
      "train loss: 0.00020486810748190486\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1067:\n",
      "train loss: 0.00016674414162200766\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1068:\n",
      "train loss: 0.00036100337022879384\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1069:\n",
      "train loss: 0.0003389013358397152\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1070:\n",
      "train loss: 0.00018689073551472556\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1071:\n",
      "train loss: 0.0001671063686587658\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1072:\n",
      "train loss: 0.0003403219625513061\n",
      "lr: 5.6245027593172946e-05\n",
      "Epoch 1073:\n",
      "train loss: 0.00030090132875239197\n",
      "Epoch 01075: reducing learning rate of group 0 to 5.3433e-05.\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1074:\n",
      "train loss: 0.00023962355823108677\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1075:\n",
      "train loss: 0.00022880116392192946\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1076:\n",
      "train loss: 0.00024807113623222314\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1077:\n",
      "train loss: 0.00020408183535323624\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1078:\n",
      "train loss: 0.0003088934321528455\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1079:\n",
      "train loss: 0.0002964238803159737\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1080:\n",
      "train loss: 0.000189426839149814\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1081:\n",
      "train loss: 0.00015476230588813673\n",
      "lr: 5.3432776213514294e-05\n",
      "Epoch 1082:\n",
      "train loss: 0.0003457526602553666\n",
      "Epoch 01084: reducing learning rate of group 0 to 5.0761e-05.\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1083:\n",
      "train loss: 0.00032353016918749506\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1084:\n",
      "train loss: 0.00017520122382293603\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1085:\n",
      "train loss: 0.00015634160647276255\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1086:\n",
      "train loss: 0.0003034989769130966\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1087:\n",
      "train loss: 0.00026849554423222325\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1088:\n",
      "train loss: 0.00021714753619589714\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1089:\n",
      "train loss: 0.00020629676414208066\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1090:\n",
      "train loss: 0.0002482701126989661\n",
      "lr: 5.0761137402838575e-05\n",
      "Epoch 1091:\n",
      "train loss: 0.00020719578342761206\n",
      "Epoch 01093: reducing learning rate of group 0 to 4.8223e-05.\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1092:\n",
      "train loss: 0.0002798755045200574\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1093:\n",
      "train loss: 0.00026927898811799104\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1094:\n",
      "train loss: 0.00016729415478709003\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1095:\n",
      "train loss: 0.00013399319757058817\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1096:\n",
      "train loss: 0.00032016341923380066\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1097:\n",
      "train loss: 0.00030306684220776317\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1098:\n",
      "train loss: 0.00014352356980222437\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1099:\n",
      "train loss: 0.00012354362127287458\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1100:\n",
      "train loss: 0.0003153251655502143\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1101:\n",
      "train loss: 0.0002849021946566954\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1102:\n",
      "train loss: 0.0001749424164280393\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1103:\n",
      "train loss: 0.000165636526115154\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1104:\n",
      "train loss: 0.000262908224907851\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1105:\n",
      "train loss: 0.00022201175839691136\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1106:\n",
      "train loss: 0.0002438389106046792\n",
      "lr: 4.822308053269664e-05\n",
      "Epoch 1107:\n",
      "train loss: 0.00023676989585720272\n",
      "Epoch 01109: reducing learning rate of group 0 to 4.5812e-05.\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1108:\n",
      "train loss: 0.00019479915521934\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1109:\n",
      "train loss: 0.0001557696759963799\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1110:\n",
      "train loss: 0.0002818626320393197\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1111:\n",
      "train loss: 0.00027121318354352794\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1112:\n",
      "train loss: 0.00014649817259799984\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1113:\n",
      "train loss: 0.00011951597993082099\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1114:\n",
      "train loss: 0.00030607404493731476\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1115:\n",
      "train loss: 0.0002849280604172286\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1116:\n",
      "train loss: 0.00014495287574754233\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1117:\n",
      "train loss: 0.00013172870210201383\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1118:\n",
      "train loss: 0.00027861146685560534\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1119:\n",
      "train loss: 0.00024356714844701047\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1120:\n",
      "train loss: 0.0001975847541124586\n",
      "lr: 4.581192650606181e-05\n",
      "Epoch 1121:\n",
      "train loss: 0.0001909111545854723\n",
      "Epoch 01123: reducing learning rate of group 0 to 4.3521e-05.\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1122:\n",
      "train loss: 0.00021584055648103797\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1123:\n",
      "train loss: 0.00017590103507444694\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1124:\n",
      "train loss: 0.00024364755238330253\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1125:\n",
      "train loss: 0.00023705636938507141\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1126:\n",
      "train loss: 0.00015425755875328364\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1127:\n",
      "train loss: 0.00012184126583455093\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1128:\n",
      "train loss: 0.00028990642039038644\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1129:\n",
      "train loss: 0.000277006868531094\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1130:\n",
      "train loss: 0.0001239596832976062\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1131:\n",
      "train loss: 0.0001054672304350963\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1132:\n",
      "train loss: 0.0002902189427730902\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1133:\n",
      "train loss: 0.0002634909218914315\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1134:\n",
      "train loss: 0.00015183465671932069\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1135:\n",
      "train loss: 0.00014513892879292382\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1136:\n",
      "train loss: 0.00023829538215305426\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1137:\n",
      "train loss: 0.00019958109922595853\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1138:\n",
      "train loss: 0.00022266837903488844\n",
      "lr: 4.3521330180758716e-05\n",
      "Epoch 1139:\n",
      "train loss: 0.0002178426754917663\n",
      "Epoch 01141: reducing learning rate of group 0 to 4.1345e-05.\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1140:\n",
      "train loss: 0.00016934488635431675\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1141:\n",
      "train loss: 0.00013252437958180915\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1142:\n",
      "train loss: 0.00026315655377747024\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1143:\n",
      "train loss: 0.000254706293873602\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1144:\n",
      "train loss: 0.00012196690329253867\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1145:\n",
      "train loss: 9.914268995410232e-05\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1146:\n",
      "train loss: 0.00028243422206419144\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1147:\n",
      "train loss: 0.0002624258289490441\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1148:\n",
      "train loss: 0.00012853780688116425\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1149:\n",
      "train loss: 0.0001214825368447588\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1150:\n",
      "train loss: 0.0002421395206833181\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1151:\n",
      "train loss: 0.00020617511812397258\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1152:\n",
      "train loss: 0.00019615595948309432\n",
      "lr: 4.134526367172078e-05\n",
      "Epoch 1153:\n",
      "train loss: 0.00019265809140643912\n",
      "Epoch 01155: reducing learning rate of group 0 to 3.9278e-05.\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1154:\n",
      "train loss: 0.0001717680699832274\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1155:\n",
      "train loss: 0.00013404012995623402\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1156:\n",
      "train loss: 0.00024444097427310144\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1157:\n",
      "train loss: 0.00023839094151459068\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1158:\n",
      "train loss: 0.00011675751792178167\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1159:\n",
      "train loss: 9.221237155059123e-05\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1160:\n",
      "train loss: 0.0002730677805805271\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1161:\n",
      "train loss: 0.0002571622831338584\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1162:\n",
      "train loss: 0.00011242502836247263\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1163:\n",
      "train loss: 0.00010666179401484239\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1164:\n",
      "train loss: 0.00023726040244049934\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1165:\n",
      "train loss: 0.000203430041836546\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1166:\n",
      "train loss: 0.00017968309054504955\n",
      "lr: 3.927800048813474e-05\n",
      "Epoch 1167:\n",
      "train loss: 0.00017663035218711822\n",
      "Epoch 01169: reducing learning rate of group 0 to 3.7314e-05.\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1168:\n",
      "train loss: 0.00016797745554594418\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1169:\n",
      "train loss: 0.00013182686558746093\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1170:\n",
      "train loss: 0.00022771750650035818\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1171:\n",
      "train loss: 0.00022103903532931263\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1172:\n",
      "train loss: 0.00011690525798642211\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1173:\n",
      "train loss: 9.37588147093154e-05\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1174:\n",
      "train loss: 0.00025276768645175654\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1175:\n",
      "train loss: 0.00023718396045363673\n",
      "lr: 3.7314100463728e-05\n",
      "Epoch 1176:\n",
      "train loss: 0.00011414383954416883\n",
      "Epoch 01178: reducing learning rate of group 0 to 3.5448e-05.\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1177:\n",
      "train loss: 0.00010700660280905064\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1178:\n",
      "train loss: 0.00022153689275330725\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1179:\n",
      "train loss: 0.00019249851052539224\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1180:\n",
      "train loss: 0.00015264728551046285\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1181:\n",
      "train loss: 0.00015010444335531622\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1182:\n",
      "train loss: 0.00015998268974506106\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1183:\n",
      "train loss: 0.00012734836954533086\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1184:\n",
      "train loss: 0.0002138741407704166\n",
      "lr: 3.54483954405416e-05\n",
      "Epoch 1185:\n",
      "train loss: 0.0002067768757869484\n",
      "Epoch 01187: reducing learning rate of group 0 to 3.3676e-05.\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1186:\n",
      "train loss: 0.00011450598750859104\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1187:\n",
      "train loss: 9.294447504698892e-05\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1188:\n",
      "train loss: 0.00021897266727525616\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1189:\n",
      "train loss: 0.00020413690847527575\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1190:\n",
      "train loss: 0.00011233682112367274\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1191:\n",
      "train loss: 0.00010328422542848619\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1192:\n",
      "train loss: 0.00019630125998966734\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1193:\n",
      "train loss: 0.00017132523550816666\n",
      "lr: 3.367597566851452e-05\n",
      "Epoch 1194:\n",
      "train loss: 0.00015230458976340947\n",
      "Epoch 01196: reducing learning rate of group 0 to 3.1992e-05.\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1195:\n",
      "train loss: 0.00014543432700546983\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1196:\n",
      "train loss: 0.00015442463068031498\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1197:\n",
      "train loss: 0.00012944178756582172\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1198:\n",
      "train loss: 0.0001751758536402347\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1199:\n",
      "train loss: 0.00016623712622966833\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1200:\n",
      "train loss: 0.00012464029273734313\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1201:\n",
      "train loss: 0.00010495038287130613\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1202:\n",
      "train loss: 0.00019251147035517657\n",
      "lr: 3.199217688508879e-05\n",
      "Epoch 1203:\n",
      "train loss: 0.00017832409295910588\n",
      "Epoch 01205: reducing learning rate of group 0 to 3.0393e-05.\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1204:\n",
      "train loss: 0.00011939265535572798\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1205:\n",
      "train loss: 0.00010647093963762222\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1206:\n",
      "train loss: 0.00016952441515380518\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1207:\n",
      "train loss: 0.00015045452459057566\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1208:\n",
      "train loss: 0.00013669657442690004\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1209:\n",
      "train loss: 0.00012723739891347985\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1210:\n",
      "train loss: 0.0001472794950759835\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1211:\n",
      "train loss: 0.00012646594947098605\n",
      "lr: 3.039256804083435e-05\n",
      "Epoch 1212:\n",
      "train loss: 0.00016050626153708893\n",
      "Epoch 01214: reducing learning rate of group 0 to 2.8873e-05.\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1213:\n",
      "train loss: 0.00015070982934091453\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1214:\n",
      "train loss: 0.00012580930036838747\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1215:\n",
      "train loss: 0.0001078373710146824\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1216:\n",
      "train loss: 0.00016209064047186802\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1217:\n",
      "train loss: 0.00015062613684185673\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1218:\n",
      "train loss: 0.00011481677502155078\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1219:\n",
      "train loss: 9.96901879677373e-05\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1220:\n",
      "train loss: 0.00016716590284124864\n",
      "lr: 2.887293963879263e-05\n",
      "Epoch 1221:\n",
      "train loss: 0.00015297497794010956\n",
      "Epoch 01223: reducing learning rate of group 0 to 2.7429e-05.\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1222:\n",
      "train loss: 0.00011502212371807144\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1223:\n",
      "train loss: 0.00010232933920295242\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1224:\n",
      "train loss: 0.00014913757941782799\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1225:\n",
      "train loss: 0.0001337573713098261\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1226:\n",
      "train loss: 0.00012216737793277965\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1227:\n",
      "train loss: 0.00011112113658416993\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1228:\n",
      "train loss: 0.0001398653671209976\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1229:\n",
      "train loss: 0.0001240517139734263\n",
      "lr: 2.7429292656852997e-05\n",
      "Epoch 1230:\n",
      "train loss: 0.00013180620709377468\n",
      "Epoch 01232: reducing learning rate of group 0 to 2.6058e-05.\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1231:\n",
      "train loss: 0.00012061421228461366\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1232:\n",
      "train loss: 0.00013090740788178107\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1233:\n",
      "train loss: 0.00011646861574386472\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1234:\n",
      "train loss: 0.00012579512832129323\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1235:\n",
      "train loss: 0.00011440240980273657\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1236:\n",
      "train loss: 0.00012535219581097834\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1237:\n",
      "train loss: 0.00011179434929522613\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1238:\n",
      "train loss: 0.0001295755676447273\n",
      "lr: 2.6057828024010345e-05\n",
      "Epoch 1239:\n",
      "train loss: 0.00011733395252791582\n",
      "Epoch 01241: reducing learning rate of group 0 to 2.4755e-05.\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1240:\n",
      "train loss: 0.00012315922339434002\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1241:\n",
      "train loss: 0.00011037046645017035\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1242:\n",
      "train loss: 0.00011828827848786484\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1243:\n",
      "train loss: 0.00010610291167786377\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1244:\n",
      "train loss: 0.00012276324746673286\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1245:\n",
      "train loss: 0.00011100076630997815\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1246:\n",
      "train loss: 0.00011734779267897301\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1247:\n",
      "train loss: 0.00010495609280110961\n",
      "lr: 2.4754936622809825e-05\n",
      "Epoch 1248:\n",
      "train loss: 0.00012400423173374546\n",
      "Epoch 01250: reducing learning rate of group 0 to 2.3517e-05.\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1249:\n",
      "train loss: 0.0001123258620305349\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1250:\n",
      "train loss: 0.00011595224967741097\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1251:\n",
      "train loss: 0.00010420281951615837\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1252:\n",
      "train loss: 0.00011323753689026171\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1253:\n",
      "train loss: 0.00010207459049205004\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1254:\n",
      "train loss: 0.0001148205583456019\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1255:\n",
      "train loss: 0.00010316327824344215\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1256:\n",
      "train loss: 0.00011416866757655912\n",
      "lr: 2.3517189791669335e-05\n",
      "Epoch 1257:\n",
      "train loss: 0.00010292631167629047\n",
      "Epoch 01259: reducing learning rate of group 0 to 2.2341e-05.\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1258:\n",
      "train loss: 0.00011399440107653138\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1259:\n",
      "train loss: 0.00010240213396040821\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1260:\n",
      "train loss: 0.00010399120992305606\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1261:\n",
      "train loss: 9.328170166183243e-05\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1262:\n",
      "train loss: 0.00011277951363414917\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1263:\n",
      "train loss: 0.00010177952416721598\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1264:\n",
      "train loss: 0.00010457519402239061\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1265:\n",
      "train loss: 9.387627973868073e-05\n",
      "lr: 2.2341330302085865e-05\n",
      "Epoch 1266:\n",
      "train loss: 0.00011214104920186511\n",
      "Epoch 01268: reducing learning rate of group 0 to 2.1224e-05.\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1267:\n",
      "train loss: 0.0001011230274634726\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1268:\n",
      "train loss: 0.00010521245668458053\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1269:\n",
      "train loss: 9.507200981062058e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1270:\n",
      "train loss: 0.00010059634749684892\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1271:\n",
      "train loss: 9.011409615738372e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1272:\n",
      "train loss: 0.00010587914665331188\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1273:\n",
      "train loss: 9.575052356634261e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1274:\n",
      "train loss: 9.988206525673127e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1275:\n",
      "train loss: 8.939816947409439e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1276:\n",
      "train loss: 0.00010656247148155614\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1277:\n",
      "train loss: 9.643438274177467e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1278:\n",
      "train loss: 9.916936100678794e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1279:\n",
      "train loss: 8.869240447485027e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1280:\n",
      "train loss: 0.00010723119436291963\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1281:\n",
      "train loss: 9.709816785031114e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1282:\n",
      "train loss: 9.847903529898249e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1283:\n",
      "train loss: 8.801030662876305e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1284:\n",
      "train loss: 0.00010787707871457339\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1285:\n",
      "train loss: 9.774051767094242e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1286:\n",
      "train loss: 9.780823925802617e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1287:\n",
      "train loss: 8.734438929788139e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1288:\n",
      "train loss: 0.00010851026317461653\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1289:\n",
      "train loss: 9.83741750490452e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1290:\n",
      "train loss: 9.714320782758963e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1291:\n",
      "train loss: 8.668057474841994e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1292:\n",
      "train loss: 0.00010914428371413195\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1293:\n",
      "train loss: 9.901153148025369e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1294:\n",
      "train loss: 9.64728676162409e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1295:\n",
      "train loss: 8.600963646135051e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1296:\n",
      "train loss: 0.00010978662553656731\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1297:\n",
      "train loss: 9.965800295253535e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1298:\n",
      "train loss: 9.579345662738823e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1299:\n",
      "train loss: 8.532970742534586e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1300:\n",
      "train loss: 0.00011043774807948495\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1301:\n",
      "train loss: 0.00010031253277346057\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1302:\n",
      "train loss: 9.510700104583708e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1303:\n",
      "train loss: 8.464363899933932e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1304:\n",
      "train loss: 0.00011109432495196643\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1305:\n",
      "train loss: 0.00010097148604141334\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1306:\n",
      "train loss: 9.441724725097874e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1307:\n",
      "train loss: 8.395516984216864e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1308:\n",
      "train loss: 0.00011175278818297132\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1309:\n",
      "train loss: 0.0001016315336359825\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1310:\n",
      "train loss: 9.372722320227922e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1311:\n",
      "train loss: 8.326690906511596e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1312:\n",
      "train loss: 0.00011241093680958699\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1313:\n",
      "train loss: 0.00010229098694494332\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1314:\n",
      "train loss: 9.303821130016598e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1315:\n",
      "train loss: 8.257973344659214e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1316:\n",
      "train loss: 0.00011306822136127474\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1317:\n",
      "train loss: 0.00010294959373889047\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1318:\n",
      "train loss: 9.235023606859219e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1319:\n",
      "train loss: 8.18934733888194e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1320:\n",
      "train loss: 0.00011372494693792725\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1321:\n",
      "train loss: 0.00010360778636965342\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1322:\n",
      "train loss: 9.166278450710669e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1323:\n",
      "train loss: 8.12075773523348e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1324:\n",
      "train loss: 0.000114381665229053\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1325:\n",
      "train loss: 0.00010426607925245392\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1326:\n",
      "train loss: 9.097538166045953e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1327:\n",
      "train loss: 8.052164425601682e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1328:\n",
      "train loss: 0.00011503870353386074\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1329:\n",
      "train loss: 0.00010492471348208769\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1330:\n",
      "train loss: 9.028785012241535e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1331:\n",
      "train loss: 7.983556209146647e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1332:\n",
      "train loss: 0.00011569612149239782\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1333:\n",
      "train loss: 0.00010558370134453229\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1334:\n",
      "train loss: 8.960021265411742e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1335:\n",
      "train loss: 7.914939078915415e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1336:\n",
      "train loss: 0.00011635382530859585\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1337:\n",
      "train loss: 0.00010624291724826312\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1338:\n",
      "train loss: 8.891261267323858e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1339:\n",
      "train loss: 7.846328229962564e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1340:\n",
      "train loss: 0.00011701166658148675\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1341:\n",
      "train loss: 0.00010690223231634865\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1342:\n",
      "train loss: 8.822516188754672e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1343:\n",
      "train loss: 7.777733089750717e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1344:\n",
      "train loss: 0.0001176695568584832\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1345:\n",
      "train loss: 0.00010756156319099015\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1346:\n",
      "train loss: 8.753793685822908e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1347:\n",
      "train loss: 7.709160732867674e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1348:\n",
      "train loss: 0.00011832743555748935\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1349:\n",
      "train loss: 0.0001082208681704022\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1350:\n",
      "train loss: 8.685096288125948e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1351:\n",
      "train loss: 7.640612321704057e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1352:\n",
      "train loss: 0.00011898529396807108\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1353:\n",
      "train loss: 0.00010888013878823863\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1354:\n",
      "train loss: 8.6164246205929e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1355:\n",
      "train loss: 7.572088704933542e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1356:\n",
      "train loss: 0.00011964312270796086\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1357:\n",
      "train loss: 0.00010953936963421378\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1358:\n",
      "train loss: 8.547778689186392e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1359:\n",
      "train loss: 7.503589572465729e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1360:\n",
      "train loss: 0.0001203009234338007\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1361:\n",
      "train loss: 0.00011019856025001564\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1362:\n",
      "train loss: 8.47915847927063e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1363:\n",
      "train loss: 7.43511516977931e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1364:\n",
      "train loss: 0.00012095868976557023\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1365:\n",
      "train loss: 0.00011085770415703417\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1366:\n",
      "train loss: 8.410564420295801e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1367:\n",
      "train loss: 7.366665917652209e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1368:\n",
      "train loss: 0.00012161641628245589\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1369:\n",
      "train loss: 0.00011151679573726354\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1370:\n",
      "train loss: 8.341996864629952e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1371:\n",
      "train loss: 7.298242168054163e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1372:\n",
      "train loss: 0.0001222740964317206\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1373:\n",
      "train loss: 0.00011217582845678564\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1374:\n",
      "train loss: 8.273456251949977e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1375:\n",
      "train loss: 7.229844401485459e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1376:\n",
      "train loss: 0.00012293172446275103\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1377:\n",
      "train loss: 0.00011283479743869914\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1378:\n",
      "train loss: 8.204942790902156e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1379:\n",
      "train loss: 7.161472717321538e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1380:\n",
      "train loss: 0.00012358929714861717\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1381:\n",
      "train loss: 0.00011349369960196011\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1382:\n",
      "train loss: 8.136456575730673e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1383:\n",
      "train loss: 7.093127266275865e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1384:\n",
      "train loss: 0.00012424681162678228\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1385:\n",
      "train loss: 0.00011415253294442778\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1386:\n",
      "train loss: 8.067997541518191e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1387:\n",
      "train loss: 7.024807888933888e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1388:\n",
      "train loss: 0.00012490426749258395\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1389:\n",
      "train loss: 0.00011481129700543074\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1390:\n",
      "train loss: 7.999565543502065e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1391:\n",
      "train loss: 6.956514504377326e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1392:\n",
      "train loss: 0.00012556166375927275\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1393:\n",
      "train loss: 0.00011546999138474051\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1394:\n",
      "train loss: 7.931160384284353e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1395:\n",
      "train loss: 6.888246859384016e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1396:\n",
      "train loss: 0.00012621900100587513\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1397:\n",
      "train loss: 0.00011612861647260201\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1398:\n",
      "train loss: 7.862781851246503e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1399:\n",
      "train loss: 6.820004799140031e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1400:\n",
      "train loss: 0.00012687627882050973\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1401:\n",
      "train loss: 0.00011678717226609706\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1402:\n",
      "train loss: 7.794429724373624e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1403:\n",
      "train loss: 6.751788072247294e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1404:\n",
      "train loss: 0.00012753349781295667\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1405:\n",
      "train loss: 0.00011744565922242491\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1406:\n",
      "train loss: 7.726103787216283e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1407:\n",
      "train loss: 6.683596506222907e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1408:\n",
      "train loss: 0.00012819065776017101\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1409:\n",
      "train loss: 0.00011810407741687107\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1410:\n",
      "train loss: 7.65780382508674e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1411:\n",
      "train loss: 6.615429869443376e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1412:\n",
      "train loss: 0.0001288477591289935\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1413:\n",
      "train loss: 0.00011876242726874043\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1414:\n",
      "train loss: 7.589529621385368e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1415:\n",
      "train loss: 6.547287975978435e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1416:\n",
      "train loss: 0.00012950480188036716\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1417:\n",
      "train loss: 0.00011942070893164695\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1418:\n",
      "train loss: 7.521280967031481e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1419:\n",
      "train loss: 6.479170610718494e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1420:\n",
      "train loss: 0.0001301617863640577\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1421:\n",
      "train loss: 0.00012007892280537661\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1422:\n",
      "train loss: 7.45305764325466e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1423:\n",
      "train loss: 6.411077575345469e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1424:\n",
      "train loss: 0.00013081871270862312\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1425:\n",
      "train loss: 0.00012073706912501983\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1426:\n",
      "train loss: 7.38485944558217e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1427:\n",
      "train loss: 6.343008667803992e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1428:\n",
      "train loss: 0.0001314755811787768\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1429:\n",
      "train loss: 0.00012139514826859244\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1430:\n",
      "train loss: 7.316686155591836e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1431:\n",
      "train loss: 6.274963683272655e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1432:\n",
      "train loss: 0.00013213239201304052\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1433:\n",
      "train loss: 0.00012205316052182938\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1434:\n",
      "train loss: 7.248537573189767e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1435:\n",
      "train loss: 6.206942429234332e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1436:\n",
      "train loss: 0.00013278914541638777\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1437:\n",
      "train loss: 0.00012271110623480457\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1438:\n",
      "train loss: 7.180413484031013e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1439:\n",
      "train loss: 6.138944701077806e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1440:\n",
      "train loss: 0.00013344584166883865\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1441:\n",
      "train loss: 0.00012336898570888965\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1442:\n",
      "train loss: 7.112313693394166e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1443:\n",
      "train loss: 6.070970314562338e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1444:\n",
      "train loss: 0.00013410248092726775\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1445:\n",
      "train loss: 0.00012402679925568424\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1446:\n",
      "train loss: 7.04423799395626e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1447:\n",
      "train loss: 6.003019070821332e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1448:\n",
      "train loss: 0.0001347590634635471\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1449:\n",
      "train loss: 0.00012468454714870909\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1450:\n",
      "train loss: 6.9761861993931e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1451:\n",
      "train loss: 5.9350907943634474e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1452:\n",
      "train loss: 0.00013541558937110333\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1453:\n",
      "train loss: 0.00012534222963847893\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1454:\n",
      "train loss: 6.908158111780847e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1455:\n",
      "train loss: 5.867185296352846e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1456:\n",
      "train loss: 0.00013607205886430963\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1457:\n",
      "train loss: 0.00012599984692912\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1458:\n",
      "train loss: 6.840153557557465e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1459:\n",
      "train loss: 5.7993024141993944e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1460:\n",
      "train loss: 0.00013672847193560577\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1461:\n",
      "train loss: 0.00012665739916244053\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1462:\n",
      "train loss: 6.772172353496015e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1463:\n",
      "train loss: 5.731441974436955e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1464:\n",
      "train loss: 0.00013738482867486337\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1465:\n",
      "train loss: 0.00012731488639904193\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1466:\n",
      "train loss: 6.704214346808706e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1467:\n",
      "train loss: 5.663603836578831e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1468:\n",
      "train loss: 0.00013804112887400315\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1469:\n",
      "train loss: 0.00012797230856989076\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1470:\n",
      "train loss: 6.636279378889262e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1471:\n",
      "train loss: 5.595787853368111e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1472:\n",
      "train loss: 0.00013869737237469753\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1473:\n",
      "train loss: 0.00012862966544902805\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1474:\n",
      "train loss: 6.568367334445199e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1475:\n",
      "train loss: 5.5279939248719264e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1476:\n",
      "train loss: 0.0001393535585570344\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1477:\n",
      "train loss: 0.00012928695652888707\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1478:\n",
      "train loss: 6.500478103735106e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1479:\n",
      "train loss: 5.460221957190311e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1480:\n",
      "train loss: 0.00014000968670639834\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1481:\n",
      "train loss: 0.0001299441809457986\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1482:\n",
      "train loss: 6.432611648565746e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1483:\n",
      "train loss: 5.392471934740359e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1484:\n",
      "train loss: 0.00014066575527708\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1485:\n",
      "train loss: 0.00013060133717931435\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1486:\n",
      "train loss: 6.364767971382001e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1487:\n",
      "train loss: 5.3247438894783506e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1488:\n",
      "train loss: 0.00014132176216999218\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1489:\n",
      "train loss: 0.00013125842278582925\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1490:\n",
      "train loss: 6.296947217297066e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1491:\n",
      "train loss: 5.2570380098859085e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1492:\n",
      "train loss: 0.00014197770348363417\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1493:\n",
      "train loss: 0.00013191543358480147\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1494:\n",
      "train loss: 6.229149686909013e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1495:\n",
      "train loss: 5.189354669732688e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1496:\n",
      "train loss: 0.00014263357318799276\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1497:\n",
      "train loss: 0.00013257236263624885\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1498:\n",
      "train loss: 6.161376042656947e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1499:\n",
      "train loss: 5.1216946467896555e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1500:\n",
      "train loss: 0.00014328936032098413\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1501:\n",
      "train loss: 0.00013322919757992376\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1502:\n",
      "train loss: 6.093627518618776e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1503:\n",
      "train loss: 5.054059407499747e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1504:\n",
      "train loss: 0.0001439450457641608\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1505:\n",
      "train loss: 0.00013388591620445143\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1506:\n",
      "train loss: 6.025906547589672e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1507:\n",
      "train loss: 4.9864518049163255e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1508:\n",
      "train loss: 0.00014460059282268498\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1509:\n",
      "train loss: 0.00013454247562894928\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1510:\n",
      "train loss: 5.958217924233149e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1511:\n",
      "train loss: 4.9188775812079866e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1512:\n",
      "train loss: 0.00014525592878682718\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1513:\n",
      "train loss: 0.00013519878950272877\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1514:\n",
      "train loss: 5.8905716550293614e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1515:\n",
      "train loss: 4.851348770987091e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1516:\n",
      "train loss: 0.00014591089828232628\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1517:\n",
      "train loss: 0.0001358546704105662\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1518:\n",
      "train loss: 5.822989800995014e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1519:\n",
      "train loss: 4.7838925250741683e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1520:\n",
      "train loss: 0.00014656514603382216\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1521:\n",
      "train loss: 0.00013650968354028169\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1522:\n",
      "train loss: 5.755524514321153e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1523:\n",
      "train loss: 4.7165740158228523e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1524:\n",
      "train loss: 0.0001472177932559611\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1525:\n",
      "train loss: 0.00013716273417503782\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1526:\n",
      "train loss: 5.6883090426258336e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1527:\n",
      "train loss: 4.649563807330626e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1528:\n",
      "train loss: 0.00014786647558235323\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1529:\n",
      "train loss: 0.000137810830032421\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1530:\n",
      "train loss: 5.621713011664615e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1531:\n",
      "train loss: 4.583345500171509e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1532:\n",
      "train loss: 0.00014850425696114807\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1533:\n",
      "train loss: 0.00013844504171652968\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1534:\n",
      "train loss: 5.556854942763218e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1535:\n",
      "train loss: 4.5194193409052555e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1536:\n",
      "train loss: 0.0001491088952420709\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1537:\n",
      "train loss: 0.0001390362872621136\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1538:\n",
      "train loss: 5.497425367824083e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1539:\n",
      "train loss: 4.4628510628213744e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1540:\n",
      "train loss: 0.00014960283256781578\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1541:\n",
      "train loss: 0.0001394817167466187\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1542:\n",
      "train loss: 5.456621075946745e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1543:\n",
      "train loss: 4.432077207963555e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1544:\n",
      "train loss: 0.00014969785491344214\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1545:\n",
      "train loss: 0.00013939559144429006\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1546:\n",
      "train loss: 5.484203730723445e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1547:\n",
      "train loss: 4.496124388988797e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1548:\n",
      "train loss: 0.00014833826677127383\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1549:\n",
      "train loss: 0.0001373736054322342\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1550:\n",
      "train loss: 5.758938551327024e-05\n",
      "lr: 2.1224263786981572e-05\n",
      "Epoch 1551:\n",
      "train loss: 4.881780032211127e-05\n",
      "Epoch 01553: reducing learning rate of group 0 to 2.0163e-05.\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1552:\n",
      "train loss: 0.0001426232029187712\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1553:\n",
      "train loss: 0.00012983623258126274\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1554:\n",
      "train loss: 5.7266895781210524e-05\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1555:\n",
      "train loss: 5.135353858481991e-05\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1556:\n",
      "train loss: 0.00012728180893151086\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1557:\n",
      "train loss: 0.00011180680978416128\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1558:\n",
      "train loss: 7.832318689194921e-05\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1559:\n",
      "train loss: 7.384113163052457e-05\n",
      "lr: 2.0163050597632492e-05\n",
      "Epoch 1560:\n",
      "train loss: 0.00010561425023609462\n",
      "Epoch 01562: reducing learning rate of group 0 to 1.9155e-05.\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1561:\n",
      "train loss: 9.103035332580935e-05\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1562:\n",
      "train loss: 9.776829716164218e-05\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1563:\n",
      "train loss: 9.234118631450475e-05\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1564:\n",
      "train loss: 7.92607508523918e-05\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1565:\n",
      "train loss: 6.705398593189926e-05\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1566:\n",
      "train loss: 0.00011055590316296239\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1567:\n",
      "train loss: 0.00010364905894481342\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1568:\n",
      "train loss: 6.9475550479114e-05\n",
      "lr: 1.9154898067750865e-05\n",
      "Epoch 1569:\n",
      "train loss: 5.862096487479245e-05\n",
      "Epoch 01571: reducing learning rate of group 0 to 1.8197e-05.\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1570:\n",
      "train loss: 0.00011752446761255497\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1571:\n",
      "train loss: 0.00010896486322794409\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1572:\n",
      "train loss: 5.71486794136312e-05\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1573:\n",
      "train loss: 4.860229382673228e-05\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1574:\n",
      "train loss: 0.00011688862381031532\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1575:\n",
      "train loss: 0.00010707326091818599\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1576:\n",
      "train loss: 6.042085041839772e-05\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1577:\n",
      "train loss: 5.28774109047503e-05\n",
      "lr: 1.8197153164363322e-05\n",
      "Epoch 1578:\n",
      "train loss: 0.00011215715286227198\n",
      "Epoch 01580: reducing learning rate of group 0 to 1.7287e-05.\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1579:\n",
      "train loss: 0.00010210994505857928\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1580:\n",
      "train loss: 6.541795522616493e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1581:\n",
      "train loss: 5.816805025182831e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1582:\n",
      "train loss: 9.884375850117477e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1583:\n",
      "train loss: 8.964040706933743e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1584:\n",
      "train loss: 6.914831761172484e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1585:\n",
      "train loss: 6.157441682196207e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1586:\n",
      "train loss: 9.571002712561083e-05\n",
      "lr: 1.7287295506145154e-05\n",
      "Epoch 1587:\n",
      "train loss: 8.673295831710333e-05\n",
      "Epoch 01589: reducing learning rate of group 0 to 1.6423e-05.\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1588:\n",
      "train loss: 7.187494725533373e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1589:\n",
      "train loss: 6.412857042539116e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1590:\n",
      "train loss: 8.544832910143774e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1591:\n",
      "train loss: 7.706530524849523e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1592:\n",
      "train loss: 7.345602349858976e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1593:\n",
      "train loss: 6.597569930068487e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1594:\n",
      "train loss: 8.36974333277782e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1595:\n",
      "train loss: 7.544288023016296e-05\n",
      "lr: 1.6422930730837896e-05\n",
      "Epoch 1596:\n",
      "train loss: 7.493538347625365e-05\n",
      "Epoch 01598: reducing learning rate of group 0 to 1.5602e-05.\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1597:\n",
      "train loss: 6.734522136343779e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1598:\n",
      "train loss: 8.241378302973754e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1599:\n",
      "train loss: 7.46499467591419e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1600:\n",
      "train loss: 6.813829588633632e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1601:\n",
      "train loss: 6.0865354479816113e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1602:\n",
      "train loss: 8.145556021074459e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1603:\n",
      "train loss: 7.374442533590287e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1604:\n",
      "train loss: 6.898220634858272e-05\n",
      "lr: 1.5601784194296002e-05\n",
      "Epoch 1605:\n",
      "train loss: 6.166937574801203e-05\n",
      "Epoch 01607: reducing learning rate of group 0 to 1.4822e-05.\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1606:\n",
      "train loss: 8.067445627774025e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1607:\n",
      "train loss: 7.30039692410706e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1608:\n",
      "train loss: 6.253699515496386e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1609:\n",
      "train loss: 5.555700392447127e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1610:\n",
      "train loss: 7.968822795058611e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1611:\n",
      "train loss: 7.242699771134707e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1612:\n",
      "train loss: 6.308242755114654e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1613:\n",
      "train loss: 5.608319476741331e-05\n",
      "lr: 1.48216949845812e-05\n",
      "Epoch 1614:\n",
      "train loss: 7.91694735426747e-05\n",
      "Epoch 01616: reducing learning rate of group 0 to 1.4081e-05.\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1615:\n",
      "train loss: 7.192473530325846e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1616:\n",
      "train loss: 6.355833522839134e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1617:\n",
      "train loss: 5.6897755764922156e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1618:\n",
      "train loss: 7.159155608295327e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1619:\n",
      "train loss: 6.472258520500126e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1620:\n",
      "train loss: 6.396104417307024e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1621:\n",
      "train loss: 5.728934973780345e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1622:\n",
      "train loss: 7.119919105652675e-05\n",
      "lr: 1.4080610235352139e-05\n",
      "Epoch 1623:\n",
      "train loss: 6.433977370033521e-05\n",
      "Epoch 01625: reducing learning rate of group 0 to 1.3377e-05.\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1624:\n",
      "train loss: 6.432535920461479e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1625:\n",
      "train loss: 5.764673006593306e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1626:\n",
      "train loss: 6.44135942665559e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1627:\n",
      "train loss: 5.790426359171901e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1628:\n",
      "train loss: 6.431039176761297e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1629:\n",
      "train loss: 5.796046960814062e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1630:\n",
      "train loss: 6.409450906059447e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1631:\n",
      "train loss: 5.7590830022968796e-05\n",
      "lr: 1.3376579723584532e-05\n",
      "Epoch 1632:\n",
      "train loss: 6.460854899828068e-05\n",
      "Epoch 01634: reducing learning rate of group 0 to 1.2708e-05.\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1633:\n",
      "train loss: 5.825510145529009e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1634:\n",
      "train loss: 6.379312002887377e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1635:\n",
      "train loss: 5.7618141292696885e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1636:\n",
      "train loss: 5.845935161523739e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1637:\n",
      "train loss: 5.2420893072356264e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1638:\n",
      "train loss: 6.351912956915194e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1639:\n",
      "train loss: 5.734773477635206e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1640:\n",
      "train loss: 5.871735836637603e-05\n",
      "lr: 1.2707750737405305e-05\n",
      "Epoch 1641:\n",
      "train loss: 5.2677001737797274e-05\n",
      "Epoch 01643: reducing learning rate of group 0 to 1.2072e-05.\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1642:\n",
      "train loss: 6.325550702406969e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1643:\n",
      "train loss: 5.7086714525319515e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1644:\n",
      "train loss: 5.316439072177084e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1645:\n",
      "train loss: 4.742433107495573e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1646:\n",
      "train loss: 6.27051583138184e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1647:\n",
      "train loss: 5.684753438960262e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1648:\n",
      "train loss: 5.3392503365875715e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1649:\n",
      "train loss: 4.7651079912101596e-05\n",
      "lr: 1.2072363200535039e-05\n",
      "Epoch 1650:\n",
      "train loss: 6.247105136102038e-05\n",
      "Epoch 01652: reducing learning rate of group 0 to 1.1469e-05.\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1651:\n",
      "train loss: 5.661536107396556e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1652:\n",
      "train loss: 5.361437542954916e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1653:\n",
      "train loss: 4.815936387721196e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1654:\n",
      "train loss: 5.644881693426287e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1655:\n",
      "train loss: 5.0887847844146604e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1656:\n",
      "train loss: 5.381977788939172e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1657:\n",
      "train loss: 4.836357126420002e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1658:\n",
      "train loss: 5.623760222692e-05\n",
      "lr: 1.1468745040508286e-05\n",
      "Epoch 1659:\n",
      "train loss: 5.067840733623383e-05\n",
      "Epoch 01661: reducing learning rate of group 0 to 1.0895e-05.\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1660:\n",
      "train loss: 5.4019582011244184e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1661:\n",
      "train loss: 4.8562729473364926e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1662:\n",
      "train loss: 5.0801099937789025e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1663:\n",
      "train loss: 4.552160357114205e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1664:\n",
      "train loss: 5.393171791466502e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1665:\n",
      "train loss: 4.874665813056729e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1666:\n",
      "train loss: 5.061056397468878e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1667:\n",
      "train loss: 4.53326828066011e-05\n",
      "lr: 1.089530778848287e-05\n",
      "Epoch 1668:\n",
      "train loss: 5.4111653338811265e-05\n",
      "Epoch 01670: reducing learning rate of group 0 to 1.0351e-05.\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1669:\n",
      "train loss: 4.892604631730084e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1670:\n",
      "train loss: 5.042416071253497e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1671:\n",
      "train loss: 4.5411307868463385e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1672:\n",
      "train loss: 4.9052950020907324e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1673:\n",
      "train loss: 4.412597119339648e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1674:\n",
      "train loss: 5.0250849382611095e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1675:\n",
      "train loss: 4.523958488133336e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1676:\n",
      "train loss: 4.921619500126464e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1677:\n",
      "train loss: 4.428863351002329e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1678:\n",
      "train loss: 5.008168633867415e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1679:\n",
      "train loss: 4.507159906705396e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1680:\n",
      "train loss: 4.937611730022534e-05\n",
      "lr: 1.0350542399058726e-05\n",
      "Epoch 1681:\n",
      "train loss: 4.444830282291957e-05\n",
      "Epoch 01683: reducing learning rate of group 0 to 9.8330e-06.\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1682:\n",
      "train loss: 4.9915265360911534e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1683:\n",
      "train loss: 4.4906082465119924e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1684:\n",
      "train loss: 4.48120636014035e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1685:\n",
      "train loss: 4.0130209225806667e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1686:\n",
      "train loss: 4.950949655192138e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1687:\n",
      "train loss: 4.4752102049365595e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1688:\n",
      "train loss: 4.495828887034885e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1689:\n",
      "train loss: 4.027601331346394e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1690:\n",
      "train loss: 4.93575354599036e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1691:\n",
      "train loss: 4.460113689417631e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1692:\n",
      "train loss: 4.51018428775288e-05\n",
      "lr: 9.833015279105789e-06\n",
      "Epoch 1693:\n",
      "train loss: 4.041941159734241e-05\n",
      "Epoch 01695: reducing learning rate of group 0 to 9.3414e-06.\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1694:\n",
      "train loss: 4.9207785698163104e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1695:\n",
      "train loss: 4.445216407789058e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1696:\n",
      "train loss: 4.075903280472757e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1697:\n",
      "train loss: 3.631038362675085e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1698:\n",
      "train loss: 4.883009629975054e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1699:\n",
      "train loss: 4.4313435094163844e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1700:\n",
      "train loss: 4.0890597409287624e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1701:\n",
      "train loss: 3.644160270325779e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1702:\n",
      "train loss: 4.869310653356303e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1703:\n",
      "train loss: 4.417733408363999e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1704:\n",
      "train loss: 4.101983706583043e-05\n",
      "lr: 9.341364515150498e-06\n",
      "Epoch 1705:\n",
      "train loss: 3.657072774178288e-05\n",
      "Epoch 01707: reducing learning rate of group 0 to 8.8743e-06.\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1706:\n",
      "train loss: 4.855804466643051e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1707:\n",
      "train loss: 4.404296997352669e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1708:\n",
      "train loss: 3.688818673770439e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1709:\n",
      "train loss: 3.266124638588896e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1710:\n",
      "train loss: 4.8206062683793315e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1711:\n",
      "train loss: 4.3917806826349024e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1712:\n",
      "train loss: 3.7006712357227044e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1713:\n",
      "train loss: 3.2779476833090644e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1714:\n",
      "train loss: 4.8082438605666765e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1715:\n",
      "train loss: 4.379498824925635e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1716:\n",
      "train loss: 3.7123165661613974e-05\n",
      "lr: 8.874296289392974e-06\n",
      "Epoch 1717:\n",
      "train loss: 3.2895840991114214e-05\n",
      "Epoch 01719: reducing learning rate of group 0 to 8.4306e-06.\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1718:\n",
      "train loss: 4.7960534255256396e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1719:\n",
      "train loss: 4.367371887420795e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1720:\n",
      "train loss: 3.319280685891237e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1721:\n",
      "train loss: 2.917660210516361e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1722:\n",
      "train loss: 4.763224856515756e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1723:\n",
      "train loss: 4.356074132649414e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1724:\n",
      "train loss: 3.3299628957339756e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1725:\n",
      "train loss: 2.9283168707723963e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1726:\n",
      "train loss: 4.7520645482793706e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1727:\n",
      "train loss: 4.3449872559867435e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1728:\n",
      "train loss: 3.34045877875489e-05\n",
      "lr: 8.430581474923325e-06\n",
      "Epoch 1729:\n",
      "train loss: 2.938805690847298e-05\n",
      "Epoch 01731: reducing learning rate of group 0 to 8.0091e-06.\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1730:\n",
      "train loss: 4.7410590268470205e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1731:\n",
      "train loss: 4.334039824225199e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1732:\n",
      "train loss: 2.9666019816773202e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1733:\n",
      "train loss: 2.5850102191555756e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1734:\n",
      "train loss: 4.710420798060935e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1735:\n",
      "train loss: 4.323840606998111e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1736:\n",
      "train loss: 2.976230107310911e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1737:\n",
      "train loss: 2.5946161808430927e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1738:\n",
      "train loss: 4.700344622436326e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1739:\n",
      "train loss: 4.313831444222775e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1740:\n",
      "train loss: 2.9856904254533265e-05\n",
      "lr: 8.009052401177158e-06\n",
      "Epoch 1741:\n",
      "train loss: 2.6040710153667952e-05\n",
      "Epoch 01743: reducing learning rate of group 0 to 7.6086e-06.\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1742:\n",
      "train loss: 4.6904078882505966e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1743:\n",
      "train loss: 4.303947878565307e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1744:\n",
      "train loss: 2.6301032120722123e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1745:\n",
      "train loss: 2.2675463745158e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1746:\n",
      "train loss: 4.661796321869119e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1747:\n",
      "train loss: 4.294739456780567e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1748:\n",
      "train loss: 2.638781708674148e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1749:\n",
      "train loss: 2.2762057181529307e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1750:\n",
      "train loss: 4.652697972860738e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1751:\n",
      "train loss: 4.285702278725987e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1752:\n",
      "train loss: 2.647309106337388e-05\n",
      "lr: 7.6085997811183e-06\n",
      "Epoch 1753:\n",
      "train loss: 2.284728992796837e-05\n",
      "Epoch 01755: reducing learning rate of group 0 to 7.2282e-06.\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1754:\n",
      "train loss: 4.6437252510030195e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1755:\n",
      "train loss: 4.2767782477093894e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1756:\n",
      "train loss: 2.3091224503734318e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1757:\n",
      "train loss: 1.9646555299605135e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1758:\n",
      "train loss: 4.616990160949643e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1759:\n",
      "train loss: 4.268463465848033e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1760:\n",
      "train loss: 2.3169454279466815e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1761:\n",
      "train loss: 1.9724620328807794e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1762:\n",
      "train loss: 4.608773738009055e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1763:\n",
      "train loss: 4.26030284575065e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1764:\n",
      "train loss: 2.324632535983301e-05\n",
      "lr: 7.228169792062385e-06\n",
      "Epoch 1765:\n",
      "train loss: 1.9801462747804575e-05\n",
      "Epoch 01767: reducing learning rate of group 0 to 6.8668e-06.\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1766:\n",
      "train loss: 4.6006701618493826e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1767:\n",
      "train loss: 4.2522437343836334e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1768:\n",
      "train loss: 2.0030170133955277e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1769:\n",
      "train loss: 1.6757418063089795e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1770:\n",
      "train loss: 4.57567220934222e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1771:\n",
      "train loss: 4.2447330539334654e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1772:\n",
      "train loss: 2.010071693630319e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1773:\n",
      "train loss: 1.682783118888685e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1774:\n",
      "train loss: 4.568246875178167e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1775:\n",
      "train loss: 4.237357445512089e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1776:\n",
      "train loss: 2.0170086871790877e-05\n",
      "lr: 6.866761302459265e-06\n",
      "Epoch 1777:\n",
      "train loss: 1.6897201924327188e-05\n",
      "Epoch 01779: reducing learning rate of group 0 to 6.5234e-06.\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1778:\n",
      "train loss: 4.5609154316890175e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1779:\n",
      "train loss: 4.230063625864783e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1780:\n",
      "train loss: 1.7111895383480825e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1781:\n",
      "train loss: 1.4002613117747672e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1782:\n",
      "train loss: 4.5374893307311684e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1783:\n",
      "train loss: 4.223226895408425e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1784:\n",
      "train loss: 1.7176175555944862e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1785:\n",
      "train loss: 1.4067064381539106e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1786:\n",
      "train loss: 4.53062669684435e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1787:\n",
      "train loss: 4.216356318597548e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1788:\n",
      "train loss: 1.724140747987546e-05\n",
      "lr: 6.523423237336301e-06\n",
      "Epoch 1789:\n",
      "train loss: 1.4133501160687961e-05\n",
      "Epoch 01791: reducing learning rate of group 0 to 6.1973e-06.\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1790:\n",
      "train loss: 4.5233665483242465e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1791:\n",
      "train loss: 4.208904343048873e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1792:\n",
      "train loss: 1.4344642305298912e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1793:\n",
      "train loss: 1.1400203711931505e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1794:\n",
      "train loss: 4.497459021320705e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1795:\n",
      "train loss: 4.197033769568665e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1796:\n",
      "train loss: 1.4480094016999034e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1797:\n",
      "train loss: 1.1598787480696808e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1798:\n",
      "train loss: 4.4609744887260366e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1799:\n",
      "train loss: 4.146924717370458e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1800:\n",
      "train loss: 1.5138850312025018e-05\n",
      "lr: 6.197252075469486e-06\n",
      "Epoch 1801:\n",
      "train loss: 1.2692704463362354e-05\n",
      "Epoch 01803: reducing learning rate of group 0 to 5.8874e-06.\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1802:\n",
      "train loss: 4.248857023758829e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1803:\n",
      "train loss: 3.845587051442869e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1804:\n",
      "train loss: 1.627464117570162e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1805:\n",
      "train loss: 1.5608694426437855e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1806:\n",
      "train loss: 3.408022631255413e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1807:\n",
      "train loss: 2.7532157156596e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1808:\n",
      "train loss: 2.940545340487905e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1809:\n",
      "train loss: 2.92745979638999e-05\n",
      "lr: 5.887389471696011e-06\n",
      "Epoch 1810:\n",
      "train loss: 2.2443788421882578e-05\n",
      "Epoch 01812: reducing learning rate of group 0 to 5.5930e-06.\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1811:\n",
      "train loss: 1.8209531599387717e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1812:\n",
      "train loss: 3.5701223209607904e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1813:\n",
      "train loss: 3.271288290962183e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1814:\n",
      "train loss: 1.8837535578240465e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1815:\n",
      "train loss: 1.7118730115759993e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1816:\n",
      "train loss: 3.253433354814277e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1817:\n",
      "train loss: 2.854623230982769e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1818:\n",
      "train loss: 2.354793946060358e-05\n",
      "lr: 5.593019998111211e-06\n",
      "Epoch 1819:\n",
      "train loss: 2.198386151369815e-05\n",
      "Epoch 01821: reducing learning rate of group 0 to 5.3134e-06.\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1820:\n",
      "train loss: 2.7862728148462507e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1821:\n",
      "train loss: 2.4290538261352773e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1822:\n",
      "train loss: 2.477603047173682e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1823:\n",
      "train loss: 2.2906849729328815e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1824:\n",
      "train loss: 2.4809720100944727e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1825:\n",
      "train loss: 2.1689653419484402e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1826:\n",
      "train loss: 2.7160940049091033e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1827:\n",
      "train loss: 2.5019300431615348e-05\n",
      "lr: 5.31336899820565e-06\n",
      "Epoch 1828:\n",
      "train loss: 2.3003228702393547e-05\n",
      "Epoch 01830: reducing learning rate of group 0 to 5.0477e-06.\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1829:\n",
      "train loss: 2.0144492969572395e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1830:\n",
      "train loss: 2.8475101675430964e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1831:\n",
      "train loss: 2.62528745155404e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1832:\n",
      "train loss: 1.9515742738219793e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1833:\n",
      "train loss: 1.6964199010218758e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1834:\n",
      "train loss: 2.9074295950430994e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1835:\n",
      "train loss: 2.6758857774427636e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1836:\n",
      "train loss: 1.9070291137340153e-05\n",
      "lr: 5.047700548295367e-06\n",
      "Epoch 1837:\n",
      "train loss: 1.657363505417435e-05\n",
      "Epoch 01839: reducing learning rate of group 0 to 4.7953e-06.\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1838:\n",
      "train loss: 2.9409540282599603e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1839:\n",
      "train loss: 2.7043001731289982e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1840:\n",
      "train loss: 1.655228765578136e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1841:\n",
      "train loss: 1.4243463466980108e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1842:\n",
      "train loss: 2.9355111140468156e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1843:\n",
      "train loss: 2.701452114732227e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1844:\n",
      "train loss: 1.6672993051999612e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1845:\n",
      "train loss: 1.445149484199779e-05\n",
      "lr: 4.795315520880598e-06\n",
      "Epoch 1846:\n",
      "train loss: 2.906823383891688e-05\n",
      "Epoch 01848: reducing learning rate of group 0 to 4.5555e-06.\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1847:\n",
      "train loss: 2.6654485859087282e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1848:\n",
      "train loss: 1.7092677971898787e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1849:\n",
      "train loss: 1.5024361039178999e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1850:\n",
      "train loss: 2.630345032912388e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1851:\n",
      "train loss: 2.400761802446743e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1852:\n",
      "train loss: 1.754117295297023e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1853:\n",
      "train loss: 1.5459831985367108e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1854:\n",
      "train loss: 2.588758314538572e-05\n",
      "lr: 4.5555497448365686e-06\n",
      "Epoch 1855:\n",
      "train loss: 2.362262805676706e-05\n",
      "Epoch 01857: reducing learning rate of group 0 to 4.3278e-06.\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1856:\n",
      "train loss: 1.7890928834992264e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1857:\n",
      "train loss: 1.577962400904914e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1858:\n",
      "train loss: 2.35239475530687e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1859:\n",
      "train loss: 2.1395743684518916e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1860:\n",
      "train loss: 1.802061480601551e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1861:\n",
      "train loss: 1.5996254892862738e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1862:\n",
      "train loss: 2.3322348827578782e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1863:\n",
      "train loss: 2.1206934764168115e-05\n",
      "lr: 4.32777225759474e-06\n",
      "Epoch 1864:\n",
      "train loss: 1.8197160312273993e-05\n",
      "Epoch 01866: reducing learning rate of group 0 to 4.1114e-06.\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1865:\n",
      "train loss: 1.6163145026586935e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1866:\n",
      "train loss: 2.3163206158971244e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1867:\n",
      "train loss: 2.1162961924696918e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1868:\n",
      "train loss: 1.6259442947007454e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1869:\n",
      "train loss: 1.4319520450284314e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1870:\n",
      "train loss: 2.304559957252953e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1871:\n",
      "train loss: 2.1053551227682823e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1872:\n",
      "train loss: 1.635856327994702e-05\n",
      "lr: 4.111383644715003e-06\n",
      "Epoch 1873:\n",
      "train loss: 1.4411437168277464e-05\n",
      "Epoch 01875: reducing learning rate of group 0 to 3.9058e-06.\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1874:\n",
      "train loss: 2.2958046256933823e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1875:\n",
      "train loss: 2.097190206555141e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1876:\n",
      "train loss: 1.4563138072998987e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1877:\n",
      "train loss: 1.2709427594113542e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1878:\n",
      "train loss: 2.2792791933583118e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1879:\n",
      "train loss: 2.090833011811415e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1880:\n",
      "train loss: 1.4623000200050385e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1881:\n",
      "train loss: 1.2767805787022893e-05\n",
      "lr: 3.905814462479253e-06\n",
      "Epoch 1882:\n",
      "train loss: 2.2734059817691747e-05\n",
      "Epoch 01884: reducing learning rate of group 0 to 3.7105e-06.\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1883:\n",
      "train loss: 2.0851187923916063e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1884:\n",
      "train loss: 1.4676590759375573e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1885:\n",
      "train loss: 1.2912767661988628e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1886:\n",
      "train loss: 2.0813898516166603e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1887:\n",
      "train loss: 1.9027354393287312e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1888:\n",
      "train loss: 1.4719871750332202e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1889:\n",
      "train loss: 1.295411313073027e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1890:\n",
      "train loss: 2.0772593697554084e-05\n",
      "lr: 3.7105237393552902e-06\n",
      "Epoch 1891:\n",
      "train loss: 1.8988099217715293e-05\n",
      "Epoch 01893: reducing learning rate of group 0 to 3.5250e-06.\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1892:\n",
      "train loss: 1.475543280203434e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1893:\n",
      "train loss: 1.298813870094788e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1894:\n",
      "train loss: 1.905178723358728e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1895:\n",
      "train loss: 1.7357817933311976e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1896:\n",
      "train loss: 1.469568009065051e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1897:\n",
      "train loss: 1.301584665339085e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1898:\n",
      "train loss: 1.9023227177026245e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1899:\n",
      "train loss: 1.7330127305309867e-05\n",
      "lr: 3.5249975523875253e-06\n",
      "Epoch 1900:\n",
      "train loss: 1.472090963380629e-05\n",
      "Epoch 01902: reducing learning rate of group 0 to 3.3487e-06.\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1901:\n",
      "train loss: 1.3040546202487562e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1902:\n",
      "train loss: 1.8997353913436848e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1903:\n",
      "train loss: 1.7389555551563946e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1904:\n",
      "train loss: 1.3056735561409608e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1905:\n",
      "train loss: 1.1459963941111568e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1906:\n",
      "train loss: 1.8974901871225878e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1907:\n",
      "train loss: 1.7367690890395646e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1908:\n",
      "train loss: 1.3076468631842304e-05\n",
      "lr: 3.348747674768149e-06\n",
      "Epoch 1909:\n",
      "train loss: 1.1479364259165148e-05\n",
      "Epoch 01911: reducing learning rate of group 0 to 3.1813e-06.\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1910:\n",
      "train loss: 1.8954211597115794e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1911:\n",
      "train loss: 1.734741165820519e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1912:\n",
      "train loss: 1.1572729300428562e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1913:\n",
      "train loss: 1.0055295859737042e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1914:\n",
      "train loss: 1.8855305106502002e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1915:\n",
      "train loss: 1.7329139078709584e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1916:\n",
      "train loss: 1.1589258594214112e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1917:\n",
      "train loss: 1.0071744172219843e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1918:\n",
      "train loss: 1.8837452068769296e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1919:\n",
      "train loss: 1.731150489714142e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1920:\n",
      "train loss: 1.1605210910737242e-05\n",
      "lr: 3.181310291029741e-06\n",
      "Epoch 1921:\n",
      "train loss: 1.008765580236245e-05\n",
      "Epoch 01923: reducing learning rate of group 0 to 3.0222e-06.\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1922:\n",
      "train loss: 1.8820119804319768e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1923:\n",
      "train loss: 1.7294379145658633e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1924:\n",
      "train loss: 1.0174938068460161e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1925:\n",
      "train loss: 8.733194586933214e-06\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1926:\n",
      "train loss: 1.8727916152366066e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1927:\n",
      "train loss: 1.7278700099258713e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1928:\n",
      "train loss: 1.0189011335822352e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1929:\n",
      "train loss: 8.747204838939096e-06\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1930:\n",
      "train loss: 1.8712585613890662e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1931:\n",
      "train loss: 1.726356709243348e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1932:\n",
      "train loss: 1.0202587218226835e-05\n",
      "lr: 3.022244776478254e-06\n",
      "Epoch 1933:\n",
      "train loss: 8.760756992354926e-06\n",
      "Epoch 01935: reducing learning rate of group 0 to 2.8711e-06.\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1934:\n",
      "train loss: 1.8697680703722522e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1935:\n",
      "train loss: 1.7248818805896022e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1936:\n",
      "train loss: 8.842609909043677e-06\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1937:\n",
      "train loss: 7.4728491846122e-06\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1938:\n",
      "train loss: 1.8611430053970637e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1939:\n",
      "train loss: 1.723518319292609e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1940:\n",
      "train loss: 8.854798676210137e-06\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1941:\n",
      "train loss: 7.4850304953439205e-06\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1942:\n",
      "train loss: 1.8597963495006413e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1943:\n",
      "train loss: 1.722184798573074e-05\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1944:\n",
      "train loss: 8.866731975104396e-06\n",
      "lr: 2.871132537654341e-06\n",
      "Epoch 1945:\n",
      "train loss: 7.496989036398886e-06\n",
      "Epoch 01947: reducing learning rate of group 0 to 2.7276e-06.\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1946:\n",
      "train loss: 1.8584697540708978e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1947:\n",
      "train loss: 1.7208685011553535e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1948:\n",
      "train loss: 7.574182384706009e-06\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1949:\n",
      "train loss: 6.2729509535778994e-06\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1950:\n",
      "train loss: 1.850343976708039e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1951:\n",
      "train loss: 1.719633535728073e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1952:\n",
      "train loss: 7.585235572056798e-06\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1953:\n",
      "train loss: 6.284067592456078e-06\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1954:\n",
      "train loss: 1.8491012151908294e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1955:\n",
      "train loss: 1.718392674684477e-05\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1956:\n",
      "train loss: 7.596447575562502e-06\n",
      "lr: 2.7275759107716237e-06\n",
      "Epoch 1957:\n",
      "train loss: 6.295484264588817e-06\n",
      "Epoch 01959: reducing learning rate of group 0 to 2.5912e-06.\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1958:\n",
      "train loss: 1.8478044896304537e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1959:\n",
      "train loss: 1.71707493455202e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1960:\n",
      "train loss: 6.369749607303949e-06\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1961:\n",
      "train loss: 5.134751867730761e-06\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1962:\n",
      "train loss: 1.8396883176267868e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1963:\n",
      "train loss: 1.7153276217866274e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1964:\n",
      "train loss: 6.388005201921457e-06\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1965:\n",
      "train loss: 5.158237362202276e-06\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1966:\n",
      "train loss: 1.8359914860829588e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1967:\n",
      "train loss: 1.710590114353718e-05\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1968:\n",
      "train loss: 6.446125748631812e-06\n",
      "lr: 2.5911971152330426e-06\n",
      "Epoch 1969:\n",
      "train loss: 5.247792887994952e-06\n",
      "Epoch 01971: reducing learning rate of group 0 to 2.4616e-06.\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1970:\n",
      "train loss: 1.8195311009946845e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1971:\n",
      "train loss: 1.68780788053311e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1972:\n",
      "train loss: 5.568501377613014e-06\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1973:\n",
      "train loss: 4.689056630776719e-06\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1974:\n",
      "train loss: 1.6926557168773968e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1975:\n",
      "train loss: 1.5142713360245793e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1976:\n",
      "train loss: 7.824342563824997e-06\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1977:\n",
      "train loss: 7.562723373867345e-06\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1978:\n",
      "train loss: 1.3343753723171845e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1979:\n",
      "train loss: 1.0739033765186451e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1980:\n",
      "train loss: 1.2854958446226699e-05\n",
      "lr: 2.4616372594713904e-06\n",
      "Epoch 1981:\n",
      "train loss: 1.262322872715951e-05\n",
      "Epoch 01983: reducing learning rate of group 0 to 2.3386e-06.\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1982:\n",
      "train loss: 9.132356846825508e-06\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1983:\n",
      "train loss: 7.605811103053508e-06\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1984:\n",
      "train loss: 1.350769943799819e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1985:\n",
      "train loss: 1.201269535771413e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1986:\n",
      "train loss: 9.698525756219894e-06\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1987:\n",
      "train loss: 8.998379562923885e-06\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1988:\n",
      "train loss: 1.1829605460656894e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1989:\n",
      "train loss: 1.0282777734245175e-05\n",
      "lr: 2.3385553964978207e-06\n",
      "Epoch 1990:\n",
      "train loss: 1.1333026564575035e-05\n",
      "Epoch 01992: reducing learning rate of group 0 to 2.2216e-06.\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1991:\n",
      "train loss: 1.0570320469338392e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1992:\n",
      "train loss: 1.0316041872828595e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1993:\n",
      "train loss: 8.98790526186472e-06\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1994:\n",
      "train loss: 1.1391836657268331e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1995:\n",
      "train loss: 1.053660646998516e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1996:\n",
      "train loss: 9.441302377644038e-06\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1997:\n",
      "train loss: 8.213856529562418e-06\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1998:\n",
      "train loss: 1.2072479097838815e-05\n",
      "lr: 2.2216276266729297e-06\n",
      "Epoch 1999:\n",
      "train loss: 1.1082016625764091e-05\n",
      "Epoch 02001: reducing learning rate of group 0 to 2.1105e-06.\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2000:\n",
      "train loss: 9.056328197287961e-06\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2001:\n",
      "train loss: 7.9630486051358e-06\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2002:\n",
      "train loss: 1.1207076113306399e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2003:\n",
      "train loss: 1.0167470043954503e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2004:\n",
      "train loss: 9.042424556791821e-06\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2005:\n",
      "train loss: 8.06984603961712e-06\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2006:\n",
      "train loss: 1.1057463045864999e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2007:\n",
      "train loss: 1.0008645518366426e-05\n",
      "lr: 2.110546245339283e-06\n",
      "Epoch 2008:\n",
      "train loss: 9.189749260656782e-06\n",
      "Epoch 02010: reducing learning rate of group 0 to 2.0050e-06.\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2009:\n",
      "train loss: 8.216777498556875e-06\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2010:\n",
      "train loss: 1.0909433931239282e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2011:\n",
      "train loss: 9.92365908278937e-06\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2012:\n",
      "train loss: 8.303552306825232e-06\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2013:\n",
      "train loss: 7.368495481345588e-06\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2014:\n",
      "train loss: 1.0808944206792892e-05\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2015:\n",
      "train loss: 9.822611020058612e-06\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2016:\n",
      "train loss: 8.411279126216808e-06\n",
      "lr: 2.005018933072319e-06\n",
      "Epoch 2017:\n",
      "train loss: 7.478766251219175e-06\n",
      "Epoch 02019: reducing learning rate of group 0 to 1.9048e-06.\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2018:\n",
      "train loss: 1.0698525146647063e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2019:\n",
      "train loss: 9.708725387533141e-06\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2020:\n",
      "train loss: 7.61711061122165e-06\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2021:\n",
      "train loss: 6.7360764616272345e-06\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2022:\n",
      "train loss: 1.0528441460195615e-05\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2023:\n",
      "train loss: 9.5884326888242e-06\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2024:\n",
      "train loss: 7.732815403193816e-06\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2025:\n",
      "train loss: 6.84845993509468e-06\n",
      "lr: 1.904767986418703e-06\n",
      "Epoch 2026:\n",
      "train loss: 1.0420421675016163e-05\n",
      "Epoch 02028: reducing learning rate of group 0 to 1.8095e-06.\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2027:\n",
      "train loss: 9.48883813668515e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2028:\n",
      "train loss: 7.822464730506296e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2029:\n",
      "train loss: 6.974620668688763e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2030:\n",
      "train loss: 9.436995729792565e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2031:\n",
      "train loss: 8.558042163599145e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2032:\n",
      "train loss: 7.882219886189093e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2033:\n",
      "train loss: 7.029459697414847e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2034:\n",
      "train loss: 9.386187188617039e-06\n",
      "lr: 1.8095295870977678e-06\n",
      "Epoch 2035:\n",
      "train loss: 8.510544023567251e-06\n",
      "Epoch 02037: reducing learning rate of group 0 to 1.7191e-06.\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2036:\n",
      "train loss: 7.926572125869472e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2037:\n",
      "train loss: 7.071353595806183e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2038:\n",
      "train loss: 8.525391334260941e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2039:\n",
      "train loss: 7.696133560801254e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2040:\n",
      "train loss: 7.915779875772223e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2041:\n",
      "train loss: 7.101167589415864e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2042:\n",
      "train loss: 8.496832280300291e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2043:\n",
      "train loss: 7.669942357931606e-06\n",
      "lr: 1.7190531077428793e-06\n",
      "Epoch 2044:\n",
      "train loss: 7.938888591475213e-06\n",
      "Epoch 02046: reducing learning rate of group 0 to 1.6331e-06.\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2045:\n",
      "train loss: 7.122439482468785e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2046:\n",
      "train loss: 8.476454681230163e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2047:\n",
      "train loss: 7.692376374966744e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2048:\n",
      "train loss: 7.134122719458148e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2049:\n",
      "train loss: 6.357321828016733e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2050:\n",
      "train loss: 8.462089254293624e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2051:\n",
      "train loss: 7.678864273106734e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2052:\n",
      "train loss: 7.146355580005686e-06\n",
      "lr: 1.6331004523557352e-06\n",
      "Epoch 2053:\n",
      "train loss: 6.368953931770338e-06\n",
      "Epoch 02055: reducing learning rate of group 0 to 1.5514e-06.\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2054:\n",
      "train loss: 8.450430834825107e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2055:\n",
      "train loss: 7.667796920211956e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2056:\n",
      "train loss: 6.415054444058019e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2057:\n",
      "train loss: 5.676161780927298e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2058:\n",
      "train loss: 8.401992870351913e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2059:\n",
      "train loss: 7.65893841794385e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2060:\n",
      "train loss: 6.422839955883255e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2061:\n",
      "train loss: 5.683601796382762e-06\n",
      "lr: 1.5514454297379485e-06\n",
      "Epoch 2062:\n",
      "train loss: 8.394278654173462e-06\n",
      "Epoch 02064: reducing learning rate of group 0 to 1.4739e-06.\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2063:\n",
      "train loss: 7.651630068339072e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2064:\n",
      "train loss: 6.429171403621864e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2065:\n",
      "train loss: 5.726625376481228e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2066:\n",
      "train loss: 7.647030293474792e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2067:\n",
      "train loss: 6.94177538524463e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2068:\n",
      "train loss: 6.43419345104417e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2069:\n",
      "train loss: 5.7314859856645935e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2070:\n",
      "train loss: 7.641760428926823e-06\n",
      "lr: 1.473873158251051e-06\n",
      "Epoch 2071:\n",
      "train loss: 6.9366832597668684e-06\n",
      "Epoch 02073: reducing learning rate of group 0 to 1.4002e-06.\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2072:\n",
      "train loss: 6.438571352221903e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2073:\n",
      "train loss: 5.735779408819378e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2074:\n",
      "train loss: 6.96835029233378e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2075:\n",
      "train loss: 6.298670615645227e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2076:\n",
      "train loss: 6.407143257201736e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2077:\n",
      "train loss: 5.739418077305866e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2078:\n",
      "train loss: 6.964248732339205e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2079:\n",
      "train loss: 6.294692277133977e-06\n",
      "lr: 1.4001795003384984e-06\n",
      "Epoch 2080:\n",
      "train loss: 6.410478853526995e-06\n",
      "Epoch 02082: reducing learning rate of group 0 to 1.3302e-06.\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2081:\n",
      "train loss: 5.7427061809950495e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2082:\n",
      "train loss: 6.96047718129716e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2083:\n",
      "train loss: 6.3244844835884455e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2084:\n",
      "train loss: 5.744859506601421e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2085:\n",
      "train loss: 5.110447934268973e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2086:\n",
      "train loss: 6.957115656734272e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2087:\n",
      "train loss: 6.321197151496899e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2088:\n",
      "train loss: 5.747588148112014e-06\n",
      "lr: 1.3301705253215734e-06\n",
      "Epoch 2089:\n",
      "train loss: 5.113166512454195e-06\n",
      "Epoch 02091: reducing learning rate of group 0 to 1.2637e-06.\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2090:\n",
      "train loss: 6.953914645996673e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2091:\n",
      "train loss: 6.318054437190242e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2092:\n",
      "train loss: 5.1467780287945185e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2093:\n",
      "train loss: 4.544073529577227e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2094:\n",
      "train loss: 6.919200296149011e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2095:\n",
      "train loss: 6.315189888307882e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2096:\n",
      "train loss: 5.149126118735882e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2097:\n",
      "train loss: 4.546421003451045e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2098:\n",
      "train loss: 6.91639121269326e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2099:\n",
      "train loss: 6.312429029031782e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2100:\n",
      "train loss: 5.1513820827189035e-06\n",
      "lr: 1.2636619990554946e-06\n",
      "Epoch 2101:\n",
      "train loss: 4.548683741013366e-06\n",
      "Epoch 02103: reducing learning rate of group 0 to 1.2005e-06.\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2102:\n",
      "train loss: 6.913662374185666e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2103:\n",
      "train loss: 6.30974005804984e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2104:\n",
      "train loss: 4.580414477743621e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2105:\n",
      "train loss: 4.007858737849972e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2106:\n",
      "train loss: 6.880935992977103e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2107:\n",
      "train loss: 6.3072504934287795e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2108:\n",
      "train loss: 4.582435109533432e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2109:\n",
      "train loss: 4.009889996347646e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2110:\n",
      "train loss: 6.878463903503619e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2111:\n",
      "train loss: 6.3048128691999e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2112:\n",
      "train loss: 4.584411645431694e-06\n",
      "lr: 1.2004788991027199e-06\n",
      "Epoch 2113:\n",
      "train loss: 4.011881426033998e-06\n",
      "Epoch 02115: reducing learning rate of group 0 to 1.1405e-06.\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2114:\n",
      "train loss: 6.876030831351467e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2115:\n",
      "train loss: 6.302411258492165e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2116:\n",
      "train loss: 4.041921571416582e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2117:\n",
      "train loss: 3.4980289178903443e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2118:\n",
      "train loss: 6.845079966202189e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2119:\n",
      "train loss: 6.300177523261071e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2120:\n",
      "train loss: 4.043719077772819e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2121:\n",
      "train loss: 3.4998371035311802e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2122:\n",
      "train loss: 6.842860243388027e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2123:\n",
      "train loss: 6.2979898482826635e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2124:\n",
      "train loss: 4.0454767827574545e-06\n",
      "lr: 1.1404549541475838e-06\n",
      "Epoch 2125:\n",
      "train loss: 3.5016087537993136e-06\n",
      "Epoch 02127: reducing learning rate of group 0 to 1.0834e-06.\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2126:\n",
      "train loss: 6.840676397516731e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2127:\n",
      "train loss: 6.295834893060246e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2128:\n",
      "train loss: 3.5300573952307322e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2129:\n",
      "train loss: 3.0133936313550467e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2130:\n",
      "train loss: 6.811396146735748e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2131:\n",
      "train loss: 6.2938295613737564e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2132:\n",
      "train loss: 3.53165683733854e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2133:\n",
      "train loss: 3.015004098960986e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2134:\n",
      "train loss: 6.809400547062332e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2135:\n",
      "train loss: 6.291862519933833e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2136:\n",
      "train loss: 3.533224159383552e-06\n",
      "lr: 1.0834322064402045e-06\n",
      "Epoch 2137:\n",
      "train loss: 3.016585753804106e-06\n",
      "Epoch 02139: reducing learning rate of group 0 to 1.0293e-06.\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2138:\n",
      "train loss: 6.807432977164275e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2139:\n",
      "train loss: 6.289920493555678e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2140:\n",
      "train loss: 3.0435390489657327e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2141:\n",
      "train loss: 2.5527443432357773e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2142:\n",
      "train loss: 6.779715650367106e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2143:\n",
      "train loss: 6.2881073458682425e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2144:\n",
      "train loss: 3.044975636802167e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2145:\n",
      "train loss: 2.554193822858175e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2146:\n",
      "train loss: 6.777903003220379e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2147:\n",
      "train loss: 6.286317721202061e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2148:\n",
      "train loss: 3.0463961118002214e-06\n",
      "lr: 1.0292605961181942e-06\n",
      "Epoch 2149:\n",
      "train loss: 2.555633166111473e-06\n",
      "Epoch 02151: reducing learning rate of group 0 to 9.7780e-07.\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2150:\n",
      "train loss: 6.776094406618091e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2151:\n",
      "train loss: 6.284524358736521e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2152:\n",
      "train loss: 2.581215108756238e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2153:\n",
      "train loss: 2.11502235493451e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2154:\n",
      "train loss: 6.74973995415103e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2155:\n",
      "train loss: 6.282736010652074e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2156:\n",
      "train loss: 2.5826934793392422e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2157:\n",
      "train loss: 2.11660149213455e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2158:\n",
      "train loss: 6.747632859503421e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2159:\n",
      "train loss: 6.280486407501698e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2160:\n",
      "train loss: 2.5847758860258466e-06\n",
      "lr: 9.777975663122844e-07\n",
      "Epoch 2161:\n",
      "train loss: 2.1190990490027156e-06\n",
      "Epoch 02163: reducing learning rate of group 0 to 9.2891e-07.\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2162:\n",
      "train loss: 6.743932381894123e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2163:\n",
      "train loss: 6.276054028691973e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2164:\n",
      "train loss: 2.1464639518315563e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2165:\n",
      "train loss: 1.7070456794662922e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2166:\n",
      "train loss: 6.704905283224565e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2167:\n",
      "train loss: 6.254327757900984e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2168:\n",
      "train loss: 2.1746171821538997e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2169:\n",
      "train loss: 1.7578117857539618e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2170:\n",
      "train loss: 6.596429648402149e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2171:\n",
      "train loss: 6.099583347039841e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2172:\n",
      "train loss: 2.3786191571065846e-06\n",
      "lr: 9.2890768799667e-07\n",
      "Epoch 2173:\n",
      "train loss: 2.087089864411482e-06\n",
      "Epoch 02175: reducing learning rate of group 0 to 8.8246e-07.\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2174:\n",
      "train loss: 5.997201506059308e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2175:\n",
      "train loss: 5.267451580595887e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2176:\n",
      "train loss: 2.999654495511646e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2177:\n",
      "train loss: 2.936244231247599e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2178:\n",
      "train loss: 4.555242640481628e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2179:\n",
      "train loss: 3.633659100138235e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2180:\n",
      "train loss: 4.7755452427639355e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2181:\n",
      "train loss: 4.658392414721716e-06\n",
      "lr: 8.824623035968365e-07\n",
      "Epoch 2182:\n",
      "train loss: 3.1624991605007122e-06\n",
      "Epoch 02184: reducing learning rate of group 0 to 8.3834e-07.\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2183:\n",
      "train loss: 2.691983154385857e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2184:\n",
      "train loss: 5.127070696260915e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2185:\n",
      "train loss: 4.4725498463435436e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2186:\n",
      "train loss: 3.4178288702784693e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2187:\n",
      "train loss: 3.283342975756963e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2188:\n",
      "train loss: 4.0607801704274975e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2189:\n",
      "train loss: 3.3922661431757583e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2190:\n",
      "train loss: 4.445888547524482e-06\n",
      "lr: 8.383391884169947e-07\n",
      "Epoch 2191:\n",
      "train loss: 4.243581503080547e-06\n",
      "Epoch 02193: reducing learning rate of group 0 to 7.9642e-07.\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2192:\n",
      "train loss: 3.1799147404312384e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2193:\n",
      "train loss: 2.6340049458571077e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2194:\n",
      "train loss: 4.7000805080321205e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2195:\n",
      "train loss: 4.429632964974228e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2196:\n",
      "train loss: 2.6841563643455867e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2197:\n",
      "train loss: 2.2191531543968425e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2198:\n",
      "train loss: 5.056034055076684e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2199:\n",
      "train loss: 4.730437502247219e-06\n",
      "lr: 7.964222289961449e-07\n",
      "Epoch 2200:\n",
      "train loss: 2.4473153871082375e-06\n",
      "Epoch 02202: reducing learning rate of group 0 to 7.5660e-07.\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2201:\n",
      "train loss: 2.07225386367547e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2202:\n",
      "train loss: 5.0539181558548815e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2203:\n",
      "train loss: 4.604794613923471e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2204:\n",
      "train loss: 2.3627165841656477e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2205:\n",
      "train loss: 2.1866312532748945e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2206:\n",
      "train loss: 4.358175627104137e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2207:\n",
      "train loss: 3.682368322781693e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2208:\n",
      "train loss: 3.473010344429183e-06\n",
      "lr: 7.566011175463376e-07\n",
      "Epoch 2209:\n",
      "train loss: 3.3623870930054044e-06\n",
      "Epoch 02211: reducing learning rate of group 0 to 7.1877e-07.\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2210:\n",
      "train loss: 3.28614020315641e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2211:\n",
      "train loss: 2.7225676361975725e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2212:\n",
      "train loss: 3.944156133217308e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2213:\n",
      "train loss: 3.702399745891396e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2214:\n",
      "train loss: 2.742329203110932e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2215:\n",
      "train loss: 2.347207892913906e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2216:\n",
      "train loss: 4.196219882678577e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2217:\n",
      "train loss: 3.8687699603631234e-06\n",
      "lr: 7.187710616690208e-07\n",
      "Epoch 2218:\n",
      "train loss: 2.632702519548541e-06\n",
      "Epoch 02220: reducing learning rate of group 0 to 6.8283e-07.\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2219:\n",
      "train loss: 2.28289097895832e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2220:\n",
      "train loss: 4.2342731052359686e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2221:\n",
      "train loss: 3.91335346321281e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2222:\n",
      "train loss: 2.2689885629910417e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2223:\n",
      "train loss: 1.941767001140387e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2224:\n",
      "train loss: 4.2384179447341936e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2225:\n",
      "train loss: 3.905457377970828e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2226:\n",
      "train loss: 2.293425238456457e-06\n",
      "lr: 6.828325085855697e-07\n",
      "Epoch 2227:\n",
      "train loss: 1.9881821051103134e-06\n",
      "Epoch 02229: reducing learning rate of group 0 to 6.4869e-07.\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2228:\n",
      "train loss: 4.157803955775553e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2229:\n",
      "train loss: 3.7873127287962416e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2230:\n",
      "train loss: 2.1406117137671623e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2231:\n",
      "train loss: 1.8955015782000822e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2232:\n",
      "train loss: 3.888360695448504e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2233:\n",
      "train loss: 3.4807777710146136e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2234:\n",
      "train loss: 2.497366889631238e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2235:\n",
      "train loss: 2.2825248741438135e-06\n",
      "lr: 6.486908831562911e-07\n",
      "Epoch 2236:\n",
      "train loss: 3.4982720619561234e-06\n",
      "Epoch 02238: reducing learning rate of group 0 to 6.1626e-07.\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2237:\n",
      "train loss: 3.087983965143497e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2238:\n",
      "train loss: 2.8872782340164226e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2239:\n",
      "train loss: 2.671665505094975e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2240:\n",
      "train loss: 2.84436294257589e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2241:\n",
      "train loss: 2.4862211166537557e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2242:\n",
      "train loss: 3.15567968526398e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2243:\n",
      "train loss: 2.912817451238945e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2244:\n",
      "train loss: 2.624822377061248e-06\n",
      "lr: 6.162563389984766e-07\n",
      "Epoch 2245:\n",
      "train loss: 2.290041565330435e-06\n",
      "Epoch 02247: reducing learning rate of group 0 to 5.8544e-07.\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2246:\n",
      "train loss: 3.3297200876935775e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2247:\n",
      "train loss: 3.068621998725088e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2248:\n",
      "train loss: 2.2083283683671893e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2249:\n",
      "train loss: 1.905175705371074e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2250:\n",
      "train loss: 3.4175735968596517e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2251:\n",
      "train loss: 3.152700265614946e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2252:\n",
      "train loss: 2.1418520520120667e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2253:\n",
      "train loss: 1.8555275461356022e-06\n",
      "lr: 5.854435220485527e-07\n",
      "Epoch 2254:\n",
      "train loss: 3.4499689370796743e-06\n",
      "Epoch 02256: reducing learning rate of group 0 to 5.5617e-07.\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2255:\n",
      "train loss: 3.1677528577107538e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2256:\n",
      "train loss: 2.143081596740896e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2257:\n",
      "train loss: 1.884204887310221e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2258:\n",
      "train loss: 3.1465537800459326e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2259:\n",
      "train loss: 2.870170907454145e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2260:\n",
      "train loss: 2.1813467331793455e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2261:\n",
      "train loss: 1.9270402591403753e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2262:\n",
      "train loss: 3.1017400193596873e-06\n",
      "lr: 5.561713459461251e-07\n",
      "Epoch 2263:\n",
      "train loss: 2.82552104977246e-06\n",
      "Epoch 02265: reducing learning rate of group 0 to 5.2836e-07.\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2264:\n",
      "train loss: 2.2243552509521895e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2265:\n",
      "train loss: 1.9684868673700994e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2266:\n",
      "train loss: 2.8109032266533717e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2267:\n",
      "train loss: 2.5515720464488418e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2268:\n",
      "train loss: 2.242426522206431e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2269:\n",
      "train loss: 1.9964510776429456e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2270:\n",
      "train loss: 2.7853927169873447e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2271:\n",
      "train loss: 2.5284364451557734e-06\n",
      "lr: 5.283627786488188e-07\n",
      "Epoch 2272:\n",
      "train loss: 2.2634247360803162e-06\n",
      "Epoch 02274: reducing learning rate of group 0 to 5.0194e-07.\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2273:\n",
      "train loss: 2.0155322590643604e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2274:\n",
      "train loss: 2.767906104623532e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2275:\n",
      "train loss: 2.5249854757844392e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2276:\n",
      "train loss: 2.026334651335882e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2277:\n",
      "train loss: 1.790100512936889e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2278:\n",
      "train loss: 2.7546576796105036e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2279:\n",
      "train loss: 2.512259944905763e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2280:\n",
      "train loss: 2.0383858593531576e-06\n",
      "lr: 5.019446397163779e-07\n",
      "Epoch 2281:\n",
      "train loss: 1.801661603092501e-06\n",
      "Epoch 02283: reducing learning rate of group 0 to 4.7685e-07.\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2282:\n",
      "train loss: 2.7435443049752456e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2283:\n",
      "train loss: 2.5018602425389943e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2284:\n",
      "train loss: 1.8203932949034513e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2285:\n",
      "train loss: 1.5948934242985391e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2286:\n",
      "train loss: 2.7235161286935293e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2287:\n",
      "train loss: 2.4945741902348966e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2288:\n",
      "train loss: 1.8268948635770818e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2289:\n",
      "train loss: 1.600814546763484e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2290:\n",
      "train loss: 2.717987995422193e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2291:\n",
      "train loss: 2.489553461127441e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2292:\n",
      "train loss: 1.831332744750727e-06\n",
      "lr: 4.76847407730559e-07\n",
      "Epoch 2293:\n",
      "train loss: 1.6048687912155833e-06\n",
      "Epoch 02295: reducing learning rate of group 0 to 4.5301e-07.\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2294:\n",
      "train loss: 2.7141158484678475e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2295:\n",
      "train loss: 2.485954425600427e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2296:\n",
      "train loss: 1.618551645220297e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2297:\n",
      "train loss: 1.4032622014511632e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2298:\n",
      "train loss: 2.699733400061676e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2299:\n",
      "train loss: 2.4830545989375374e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2300:\n",
      "train loss: 1.6212614789121184e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2301:\n",
      "train loss: 1.4059318293547456e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2302:\n",
      "train loss: 2.696977156140921e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2303:\n",
      "train loss: 2.480351460425379e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2304:\n",
      "train loss: 1.623779649906846e-06\n",
      "lr: 4.53005037344031e-07\n",
      "Epoch 2305:\n",
      "train loss: 1.4084046714698617e-06\n",
      "Epoch 02307: reducing learning rate of group 0 to 4.3035e-07.\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2306:\n",
      "train loss: 2.6944385710435695e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2307:\n",
      "train loss: 2.4778952309173654e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2308:\n",
      "train loss: 1.4208279442684397e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2309:\n",
      "train loss: 1.2161696325816965e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2310:\n",
      "train loss: 2.6814548196152604e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2311:\n",
      "train loss: 2.4758016179105836e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2312:\n",
      "train loss: 1.422733642470478e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2313:\n",
      "train loss: 1.2180183430812777e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2314:\n",
      "train loss: 2.6795499683202474e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2315:\n",
      "train loss: 2.4739759386872996e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2316:\n",
      "train loss: 1.4243609317971953e-06\n",
      "lr: 4.3035478547682945e-07\n",
      "Epoch 2317:\n",
      "train loss: 1.2195848691907984e-06\n",
      "Epoch 02319: reducing learning rate of group 0 to 4.0884e-07.\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2318:\n",
      "train loss: 2.677922738471775e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2319:\n",
      "train loss: 2.4724198584216484e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2320:\n",
      "train loss: 1.2308234262947456e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2321:\n",
      "train loss: 1.036252839588354e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2322:\n",
      "train loss: 2.6662765304589165e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2323:\n",
      "train loss: 2.471072925551856e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2324:\n",
      "train loss: 1.2320353077794777e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2325:\n",
      "train loss: 1.03747171963637e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2326:\n",
      "train loss: 2.6649077082461004e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2327:\n",
      "train loss: 2.469682817608571e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2328:\n",
      "train loss: 1.2333385086542307e-06\n",
      "lr: 4.0883704620298793e-07\n",
      "Epoch 2329:\n",
      "train loss: 1.0388490132015072e-06\n",
      "Epoch 02331: reducing learning rate of group 0 to 3.8840e-07.\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2330:\n",
      "train loss: 2.663275481580743e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2331:\n",
      "train loss: 2.4679289007315467e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2332:\n",
      "train loss: 1.0499706997177016e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2333:\n",
      "train loss: 8.655524886267095e-07\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2334:\n",
      "train loss: 2.6506259992011543e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2335:\n",
      "train loss: 2.46441358480642e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2336:\n",
      "train loss: 1.05404406712866e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2337:\n",
      "train loss: 8.709320784290469e-07\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2338:\n",
      "train loss: 2.6425213557085966e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2339:\n",
      "train loss: 2.4540064739227588e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2340:\n",
      "train loss: 1.0667971390670734e-06\n",
      "lr: 3.883951938928385e-07\n",
      "Epoch 2341:\n",
      "train loss: 8.884758458871333e-07\n",
      "Epoch 02343: reducing learning rate of group 0 to 3.6898e-07.\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2342:\n",
      "train loss: 2.615344811338651e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2343:\n",
      "train loss: 2.418333181252011e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2344:\n",
      "train loss: 9.354694938953407e-07\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2345:\n",
      "train loss: 7.894420356846583e-07\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2346:\n",
      "train loss: 2.4877586089400036e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2347:\n",
      "train loss: 2.256755116244524e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2348:\n",
      "train loss: 1.139882439205036e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2349:\n",
      "train loss: 1.0487288994462548e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2350:\n",
      "train loss: 2.1531250030566567e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2351:\n",
      "train loss: 1.8453151053762112e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2352:\n",
      "train loss: 1.6184318818864775e-06\n",
      "lr: 3.6897543419819657e-07\n",
      "Epoch 2353:\n",
      "train loss: 1.552696804701186e-06\n",
      "Epoch 02355: reducing learning rate of group 0 to 3.5053e-07.\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2354:\n",
      "train loss: 1.6863109298269574e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2355:\n",
      "train loss: 1.4084333351455104e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2356:\n",
      "train loss: 1.8444876597789092e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2357:\n",
      "train loss: 1.7298151574259769e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2358:\n",
      "train loss: 1.4101064238863217e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2359:\n",
      "train loss: 1.2151210035100075e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2360:\n",
      "train loss: 1.9716784129101294e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2361:\n",
      "train loss: 1.8052101621804607e-06\n",
      "lr: 3.505266624882867e-07\n",
      "Epoch 2362:\n",
      "train loss: 1.3723075300031857e-06\n",
      "Epoch 02364: reducing learning rate of group 0 to 3.3300e-07.\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2363:\n",
      "train loss: 1.211055913852971e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2364:\n",
      "train loss: 1.9543092603773593e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2365:\n",
      "train loss: 1.7876208931119622e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2366:\n",
      "train loss: 1.2335786846893371e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2367:\n",
      "train loss: 1.082009787395399e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2368:\n",
      "train loss: 1.9250273682727158e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2369:\n",
      "train loss: 1.7604722951939452e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2370:\n",
      "train loss: 1.2592044559534126e-06\n",
      "lr: 3.330003293638724e-07\n",
      "Epoch 2371:\n",
      "train loss: 1.1063393496324221e-06\n",
      "Epoch 02373: reducing learning rate of group 0 to 3.1635e-07.\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2372:\n",
      "train loss: 1.8998504105921638e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2373:\n",
      "train loss: 1.7324667604637802e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2374:\n",
      "train loss: 1.140956788210519e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2375:\n",
      "train loss: 1.001502521911427e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2376:\n",
      "train loss: 1.845961145905396e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2377:\n",
      "train loss: 1.6772779814903909e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2378:\n",
      "train loss: 1.2056618197414753e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2379:\n",
      "train loss: 1.073501535779232e-06\n",
      "lr: 3.1635031289567875e-07\n",
      "Epoch 2380:\n",
      "train loss: 1.769766286982061e-06\n",
      "Epoch 02382: reducing learning rate of group 0 to 3.0053e-07.\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2381:\n",
      "train loss: 1.5970847963949088e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2382:\n",
      "train loss: 1.2887152010474036e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2383:\n",
      "train loss: 1.1637636763366362e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2384:\n",
      "train loss: 1.5402005559311426e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2385:\n",
      "train loss: 1.3802795403582846e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2386:\n",
      "train loss: 1.3560971295714737e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2387:\n",
      "train loss: 1.2265906870753817e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2388:\n",
      "train loss: 1.4818328502563675e-06\n",
      "lr: 3.005327972508948e-07\n",
      "Epoch 2389:\n",
      "train loss: 1.3274291855907323e-06\n",
      "Epoch 02391: reducing learning rate of group 0 to 2.8551e-07.\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2390:\n",
      "train loss: 1.403284159217517e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2391:\n",
      "train loss: 1.2692060975809802e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2392:\n",
      "train loss: 1.307519220881609e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2393:\n",
      "train loss: 1.1644523244325248e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2394:\n",
      "train loss: 1.4263644190646674e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2395:\n",
      "train loss: 1.2958296268034335e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2396:\n",
      "train loss: 1.2839004528098087e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2397:\n",
      "train loss: 1.1433361505695403e-06\n",
      "lr: 2.8550615738835005e-07\n",
      "Epoch 2398:\n",
      "train loss: 1.4452715623786935e-06\n",
      "Epoch 02400: reducing learning rate of group 0 to 2.7123e-07.\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2399:\n",
      "train loss: 1.31247553028081e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2400:\n",
      "train loss: 1.2694347347190927e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2401:\n",
      "train loss: 1.137664866939214e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2402:\n",
      "train loss: 1.3200562277210578e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2403:\n",
      "train loss: 1.1926769186148552e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2404:\n",
      "train loss: 1.2611081157263777e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2405:\n",
      "train loss: 1.1303466249335973e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2406:\n",
      "train loss: 1.3263780072271883e-06\n",
      "lr: 2.7123084951893255e-07\n",
      "Epoch 2407:\n",
      "train loss: 1.1983438986864758e-06\n",
      "Epoch 02409: reducing learning rate of group 0 to 2.5767e-07.\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2408:\n",
      "train loss: 1.255883052062228e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2409:\n",
      "train loss: 1.1257050742009879e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2410:\n",
      "train loss: 1.2075858695678748e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2411:\n",
      "train loss: 1.0855560554678461e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2412:\n",
      "train loss: 1.2462278314689846e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2413:\n",
      "train loss: 1.122871288851896e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2414:\n",
      "train loss: 1.210080876101409e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2415:\n",
      "train loss: 1.0877824627221882e-06\n",
      "lr: 2.576693070429859e-07\n",
      "Epoch 2416:\n",
      "train loss: 1.2441974697061584e-06\n",
      "Epoch 02418: reducing learning rate of group 0 to 2.4479e-07.\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2417:\n",
      "train loss: 1.121036866380357e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2418:\n",
      "train loss: 1.2117026683740624e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2419:\n",
      "train loss: 1.0953595260560576e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2420:\n",
      "train loss: 1.1201393909558102e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2421:\n",
      "train loss: 1.0032937948544645e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2422:\n",
      "train loss: 1.2126064487622114e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2423:\n",
      "train loss: 1.096142006140934e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2424:\n",
      "train loss: 1.1194000920704216e-06\n",
      "lr: 2.447858416908366e-07\n",
      "Epoch 2425:\n",
      "train loss: 1.0026680391623344e-06\n",
      "Epoch 02427: reducing learning rate of group 0 to 2.3255e-07.\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2426:\n",
      "train loss: 1.2130696020158577e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2427:\n",
      "train loss: 1.0965434292154563e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2428:\n",
      "train loss: 1.0082062428674756e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2429:\n",
      "train loss: 8.973674158370254e-07\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2430:\n",
      "train loss: 1.2074718649998537e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2431:\n",
      "train loss: 1.0967389458597109e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2432:\n",
      "train loss: 1.0079794355896996e-06\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2433:\n",
      "train loss: 8.971709336129243e-07\n",
      "lr: 2.3254654960629475e-07\n",
      "Epoch 2434:\n",
      "train loss: 1.207582394522306e-06\n",
      "Epoch 02436: reducing learning rate of group 0 to 2.2092e-07.\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2435:\n",
      "train loss: 1.0968267526463813e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2436:\n",
      "train loss: 1.0078582588728466e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2437:\n",
      "train loss: 9.026126646486482e-07\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2438:\n",
      "train loss: 1.096830803226878e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2439:\n",
      "train loss: 9.915981331253006e-07\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2440:\n",
      "train loss: 1.0078131437352325e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2441:\n",
      "train loss: 9.025840641010352e-07\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2442:\n",
      "train loss: 1.0967918883787445e-06\n",
      "lr: 2.2091922212598e-07\n",
      "Epoch 2443:\n",
      "train loss: 9.915530479017064e-07\n",
      "Epoch 02445: reducing learning rate of group 0 to 2.0987e-07.\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2444:\n",
      "train loss: 1.0078092751369777e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2445:\n",
      "train loss: 9.025915369196552e-07\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2446:\n",
      "train loss: 9.967549265653641e-07\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2447:\n",
      "train loss: 8.967776237781027e-07\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2448:\n",
      "train loss: 1.0025640964108687e-06\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2449:\n",
      "train loss: 9.026126588660014e-07\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2450:\n",
      "train loss: 9.966772387822368e-07\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2451:\n",
      "train loss: 8.967014713501829e-07\n",
      "lr: 2.09873261019681e-07\n",
      "Epoch 2452:\n",
      "train loss: 1.0025873714685845e-06\n",
      "Epoch 02454: reducing learning rate of group 0 to 1.9938e-07.\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2453:\n",
      "train loss: 9.02639896092818e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2454:\n",
      "train loss: 9.965954051103844e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2455:\n",
      "train loss: 9.016199326021697e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2456:\n",
      "train loss: 9.026552752037453e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2457:\n",
      "train loss: 8.077084627060965e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2458:\n",
      "train loss: 9.965143612731641e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2459:\n",
      "train loss: 9.015406443543183e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2460:\n",
      "train loss: 9.026854069382556e-07\n",
      "lr: 1.9937959796869694e-07\n",
      "Epoch 2461:\n",
      "train loss: 8.077419784907416e-07\n",
      "Epoch 02463: reducing learning rate of group 0 to 1.8941e-07.\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2462:\n",
      "train loss: 9.96430227927002e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2463:\n",
      "train loss: 9.014586024013833e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2464:\n",
      "train loss: 8.125093127118634e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2465:\n",
      "train loss: 7.223161047423073e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2466:\n",
      "train loss: 9.915997588694956e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2467:\n",
      "train loss: 9.013789685090091e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2468:\n",
      "train loss: 8.125417812224662e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2469:\n",
      "train loss: 7.22351196700868e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2470:\n",
      "train loss: 9.915175687302982e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2471:\n",
      "train loss: 9.012995377700088e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2472:\n",
      "train loss: 8.125738492167491e-07\n",
      "lr: 1.894106180702621e-07\n",
      "Epoch 2473:\n",
      "train loss: 7.223854272795267e-07\n",
      "Epoch 02475: reducing learning rate of group 0 to 1.7994e-07.\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2474:\n",
      "train loss: 9.914368206056312e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2475:\n",
      "train loss: 9.012216346548202e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2476:\n",
      "train loss: 7.269134324667863e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2477:\n",
      "train loss: 6.412363151509588e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2478:\n",
      "train loss: 9.868511571994049e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2479:\n",
      "train loss: 9.011495003719428e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2480:\n",
      "train loss: 7.269410971388265e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2481:\n",
      "train loss: 6.412659519913851e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2482:\n",
      "train loss: 9.867778481789663e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2483:\n",
      "train loss: 9.0107873340672e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2484:\n",
      "train loss: 7.269678326798703e-07\n",
      "lr: 1.7994008716674898e-07\n",
      "Epoch 2485:\n",
      "train loss: 6.412948140394135e-07\n",
      "Epoch 02487: reducing learning rate of group 0 to 1.7094e-07.\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2486:\n",
      "train loss: 9.86705438367519e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2487:\n",
      "train loss: 9.01008768563486e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2488:\n",
      "train loss: 6.455939905539103e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2489:\n",
      "train loss: 5.642065786391851e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2490:\n",
      "train loss: 9.823527222145699e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2491:\n",
      "train loss: 9.009433457763303e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2492:\n",
      "train loss: 6.456179645534192e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2493:\n",
      "train loss: 5.642324592878533e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2494:\n",
      "train loss: 9.82286040347282e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2495:\n",
      "train loss: 9.008790674797702e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2496:\n",
      "train loss: 6.456410747592626e-07\n",
      "lr: 1.7094308280841153e-07\n",
      "Epoch 2497:\n",
      "train loss: 5.642575070850826e-07\n",
      "Epoch 02499: reducing learning rate of group 0 to 1.6240e-07.\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2498:\n",
      "train loss: 9.822203740635867e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2499:\n",
      "train loss: 9.008157324060024e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2500:\n",
      "train loss: 5.683396288933699e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2501:\n",
      "train loss: 4.910270238682564e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2502:\n",
      "train loss: 9.780887617506806e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2503:\n",
      "train loss: 9.00756655101541e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2504:\n",
      "train loss: 5.683600229051007e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2505:\n",
      "train loss: 4.910492296527529e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2506:\n",
      "train loss: 9.78028387841973e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2507:\n",
      "train loss: 9.006984323962772e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2508:\n",
      "train loss: 5.683799157534949e-07\n",
      "lr: 1.6239592866799096e-07\n",
      "Epoch 2509:\n",
      "train loss: 4.910710478685232e-07\n",
      "Epoch 02511: reducing learning rate of group 0 to 1.5428e-07.\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2510:\n",
      "train loss: 9.779685014234044e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2511:\n",
      "train loss: 9.006405710914803e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2512:\n",
      "train loss: 4.949478139473739e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2513:\n",
      "train loss: 4.215062637612688e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2514:\n",
      "train loss: 9.74045332297073e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2515:\n",
      "train loss: 9.005856577553977e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2516:\n",
      "train loss: 4.949668932774397e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2517:\n",
      "train loss: 4.215273738989416e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2518:\n",
      "train loss: 9.739881146102866e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2519:\n",
      "train loss: 9.005300105739877e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2520:\n",
      "train loss: 4.9498718834516e-07\n",
      "lr: 1.542761322345914e-07\n",
      "Epoch 2521:\n",
      "train loss: 4.215500358772797e-07\n",
      "Epoch 02523: reducing learning rate of group 0 to 1.4656e-07.\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2522:\n",
      "train loss: 9.73929056267818e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2523:\n",
      "train loss: 9.004720420343602e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2524:\n",
      "train loss: 4.252364630949907e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2525:\n",
      "train loss: 3.554745316963826e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2526:\n",
      "train loss: 9.701939246744591e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2527:\n",
      "train loss: 9.004087521105322e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2528:\n",
      "train loss: 4.252690414429957e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2529:\n",
      "train loss: 3.5551361650166005e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2530:\n",
      "train loss: 9.701127498495474e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2531:\n",
      "train loss: 9.003215833931809e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2532:\n",
      "train loss: 4.2533082448015406e-07\n",
      "lr: 1.4656232562286184e-07\n",
      "Epoch 2533:\n",
      "train loss: 3.555910440822481e-07\n",
      "Epoch 02535: reducing learning rate of group 0 to 1.3923e-07.\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2534:\n",
      "train loss: 9.699763888475413e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2535:\n",
      "train loss: 9.001639244249274e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2536:\n",
      "train loss: 3.591986415835602e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2537:\n",
      "train loss: 2.930112513841991e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2538:\n",
      "train loss: 9.661047735277684e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2539:\n",
      "train loss: 8.996656811499665e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2540:\n",
      "train loss: 3.5978716019303086e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2541:\n",
      "train loss: 2.938927413972242e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2542:\n",
      "train loss: 9.645542187842585e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2543:\n",
      "train loss: 8.975832321262976e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2544:\n",
      "train loss: 3.6239061588694886e-07\n",
      "lr: 1.3923420934171875e-07\n",
      "Epoch 2545:\n",
      "train loss: 2.978553424782248e-07\n",
      "Epoch 02547: reducing learning rate of group 0 to 1.3227e-07.\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2546:\n",
      "train loss: 9.575859271797785e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2547:\n",
      "train loss: 8.881152319050596e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2548:\n",
      "train loss: 3.114670562412011e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2549:\n",
      "train loss: 2.5919956804935286e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2550:\n",
      "train loss: 9.117379004811047e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2551:\n",
      "train loss: 8.283699492736406e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2552:\n",
      "train loss: 3.874876068295959e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2553:\n",
      "train loss: 3.613190465784299e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2554:\n",
      "train loss: 7.678256304884386e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2555:\n",
      "train loss: 6.455581625622015e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2556:\n",
      "train loss: 6.01320945035303e-07\n",
      "lr: 1.322724988746328e-07\n",
      "Epoch 2557:\n",
      "train loss: 5.856914170399453e-07\n",
      "Epoch 02559: reducing learning rate of group 0 to 1.2566e-07.\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2558:\n",
      "train loss: 5.669233284331713e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2559:\n",
      "train loss: 4.6669625036187415e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2560:\n",
      "train loss: 6.858028250662417e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2561:\n",
      "train loss: 6.292418610778733e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2562:\n",
      "train loss: 5.129228370278304e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2563:\n",
      "train loss: 4.60695736763481e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2564:\n",
      "train loss: 6.584349712030178e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2565:\n",
      "train loss: 5.725704575192736e-07\n",
      "lr: 1.2565887393090114e-07\n",
      "Epoch 2566:\n",
      "train loss: 5.904219041526082e-07\n",
      "Epoch 02568: reducing learning rate of group 0 to 1.1938e-07.\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2567:\n",
      "train loss: 5.503799110431115e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2568:\n",
      "train loss: 5.696497789858229e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2569:\n",
      "train loss: 4.965392677147447e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2570:\n",
      "train loss: 5.974279002757571e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2571:\n",
      "train loss: 5.525310156776214e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2572:\n",
      "train loss: 5.15904072558638e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2573:\n",
      "train loss: 4.49992176220409e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2574:\n",
      "train loss: 6.366564972556389e-07\n",
      "lr: 1.1937593023435608e-07\n",
      "Epoch 2575:\n",
      "train loss: 5.872810958766902e-07\n",
      "Epoch 02577: reducing learning rate of group 0 to 1.1341e-07.\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2576:\n",
      "train loss: 4.845808497804363e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2577:\n",
      "train loss: 4.2190107439609856e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2578:\n",
      "train loss: 6.071286990478291e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2579:\n",
      "train loss: 5.564615045731867e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2580:\n",
      "train loss: 4.6625348221511737e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2581:\n",
      "train loss: 4.1031088922728903e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2582:\n",
      "train loss: 6.150835108383224e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2583:\n",
      "train loss: 5.596910158880257e-07\n",
      "lr: 1.1340713372263827e-07\n",
      "Epoch 2584:\n",
      "train loss: 4.6815979550340203e-07\n",
      "Epoch 02586: reducing learning rate of group 0 to 1.0774e-07.\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2585:\n",
      "train loss: 4.163771656083637e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2586:\n",
      "train loss: 6.058882441398272e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2587:\n",
      "train loss: 5.50197672644727e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2588:\n",
      "train loss: 4.292181095653305e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2589:\n",
      "train loss: 3.827836957808279e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2590:\n",
      "train loss: 5.862757287061094e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2591:\n",
      "train loss: 5.292900266722638e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2592:\n",
      "train loss: 4.505875999421672e-07\n",
      "lr: 1.0773677703650634e-07\n",
      "Epoch 2593:\n",
      "train loss: 4.0427973875436815e-07\n",
      "Epoch 02595: reducing learning rate of group 0 to 1.0235e-07.\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2594:\n",
      "train loss: 5.654149127860381e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2595:\n",
      "train loss: 5.097657076666896e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2596:\n",
      "train loss: 4.195943227981788e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2597:\n",
      "train loss: 3.743206950316169e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2598:\n",
      "train loss: 5.481122158064921e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2599:\n",
      "train loss: 4.966033573133909e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2600:\n",
      "train loss: 4.314395444316207e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2601:\n",
      "train loss: 3.8499205373840366e-07\n",
      "lr: 1.0234993818468103e-07\n",
      "Epoch 2602:\n",
      "train loss: 5.384570863790546e-07\n",
      "Epoch 02604: reducing learning rate of group 0 to 9.7232e-08.\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2603:\n",
      "train loss: 4.878270921468135e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2604:\n",
      "train loss: 4.394807512440928e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2605:\n",
      "train loss: 3.94674609845203e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2606:\n",
      "train loss: 4.832508429479341e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2607:\n",
      "train loss: 4.356503672513165e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2608:\n",
      "train loss: 4.4486702410904955e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2609:\n",
      "train loss: 3.9962955702068933e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2610:\n",
      "train loss: 4.787139061523786e-07\n",
      "lr: 9.723244127544697e-08\n",
      "Epoch 2611:\n",
      "train loss: 4.315002502486617e-07\n",
      "Epoch 02613: reducing learning rate of group 0 to 9.2371e-08.\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2612:\n",
      "train loss: 4.486529893811755e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2613:\n",
      "train loss: 4.031175666186999e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2614:\n",
      "train loss: 4.3156650130138963e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2615:\n",
      "train loss: 3.87012518891119e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2616:\n",
      "train loss: 4.4882097721022883e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2617:\n",
      "train loss: 4.053251039957292e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2618:\n",
      "train loss: 4.2954370512674875e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2619:\n",
      "train loss: 3.852036969458227e-07\n",
      "lr: 9.237081921167461e-08\n",
      "Epoch 2620:\n",
      "train loss: 4.5041069739923025e-07\n",
      "Epoch 02622: reducing learning rate of group 0 to 8.7752e-08.\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2621:\n",
      "train loss: 4.067563433939866e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2622:\n",
      "train loss: 4.2823004654305524e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2623:\n",
      "train loss: 3.8622892216109565e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2624:\n",
      "train loss: 4.0747815816099056e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2625:\n",
      "train loss: 3.659057729813946e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2626:\n",
      "train loss: 4.274068155680967e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2627:\n",
      "train loss: 3.8548169594201863e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2628:\n",
      "train loss: 4.081454679378737e-07\n",
      "lr: 8.775227825109088e-08\n",
      "Epoch 2629:\n",
      "train loss: 3.6651073105384596e-07\n",
      "Epoch 02631: reducing learning rate of group 0 to 8.3365e-08.\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2630:\n",
      "train loss: 4.2684509351314614e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2631:\n",
      "train loss: 3.849704770378175e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2632:\n",
      "train loss: 3.6892057118539016e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2633:\n",
      "train loss: 3.293350158702202e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2634:\n",
      "train loss: 4.2436500474728414e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2635:\n",
      "train loss: 3.8461111287524217e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2636:\n",
      "train loss: 3.692385106459955e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2637:\n",
      "train loss: 3.296338109998082e-07\n",
      "lr: 8.336466433853634e-08\n",
      "Epoch 2638:\n",
      "train loss: 4.240687459012927e-07\n",
      "Epoch 02640: reducing learning rate of group 0 to 7.9196e-08.\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2639:\n",
      "train loss: 3.8433871996966963e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2640:\n",
      "train loss: 3.694701841276377e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2641:\n",
      "train loss: 3.3182844635081204e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2642:\n",
      "train loss: 3.841891098615206e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2643:\n",
      "train loss: 3.464644408784032e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2644:\n",
      "train loss: 3.6962004446799576e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2645:\n",
      "train loss: 3.3196429568864147e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2646:\n",
      "train loss: 3.840498752395307e-07\n",
      "lr: 7.919643112160952e-08\n",
      "Epoch 2647:\n",
      "train loss: 3.463379765927599e-07\n",
      "Epoch 02649: reducing learning rate of group 0 to 7.5237e-08.\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2648:\n",
      "train loss: 3.6971949437367443e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2649:\n",
      "train loss: 3.3205457009811693e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2650:\n",
      "train loss: 3.4815227193172707e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2651:\n",
      "train loss: 3.123342164897931e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2652:\n",
      "train loss: 3.6789829844496173e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2653:\n",
      "train loss: 3.3211073011885633e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2654:\n",
      "train loss: 3.4808715802727976e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2655:\n",
      "train loss: 3.122754273483462e-07\n",
      "lr: 7.523660956552904e-08\n",
      "Epoch 2656:\n",
      "train loss: 3.67936966197145e-07\n",
      "Epoch 02658: reducing learning rate of group 0 to 7.1475e-08.\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2657:\n",
      "train loss: 3.321459417894022e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2658:\n",
      "train loss: 3.480404317517821e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2659:\n",
      "train loss: 3.1402345577023315e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2660:\n",
      "train loss: 3.32160871442126e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2661:\n",
      "train loss: 2.981580153887838e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2662:\n",
      "train loss: 3.4800619973015763e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2663:\n",
      "train loss: 3.139918648530102e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2664:\n",
      "train loss: 3.321760566230041e-07\n",
      "lr: 7.147477908725257e-08\n",
      "Epoch 2665:\n",
      "train loss: 2.9817256057944756e-07\n",
      "Epoch 02667: reducing learning rate of group 0 to 6.7901e-08.\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2666:\n",
      "train loss: 3.4797833415885337e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2667:\n",
      "train loss: 3.1396604200795384e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2668:\n",
      "train loss: 2.99878576879018e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2669:\n",
      "train loss: 2.6757497265659394e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2670:\n",
      "train loss: 3.4625556051880315e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2671:\n",
      "train loss: 3.1394538579984613e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2672:\n",
      "train loss: 2.9988489041512435e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2673:\n",
      "train loss: 2.67581363675642e-07\n",
      "lr: 6.790104013288995e-08\n",
      "Epoch 2674:\n",
      "train loss: 3.4623610734387645e-07\n",
      "Epoch 02676: reducing learning rate of group 0 to 6.4506e-08.\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2675:\n",
      "train loss: 3.139270796543008e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2676:\n",
      "train loss: 2.998892660813532e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2677:\n",
      "train loss: 2.6920125337343596e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2678:\n",
      "train loss: 3.1391282453233565e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2679:\n",
      "train loss: 2.832202494887997e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2680:\n",
      "train loss: 2.998921152118789e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2681:\n",
      "train loss: 2.692044836005203e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2682:\n",
      "train loss: 3.1389706647156275e-07\n",
      "lr: 6.450598812624545e-08\n",
      "Epoch 2683:\n",
      "train loss: 2.832054450875709e-07\n",
      "Epoch 02685: reducing learning rate of group 0 to 6.1281e-08.\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2684:\n",
      "train loss: 2.9989388652052714e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2685:\n",
      "train loss: 2.692067199154641e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2686:\n",
      "train loss: 2.847278226881078e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2687:\n",
      "train loss: 2.555716317241424e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2688:\n",
      "train loss: 2.983604388036793e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2689:\n",
      "train loss: 2.692080855335318e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2690:\n",
      "train loss: 2.8471457960201877e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2691:\n",
      "train loss: 2.5555917671369167e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2692:\n",
      "train loss: 2.9836075322228824e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2693:\n",
      "train loss: 2.6920890992830573e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2694:\n",
      "train loss: 2.847018957660369e-07\n",
      "lr: 6.128068871993318e-08\n",
      "Epoch 2695:\n",
      "train loss: 2.555472162911209e-07\n",
      "Epoch 02697: reducing learning rate of group 0 to 5.8217e-08.\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2696:\n",
      "train loss: 2.9836069768717276e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2697:\n",
      "train loss: 2.69209400470135e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2698:\n",
      "train loss: 2.569946297757046e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2699:\n",
      "train loss: 2.2929835417554475e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2700:\n",
      "train loss: 2.9690280872966625e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2701:\n",
      "train loss: 2.6920958963681594e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2702:\n",
      "train loss: 2.5698327966884855e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2703:\n",
      "train loss: 2.2928765951188924e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2704:\n",
      "train loss: 2.969022423475978e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2705:\n",
      "train loss: 2.6920956012745146e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2706:\n",
      "train loss: 2.5697218374654814e-07\n",
      "lr: 5.8216654283936515e-08\n",
      "Epoch 2707:\n",
      "train loss: 2.2927719383743877e-07\n",
      "Epoch 02709: reducing learning rate of group 0 to 5.5306e-08.\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2708:\n",
      "train loss: 2.9690153162349017e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2709:\n",
      "train loss: 2.692094045531992e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2710:\n",
      "train loss: 2.3065271786734932e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2711:\n",
      "train loss: 2.0434306834728774e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2712:\n",
      "train loss: 2.955161487845531e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2713:\n",
      "train loss: 2.692091442744526e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2714:\n",
      "train loss: 2.3064251661838538e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2715:\n",
      "train loss: 2.0433344776661076e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2716:\n",
      "train loss: 2.955152801058837e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2717:\n",
      "train loss: 2.692088038867007e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2718:\n",
      "train loss: 2.3063244244818942e-07\n",
      "lr: 5.5305821569739684e-08\n",
      "Epoch 2719:\n",
      "train loss: 2.0432393870722535e-07\n",
      "Epoch 02721: reducing learning rate of group 0 to 5.2541e-08.\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2720:\n",
      "train loss: 2.9551437084547007e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2721:\n",
      "train loss: 2.692084270547409e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2722:\n",
      "train loss: 2.0563090830278117e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2723:\n",
      "train loss: 1.8063836909334496e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2724:\n",
      "train loss: 2.9419816191569717e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2725:\n",
      "train loss: 2.692080047689457e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2726:\n",
      "train loss: 2.0562155685748244e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2727:\n",
      "train loss: 1.8062955634152774e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2728:\n",
      "train loss: 2.9419718216911063e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2729:\n",
      "train loss: 2.692075178034048e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2730:\n",
      "train loss: 2.0561231920805307e-07\n",
      "lr: 5.2540530491252695e-08\n",
      "Epoch 2731:\n",
      "train loss: 1.8062084712538953e-07\n",
      "Epoch 02733: reducing learning rate of group 0 to 4.9914e-08.\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2732:\n",
      "train loss: 2.94196161047603e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2733:\n",
      "train loss: 2.692069940068417e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2734:\n",
      "train loss: 1.8186265495904208e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2735:\n",
      "train loss: 1.5812126281546786e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2736:\n",
      "train loss: 2.929456846488401e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2737:\n",
      "train loss: 2.6920643134650263e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2738:\n",
      "train loss: 1.8185409163523317e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2739:\n",
      "train loss: 1.5811320235421367e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2740:\n",
      "train loss: 2.929446007389248e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2741:\n",
      "train loss: 2.692058092604734e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2742:\n",
      "train loss: 1.8184563122160453e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2743:\n",
      "train loss: 1.581052337206455e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2744:\n",
      "train loss: 2.929434835256282e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2745:\n",
      "train loss: 2.6920515737994263e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2746:\n",
      "train loss: 1.818372466155089e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2747:\n",
      "train loss: 1.5809733504059224e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2748:\n",
      "train loss: 2.9294235257186845e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2749:\n",
      "train loss: 2.692044920107325e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2750:\n",
      "train loss: 1.8182892322778393e-07\n",
      "lr: 4.991350396669006e-08\n",
      "Epoch 2751:\n",
      "train loss: 1.5808948933400405e-07\n",
      "Epoch 02753: reducing learning rate of group 0 to 4.7418e-08.\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2752:\n",
      "train loss: 2.9294122200712123e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2753:\n",
      "train loss: 2.692038276647443e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2754:\n",
      "train loss: 1.5926942017678156e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2755:\n",
      "train loss: 1.367174203874837e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2756:\n",
      "train loss: 2.917532555183352e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2757:\n",
      "train loss: 2.692031542877787e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2758:\n",
      "train loss: 1.5926166415641902e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2759:\n",
      "train loss: 1.3671012701348164e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2760:\n",
      "train loss: 2.91752104378559e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2761:\n",
      "train loss: 2.69202429282104e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2762:\n",
      "train loss: 1.5925400446534952e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2763:\n",
      "train loss: 1.3670292524530267e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2764:\n",
      "train loss: 2.917509114287714e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2765:\n",
      "train loss: 2.6920165881958784e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2766:\n",
      "train loss: 1.5924643761035198e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2767:\n",
      "train loss: 1.3669581527911407e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2768:\n",
      "train loss: 2.9174966981809165e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2769:\n",
      "train loss: 2.692008340148204e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2770:\n",
      "train loss: 1.5923897556433684e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2771:\n",
      "train loss: 1.3668881263224807e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2772:\n",
      "train loss: 2.9174835824554757e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2773:\n",
      "train loss: 2.691999292966558e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2774:\n",
      "train loss: 1.5923165128990016e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2775:\n",
      "train loss: 1.3668195576153003e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2776:\n",
      "train loss: 2.9174693430593745e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2777:\n",
      "train loss: 2.691988956598985e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2778:\n",
      "train loss: 1.5922451519752563e-07\n",
      "lr: 4.741782876835556e-08\n",
      "Epoch 2779:\n",
      "train loss: 1.366753026140674e-07\n",
      "Epoch 02781: reducing learning rate of group 0 to 4.5047e-08.\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2780:\n",
      "train loss: 2.917453282473034e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2781:\n",
      "train loss: 2.6919765282763754e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2782:\n",
      "train loss: 1.3779689310778646e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2783:\n",
      "train loss: 1.1637567847908177e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2784:\n",
      "train loss: 2.9061595494627395e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2785:\n",
      "train loss: 2.69195862994785e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2786:\n",
      "train loss: 1.3779116803938665e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2787:\n",
      "train loss: 1.1637063778561108e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2788:\n",
      "train loss: 2.9061286265865423e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2789:\n",
      "train loss: 2.691927784673034e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2790:\n",
      "train loss: 1.3778697531365575e-07\n",
      "lr: 4.504693732993778e-08\n",
      "Epoch 2791:\n",
      "train loss: 1.1636744480505959e-07\n",
      "Epoch 02793: reducing learning rate of group 0 to 4.2795e-08.\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2792:\n",
      "train loss: 2.9060741024961167e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2793:\n",
      "train loss: 2.691868265725102e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2794:\n",
      "train loss: 1.1743760429119458e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2795:\n",
      "train loss: 9.70915437629931e-08\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2796:\n",
      "train loss: 2.895230248680455e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2797:\n",
      "train loss: 2.691701612725293e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2798:\n",
      "train loss: 1.1745067385869632e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2799:\n",
      "train loss: 9.711215881224367e-08\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2800:\n",
      "train loss: 2.8948113952848776e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2801:\n",
      "train loss: 2.6911668092707157e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2802:\n",
      "train loss: 1.175088671095005e-07\n",
      "lr: 4.279459046344089e-08\n",
      "Epoch 2803:\n",
      "train loss: 9.719580736830908e-08\n",
      "Epoch 02805: reducing learning rate of group 0 to 4.0655e-08.\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2804:\n",
      "train loss: 2.893399205913443e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2805:\n",
      "train loss: 2.6893293886011064e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2806:\n",
      "train loss: 9.839825539987394e-08\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2807:\n",
      "train loss: 7.924477208442167e-08\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2808:\n",
      "train loss: 2.87619088081978e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2809:\n",
      "train loss: 2.679620618753525e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2810:\n",
      "train loss: 9.96277426846406e-08\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2811:\n",
      "train loss: 8.124193547116375e-08\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2812:\n",
      "train loss: 2.838571877670001e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2813:\n",
      "train loss: 2.627953140678306e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2814:\n",
      "train loss: 1.0614529540830784e-07\n",
      "lr: 4.0654860940268846e-08\n",
      "Epoch 2815:\n",
      "train loss: 9.109262095995295e-08\n",
      "Epoch 02817: reducing learning rate of group 0 to 3.8622e-08.\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2816:\n",
      "train loss: 2.6704330191024733e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2817:\n",
      "train loss: 2.4021283608513075e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2818:\n",
      "train loss: 1.1547160958919869e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2819:\n",
      "train loss: 1.0959131380627834e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2820:\n",
      "train loss: 2.1744454078316097e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2821:\n",
      "train loss: 1.79934242804365e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2822:\n",
      "train loss: 1.844430583302676e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2823:\n",
      "train loss: 1.8048428195791938e-07\n",
      "lr: 3.8622117893255404e-08\n",
      "Epoch 2824:\n",
      "train loss: 1.5578578465368203e-07\n",
      "Epoch 02826: reducing learning rate of group 0 to 3.6691e-08.\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2825:\n",
      "train loss: 1.2909421635789545e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2826:\n",
      "train loss: 2.1803182832035552e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2827:\n",
      "train loss: 1.9565403209921744e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2828:\n",
      "train loss: 1.436097225752719e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2829:\n",
      "train loss: 1.3598377213827326e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2830:\n",
      "train loss: 1.8152415579199008e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2831:\n",
      "train loss: 1.4815543635684033e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2832:\n",
      "train loss: 1.9636351355419407e-07\n",
      "lr: 3.669101199859263e-08\n",
      "Epoch 2833:\n",
      "train loss: 1.8740486123368464e-07\n",
      "Epoch 02835: reducing learning rate of group 0 to 3.4856e-08.\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2834:\n",
      "train loss: 1.3926598499423387e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2835:\n",
      "train loss: 1.196618292716954e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2836:\n",
      "train loss: 1.9302536908567133e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2837:\n",
      "train loss: 1.7181703169104433e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2838:\n",
      "train loss: 1.4843293657999506e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2839:\n",
      "train loss: 1.3650081570728202e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2840:\n",
      "train loss: 1.7384125029275093e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2841:\n",
      "train loss: 1.523990009997216e-07\n",
      "lr: 3.4856461398663e-08\n",
      "Epoch 2842:\n",
      "train loss: 1.6696890160938857e-07\n",
      "Epoch 02844: reducing learning rate of group 0 to 3.3114e-08.\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2843:\n",
      "train loss: 1.5425474441787912e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2844:\n",
      "train loss: 1.571068298419593e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2845:\n",
      "train loss: 1.3843160337907588e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2846:\n",
      "train loss: 1.6319459154211326e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2847:\n",
      "train loss: 1.4984868450512725e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2848:\n",
      "train loss: 1.4696567696944333e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2849:\n",
      "train loss: 1.2927996367096773e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2850:\n",
      "train loss: 1.7140538665412915e-07\n",
      "lr: 3.311363832872985e-08\n",
      "Epoch 2851:\n",
      "train loss: 1.570905356031073e-07\n",
      "Epoch 02853: reducing learning rate of group 0 to 3.1458e-08.\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2852:\n",
      "train loss: 1.4076011859420367e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2853:\n",
      "train loss: 1.2390976669862685e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2854:\n",
      "train loss: 1.6101005743632092e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2855:\n",
      "train loss: 1.4652374163564588e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2856:\n",
      "train loss: 1.3736133796920666e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2857:\n",
      "train loss: 1.220570414457777e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2858:\n",
      "train loss: 1.6232181200457093e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2859:\n",
      "train loss: 1.4726850838177258e-07\n",
      "lr: 3.1457956412293355e-08\n",
      "Epoch 2860:\n",
      "train loss: 1.3715745601222353e-07\n",
      "Epoch 02862: reducing learning rate of group 0 to 2.9885e-08.\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2861:\n",
      "train loss: 1.223249406071573e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2862:\n",
      "train loss: 1.6171769370382614e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2863:\n",
      "train loss: 1.4720295889851765e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2864:\n",
      "train loss: 1.2316006628310607e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2865:\n",
      "train loss: 1.0927564743075666e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2866:\n",
      "train loss: 1.603923376996295e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2867:\n",
      "train loss: 1.4582963409814843e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2868:\n",
      "train loss: 1.2450004780001658e-07\n",
      "lr: 2.9885058591678684e-08\n",
      "Epoch 2869:\n",
      "train loss: 1.1059109978624337e-07\n",
      "Epoch 02871: reducing learning rate of group 0 to 2.8391e-08.\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2870:\n",
      "train loss: 1.5912839778029067e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2871:\n",
      "train loss: 1.4467046118428622e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2872:\n",
      "train loss: 1.1203168177110027e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2873:\n",
      "train loss: 9.872376031447669e-08\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2874:\n",
      "train loss: 1.5758476924956081e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2875:\n",
      "train loss: 1.4391754379885622e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2876:\n",
      "train loss: 1.1273208327598015e-07\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2877:\n",
      "train loss: 9.938072335263988e-08\n",
      "lr: 2.8390805662094748e-08\n",
      "Epoch 2878:\n",
      "train loss: 1.5694929660074176e-07\n",
      "Epoch 02880: reducing learning rate of group 0 to 2.6971e-08.\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2879:\n",
      "train loss: 1.4328487972031466e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2880:\n",
      "train loss: 1.1337309102781561e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2881:\n",
      "train loss: 1.0069237906173901e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2882:\n",
      "train loss: 1.4282484332212602e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2883:\n",
      "train loss: 1.2983926343748916e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2884:\n",
      "train loss: 1.139876021615288e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2885:\n",
      "train loss: 1.0130437574756441e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2886:\n",
      "train loss: 1.4222183102280455e-07\n",
      "lr: 2.697126537899001e-08\n",
      "Epoch 2887:\n",
      "train loss: 1.2925346331244945e-07\n",
      "Epoch 02889: reducing learning rate of group 0 to 2.5623e-08.\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2888:\n",
      "train loss: 1.1454897759070788e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2889:\n",
      "train loss: 1.0184749394107444e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2890:\n",
      "train loss: 1.295229932366339e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2891:\n",
      "train loss: 1.1723551789182189e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2892:\n",
      "train loss: 1.1433728993210108e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2893:\n",
      "train loss: 1.0223984905377613e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2894:\n",
      "train loss: 1.2915695897929617e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2895:\n",
      "train loss: 1.1690362375525429e-07\n",
      "lr: 2.5622702110040507e-08\n",
      "Epoch 2896:\n",
      "train loss: 1.1463155825071244e-07\n",
      "Epoch 02898: reducing learning rate of group 0 to 2.4342e-08.\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2897:\n",
      "train loss: 1.0250633576944308e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2898:\n",
      "train loss: 1.2891102065249926e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2899:\n",
      "train loss: 1.1729328870893773e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2900:\n",
      "train loss: 1.026404797591129e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2901:\n",
      "train loss: 9.110319608964102e-08\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2902:\n",
      "train loss: 1.2875522891762083e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2903:\n",
      "train loss: 1.171508920617137e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2904:\n",
      "train loss: 1.0276803679926819e-07\n",
      "lr: 2.434156700453848e-08\n",
      "Epoch 2905:\n",
      "train loss: 9.12207802281875e-08\n",
      "Epoch 02907: reducing learning rate of group 0 to 2.3124e-08.\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2906:\n",
      "train loss: 1.2864255404062094e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2907:\n",
      "train loss: 1.1704517479636255e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2908:\n",
      "train loss: 9.186929724347191e-08\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2909:\n",
      "train loss: 8.089590763371375e-08\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2910:\n",
      "train loss: 1.2797277498681515e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2911:\n",
      "train loss: 1.1695695376394254e-07\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2912:\n",
      "train loss: 9.195244666392369e-08\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2913:\n",
      "train loss: 8.097784355648664e-08\n",
      "lr: 2.3124488654311553e-08\n",
      "Epoch 2914:\n",
      "train loss: 1.278891794670607e-07\n",
      "Epoch 02916: reducing learning rate of group 0 to 2.1968e-08.\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2915:\n",
      "train loss: 1.1687596496352699e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2916:\n",
      "train loss: 9.202700954276932e-08\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2917:\n",
      "train loss: 8.159863130867643e-08\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2918:\n",
      "train loss: 1.1682507588607458e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2919:\n",
      "train loss: 1.0636669153396456e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2920:\n",
      "train loss: 9.208337825681036e-08\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2921:\n",
      "train loss: 8.165133564833218e-08\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2922:\n",
      "train loss: 1.1677266605051774e-07\n",
      "lr: 2.1968264221595975e-08\n",
      "Epoch 2923:\n",
      "train loss: 1.0631858077275051e-07\n",
      "Epoch 02925: reducing learning rate of group 0 to 2.0870e-08.\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2924:\n",
      "train loss: 9.212386866460244e-08\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2925:\n",
      "train loss: 8.168844420631465e-08\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2926:\n",
      "train loss: 1.0681415112605268e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2927:\n",
      "train loss: 9.688610554845142e-08\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2928:\n",
      "train loss: 9.16278616924855e-08\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2929:\n",
      "train loss: 8.171174799077568e-08\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2930:\n",
      "train loss: 1.0678982622343679e-07\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2931:\n",
      "train loss: 9.68641365178211e-08\n",
      "lr: 2.0869851010516175e-08\n",
      "Epoch 2932:\n",
      "train loss: 9.1644483863659e-08\n",
      "Epoch 02934: reducing learning rate of group 0 to 1.9826e-08.\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2933:\n",
      "train loss: 8.172677573329211e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2934:\n",
      "train loss: 1.0677300512207819e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2935:\n",
      "train loss: 9.734507964051112e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2936:\n",
      "train loss: 8.173377173932665e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2937:\n",
      "train loss: 7.231109111016245e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2938:\n",
      "train loss: 1.0676140293529804e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2939:\n",
      "train loss: 9.733442362961865e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2940:\n",
      "train loss: 8.174054526592493e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2941:\n",
      "train loss: 7.231740461893382e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2942:\n",
      "train loss: 1.0675248839290613e-07\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2943:\n",
      "train loss: 9.732620999998235e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2944:\n",
      "train loss: 8.174508960803843e-08\n",
      "lr: 1.9826358459990364e-08\n",
      "Epoch 2945:\n",
      "train loss: 7.232167088690922e-08\n",
      "Epoch 02947: reducing learning rate of group 0 to 1.8835e-08.\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2946:\n",
      "train loss: 1.0674547655437191e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2947:\n",
      "train loss: 9.731977111013962e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2948:\n",
      "train loss: 7.27945988521575e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2949:\n",
      "train loss: 6.384218865809395e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2950:\n",
      "train loss: 1.0626890284897785e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2951:\n",
      "train loss: 9.731492515466861e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2952:\n",
      "train loss: 7.279618809975681e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2953:\n",
      "train loss: 6.384368893166346e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2954:\n",
      "train loss: 1.0626464960404769e-07\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2955:\n",
      "train loss: 9.731104260764456e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2956:\n",
      "train loss: 7.279690106722751e-08\n",
      "lr: 1.8835040536990846e-08\n",
      "Epoch 2957:\n",
      "train loss: 6.384438294951882e-08\n",
      "Epoch 02959: reducing learning rate of group 0 to 1.7893e-08.\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2958:\n",
      "train loss: 1.0626114313329697e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2959:\n",
      "train loss: 9.730782762868946e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2960:\n",
      "train loss: 6.42917984916331e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2961:\n",
      "train loss: 5.578696081190531e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2962:\n",
      "train loss: 1.058105549670074e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2963:\n",
      "train loss: 9.73051055874105e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2964:\n",
      "train loss: 6.429168778199549e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2965:\n",
      "train loss: 5.5786972159125316e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2966:\n",
      "train loss: 1.0580775826880101e-07\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2967:\n",
      "train loss: 9.730244496052237e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2968:\n",
      "train loss: 6.429159103156185e-08\n",
      "lr: 1.78932885101413e-08\n",
      "Epoch 2969:\n",
      "train loss: 5.578705097461766e-08\n",
      "Epoch 02971: reducing learning rate of group 0 to 1.6999e-08.\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2970:\n",
      "train loss: 1.0580485424098827e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2971:\n",
      "train loss: 9.729962536013131e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2972:\n",
      "train loss: 5.621215697412863e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2973:\n",
      "train loss: 4.813310517590826e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2974:\n",
      "train loss: 1.0537637247050477e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2975:\n",
      "train loss: 9.729633540856499e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2976:\n",
      "train loss: 5.621304203981956e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2977:\n",
      "train loss: 4.8134377422885524e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2978:\n",
      "train loss: 1.0537214383554665e-07\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2979:\n",
      "train loss: 9.729191003419321e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2980:\n",
      "train loss: 5.6215206317988964e-08\n",
      "lr: 1.6998624084634237e-08\n",
      "Epoch 2981:\n",
      "train loss: 4.8137101973968794e-08\n",
      "Epoch 02983: reducing learning rate of group 0 to 1.6149e-08.\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2982:\n",
      "train loss: 1.0536621357671618e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2983:\n",
      "train loss: 9.728552950556011e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2984:\n",
      "train loss: 4.854437842705953e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2985:\n",
      "train loss: 4.087142820305753e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2986:\n",
      "train loss: 1.0495221155349734e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2987:\n",
      "train loss: 9.727384060771834e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2988:\n",
      "train loss: 4.8555435529736486e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2989:\n",
      "train loss: 4.0885374559097224e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2990:\n",
      "train loss: 1.0493117018737274e-07\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2991:\n",
      "train loss: 9.724875459855076e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2992:\n",
      "train loss: 4.85822392310974e-08\n",
      "lr: 1.6148692880402524e-08\n",
      "Epoch 2993:\n",
      "train loss: 4.091900829600261e-08\n",
      "Epoch 02995: reducing learning rate of group 0 to 1.5341e-08.\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 2994:\n",
      "train loss: 1.0488345641448837e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 2995:\n",
      "train loss: 9.719069475735754e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 2996:\n",
      "train loss: 4.1357136907566555e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 2997:\n",
      "train loss: 3.4102919611923344e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 2998:\n",
      "train loss: 1.0435321092865772e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 2999:\n",
      "train loss: 9.699946387512548e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3000:\n",
      "train loss: 4.15910629551615e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3001:\n",
      "train loss: 3.442994333717916e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3002:\n",
      "train loss: 1.038372818460954e-07\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3003:\n",
      "train loss: 9.632553455059246e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3004:\n",
      "train loss: 4.241938361757113e-08\n",
      "lr: 1.5341258236382396e-08\n",
      "Epoch 3005:\n",
      "train loss: 3.5579499613413016e-08\n",
      "Epoch 03007: reducing learning rate of group 0 to 1.4574e-08.\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3006:\n",
      "train loss: 1.0205182160037983e-07\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3007:\n",
      "train loss: 9.399600404079338e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3008:\n",
      "train loss: 3.834919624442712e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3009:\n",
      "train loss: 3.3180248366697864e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3010:\n",
      "train loss: 9.48308440884583e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3011:\n",
      "train loss: 8.487229977469842e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3012:\n",
      "train loss: 4.9509686260145104e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3013:\n",
      "train loss: 4.637661092131567e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3014:\n",
      "train loss: 7.961973950595167e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3015:\n",
      "train loss: 6.753734804022237e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3016:\n",
      "train loss: 6.845625318817692e-08\n",
      "lr: 1.4574195324563275e-08\n",
      "Epoch 3017:\n",
      "train loss: 6.545562845787786e-08\n",
      "Epoch 03019: reducing learning rate of group 0 to 1.3845e-08.\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3018:\n",
      "train loss: 6.260084772133472e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3019:\n",
      "train loss: 5.2655655010534275e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3020:\n",
      "train loss: 7.384173595586268e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3021:\n",
      "train loss: 6.780426895560803e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3022:\n",
      "train loss: 5.726901416173598e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3023:\n",
      "train loss: 5.092384343497264e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3024:\n",
      "train loss: 7.30231255419298e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3025:\n",
      "train loss: 6.471569398123554e-08\n",
      "lr: 1.3845485558335111e-08\n",
      "Epoch 3026:\n",
      "train loss: 6.214936173615322e-08\n",
      "Epoch 03028: reducing learning rate of group 0 to 1.3153e-08.\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3027:\n",
      "train loss: 5.7023632397915e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3028:\n",
      "train loss: 6.66317139621261e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3029:\n",
      "train loss: 5.888658661091525e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3030:\n",
      "train loss: 6.131745943649296e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3031:\n",
      "train loss: 5.624211450465323e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3032:\n",
      "train loss: 6.146429773791379e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3033:\n",
      "train loss: 5.422326520933617e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3034:\n",
      "train loss: 6.539486149028093e-08\n",
      "lr: 1.3153211280418355e-08\n",
      "Epoch 3035:\n",
      "train loss: 5.992782643714343e-08\n",
      "Epoch 03037: reducing learning rate of group 0 to 1.2496e-08.\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3036:\n",
      "train loss: 5.8084289432490135e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3037:\n",
      "train loss: 5.12099025058766e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3038:\n",
      "train loss: 6.209512232003853e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3039:\n",
      "train loss: 5.663571234489368e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3040:\n",
      "train loss: 5.571639524091968e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3041:\n",
      "train loss: 4.938687918713002e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3042:\n",
      "train loss: 6.37314760285923e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3043:\n",
      "train loss: 5.8051362592859785e-08\n",
      "lr: 1.2495550716397437e-08\n",
      "Epoch 3044:\n",
      "train loss: 5.4543275877390865e-08\n",
      "Epoch 03046: reducing learning rate of group 0 to 1.1871e-08.\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3045:\n",
      "train loss: 4.8402361323764856e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3046:\n",
      "train loss: 6.454849194538676e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3047:\n",
      "train loss: 5.895499407090841e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3048:\n",
      "train loss: 4.82390240676141e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3049:\n",
      "train loss: 4.262333443677039e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3050:\n",
      "train loss: 6.446168924292703e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3051:\n",
      "train loss: 5.865978464349432e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3052:\n",
      "train loss: 4.8712903426588344e-08\n",
      "lr: 1.1870773180577564e-08\n",
      "Epoch 3053:\n",
      "train loss: 4.324443372760502e-08\n",
      "Epoch 03055: reducing learning rate of group 0 to 1.1277e-08.\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3054:\n",
      "train loss: 6.374712398089665e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3055:\n",
      "train loss: 5.788564114764434e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3056:\n",
      "train loss: 4.41562265211351e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3057:\n",
      "train loss: 3.900129058647e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3058:\n",
      "train loss: 6.261511560532593e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3059:\n",
      "train loss: 5.704292310408737e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3060:\n",
      "train loss: 4.498630630493208e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3061:\n",
      "train loss: 3.981173271019974e-08\n",
      "lr: 1.1277234521548685e-08\n",
      "Epoch 3062:\n",
      "train loss: 6.184180365074523e-08\n",
      "Epoch 03064: reducing learning rate of group 0 to 1.0713e-08.\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3063:\n",
      "train loss: 5.632032039736724e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3064:\n",
      "train loss: 4.565348788832574e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3065:\n",
      "train loss: 4.0689093095260403e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3066:\n",
      "train loss: 5.592991510011124e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3067:\n",
      "train loss: 5.073529980331994e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3068:\n",
      "train loss: 4.609034812578543e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3069:\n",
      "train loss: 4.108298689061466e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3070:\n",
      "train loss: 5.5573158311810994e-08\n",
      "lr: 1.071337279547125e-08\n",
      "Epoch 3071:\n",
      "train loss: 5.0413548018752104e-08\n",
      "Epoch 03073: reducing learning rate of group 0 to 1.0178e-08.\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3072:\n",
      "train loss: 4.6379777698846016e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3073:\n",
      "train loss: 4.1344173128211633e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3074:\n",
      "train loss: 5.050191740910243e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3075:\n",
      "train loss: 4.56206072354907e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3076:\n",
      "train loss: 4.6314143761458125e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3077:\n",
      "train loss: 4.151332943268521e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3078:\n",
      "train loss: 5.034753303859026e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3079:\n",
      "train loss: 4.5479815971169105e-08\n",
      "lr: 1.0177704155697687e-08\n",
      "Epoch 3080:\n",
      "train loss: 4.644197093719043e-08\n",
      "Epoch 03082: reducing learning rate of group 0 to 9.6688e-09.\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3081:\n",
      "train loss: 4.1630340161419795e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3082:\n",
      "train loss: 5.023951565448289e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3083:\n",
      "train loss: 4.562402534675408e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3084:\n",
      "train loss: 4.1692888879927596e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3085:\n",
      "train loss: 3.711572744612399e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3086:\n",
      "train loss: 5.016473126274163e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3087:\n",
      "train loss: 4.555475550326741e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3088:\n",
      "train loss: 4.1755444325078614e-08\n",
      "lr: 9.668818947912803e-09\n",
      "Epoch 3089:\n",
      "train loss: 3.717366819194478e-08\n",
      "Epoch 03091: reducing learning rate of group 0 to 9.1854e-09.\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3090:\n",
      "train loss: 5.011001802619116e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3091:\n",
      "train loss: 4.550487950983906e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3092:\n",
      "train loss: 3.743417647039597e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3093:\n",
      "train loss: 3.307777235585164e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3094:\n",
      "train loss: 4.984393047616224e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3095:\n",
      "train loss: 4.547247896567465e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3096:\n",
      "train loss: 3.746218073639565e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3097:\n",
      "train loss: 3.310303133231077e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3098:\n",
      "train loss: 4.981991733142586e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3099:\n",
      "train loss: 4.545080633724731e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3100:\n",
      "train loss: 3.748057389642057e-08\n",
      "lr: 9.185378000517162e-09\n",
      "Epoch 3101:\n",
      "train loss: 3.311968586980278e-08\n",
      "Epoch 03103: reducing learning rate of group 0 to 8.7261e-09.\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3102:\n",
      "train loss: 4.9803554621747386e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3103:\n",
      "train loss: 4.543582717306799e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3104:\n",
      "train loss: 3.334674659564726e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3105:\n",
      "train loss: 2.9203095172197994e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3106:\n",
      "train loss: 4.957341620511487e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3107:\n",
      "train loss: 4.5424623850245036e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3108:\n",
      "train loss: 3.335636212056615e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3109:\n",
      "train loss: 2.921243229131977e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3110:\n",
      "train loss: 4.9563178951738014e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3111:\n",
      "train loss: 4.541468341952436e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3112:\n",
      "train loss: 3.336491233143307e-08\n",
      "lr: 8.726109100491304e-09\n",
      "Epoch 3113:\n",
      "train loss: 2.922085263436916e-08\n",
      "Epoch 03115: reducing learning rate of group 0 to 8.2898e-09.\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3114:\n",
      "train loss: 4.9553777717475016e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3115:\n",
      "train loss: 4.540553421433837e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3116:\n",
      "train loss: 2.943381277255864e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3117:\n",
      "train loss: 2.5496974546540017e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3118:\n",
      "train loss: 4.9337726436440397e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3119:\n",
      "train loss: 4.539688813908946e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3120:\n",
      "train loss: 2.9441385683601056e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3121:\n",
      "train loss: 2.5504632859905043e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3122:\n",
      "train loss: 4.9328952393831703e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3123:\n",
      "train loss: 4.538818891871376e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3124:\n",
      "train loss: 2.9448946155757124e-08\n",
      "lr: 8.289803645466739e-09\n",
      "Epoch 3125:\n",
      "train loss: 2.551222816792946e-08\n",
      "Epoch 03127: reducing learning rate of group 0 to 7.8753e-09.\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3126:\n",
      "train loss: 4.932026595028223e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3127:\n",
      "train loss: 4.537958685252348e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3128:\n",
      "train loss: 2.5714644241741332e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3129:\n",
      "train loss: 2.197499622154961e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3130:\n",
      "train loss: 4.911438547431693e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3131:\n",
      "train loss: 4.537039627425349e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3132:\n",
      "train loss: 2.5723167701480822e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3133:\n",
      "train loss: 2.1984053536309433e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3134:\n",
      "train loss: 4.910357510876176e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3135:\n",
      "train loss: 4.535899198147885e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3136:\n",
      "train loss: 2.57341698593525e-08\n",
      "lr: 7.875313463193401e-09\n",
      "Epoch 3137:\n",
      "train loss: 2.199598228075732e-08\n",
      "Epoch 03139: reducing learning rate of group 0 to 7.4815e-09.\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3138:\n",
      "train loss: 4.908926929240682e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3139:\n",
      "train loss: 4.534348945330522e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3140:\n",
      "train loss: 2.2195324398013974e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3141:\n",
      "train loss: 1.8646741664199895e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3142:\n",
      "train loss: 4.887823304757875e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3143:\n",
      "train loss: 4.531532733924563e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3144:\n",
      "train loss: 2.2226847923724384e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3145:\n",
      "train loss: 1.8685141107875258e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3146:\n",
      "train loss: 4.882715324619545e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3147:\n",
      "train loss: 4.525392021890288e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3148:\n",
      "train loss: 2.2297541224532068e-08\n",
      "lr: 7.481547790033731e-09\n",
      "Epoch 3149:\n",
      "train loss: 1.877287534231998e-08\n",
      "Epoch 03151: reducing learning rate of group 0 to 7.1075e-09.\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3150:\n",
      "train loss: 4.87085794499424e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3151:\n",
      "train loss: 4.5108768468314514e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3152:\n",
      "train loss: 1.9091128084345456e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3153:\n",
      "train loss: 1.5808163192523082e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3154:\n",
      "train loss: 4.8161685471635813e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3155:\n",
      "train loss: 4.4626432213175234e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3156:\n",
      "train loss: 1.9684159640442426e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3157:\n",
      "train loss: 1.6615491565385715e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3158:\n",
      "train loss: 4.6944613766210705e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3159:\n",
      "train loss: 4.305927995690457e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3160:\n",
      "train loss: 2.1583963749897563e-08\n",
      "lr: 7.107470400532044e-09\n",
      "Epoch 3161:\n",
      "train loss: 1.902204467712043e-08\n",
      "Epoch 03163: reducing learning rate of group 0 to 6.7521e-09.\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3162:\n",
      "train loss: 4.371953152119393e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3163:\n",
      "train loss: 3.9086463460315716e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3164:\n",
      "train loss: 2.3008215860339665e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3165:\n",
      "train loss: 2.1345679830651894e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3166:\n",
      "train loss: 3.7344843783913654e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3167:\n",
      "train loss: 3.201409739636794e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3168:\n",
      "train loss: 3.080752433956782e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3169:\n",
      "train loss: 2.929731078688906e-08\n",
      "lr: 6.752096880505442e-09\n",
      "Epoch 3170:\n",
      "train loss: 3.005288106785687e-08\n",
      "Epoch 03172: reducing learning rate of group 0 to 6.4145e-09.\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3171:\n",
      "train loss: 2.5392247613591626e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3172:\n",
      "train loss: 3.651230213524923e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3173:\n",
      "train loss: 3.3990282427587465e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3174:\n",
      "train loss: 2.374146139254958e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3175:\n",
      "train loss: 2.0907225836655426e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3176:\n",
      "train loss: 3.611937817805599e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3177:\n",
      "train loss: 3.19757553209334e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3178:\n",
      "train loss: 2.7065442659728886e-08\n",
      "lr: 6.41449203648017e-09\n",
      "Epoch 3179:\n",
      "train loss: 2.5054572737006717e-08\n",
      "Epoch 03181: reducing learning rate of group 0 to 6.0938e-09.\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3180:\n",
      "train loss: 3.179847950826263e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3181:\n",
      "train loss: 2.7653007148729392e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3182:\n",
      "train loss: 2.8327996044346353e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3183:\n",
      "train loss: 2.624011056875206e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3184:\n",
      "train loss: 2.8053140265891654e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3185:\n",
      "train loss: 2.4471199218862453e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3186:\n",
      "train loss: 3.113661112731584e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3187:\n",
      "train loss: 2.8742484111047028e-08\n",
      "lr: 6.093767434656161e-09\n",
      "Epoch 3188:\n",
      "train loss: 2.5822119070230128e-08\n",
      "Epoch 03190: reducing learning rate of group 0 to 5.7891e-09.\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3189:\n",
      "train loss: 2.2537724783517723e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3190:\n",
      "train loss: 3.2802582647972785e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3191:\n",
      "train loss: 3.034035148040516e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3192:\n",
      "train loss: 2.164570079655122e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3193:\n",
      "train loss: 1.8665026762577294e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3194:\n",
      "train loss: 3.3784583165950844e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3195:\n",
      "train loss: 3.122011872053975e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3196:\n",
      "train loss: 2.0861440661523913e-08\n",
      "lr: 5.789079062923353e-09\n",
      "Epoch 3197:\n",
      "train loss: 1.7973320085286087e-08\n",
      "Epoch 03199: reducing learning rate of group 0 to 5.4996e-09.\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3198:\n",
      "train loss: 3.436837413428665e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3199:\n",
      "train loss: 3.1697249792682985e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3200:\n",
      "train loss: 1.7887934278927033e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3201:\n",
      "train loss: 1.528526780261987e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3202:\n",
      "train loss: 3.422917871723429e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3203:\n",
      "train loss: 3.1491801803782314e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3204:\n",
      "train loss: 1.8287616637290783e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3205:\n",
      "train loss: 1.5903846791349658e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3206:\n",
      "train loss: 3.3335913616807444e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3207:\n",
      "train loss: 3.033552446267882e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3208:\n",
      "train loss: 1.969317782439215e-08\n",
      "lr: 5.499625109777185e-09\n",
      "Epoch 3209:\n",
      "train loss: 1.7539363726733145e-08\n",
      "Epoch 03211: reducing learning rate of group 0 to 5.2246e-09.\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3210:\n",
      "train loss: 3.148163977398594e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3211:\n",
      "train loss: 2.8257509866251072e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3212:\n",
      "train loss: 1.947969668704324e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3213:\n",
      "train loss: 1.7619841265069803e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3214:\n",
      "train loss: 2.8786410387418476e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3215:\n",
      "train loss: 2.5549732785766466e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3216:\n",
      "train loss: 2.2327107572636402e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3217:\n",
      "train loss: 2.0496492528687865e-08\n",
      "lr: 5.224643854288325e-09\n",
      "Epoch 3218:\n",
      "train loss: 2.6024428823379118e-08\n",
      "Epoch 03220: reducing learning rate of group 0 to 4.9634e-09.\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3219:\n",
      "train loss: 2.2909289020313584e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3220:\n",
      "train loss: 2.482717873820012e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3221:\n",
      "train loss: 2.292180016033844e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3222:\n",
      "train loss: 2.1495611757549622e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3223:\n",
      "train loss: 1.8785380839836464e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3224:\n",
      "train loss: 2.6308381165712406e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3225:\n",
      "train loss: 2.418378357128921e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3226:\n",
      "train loss: 2.0419014436049974e-08\n",
      "lr: 4.963411661573909e-09\n",
      "Epoch 3227:\n",
      "train loss: 1.7887985446285142e-08\n",
      "Epoch 03229: reducing learning rate of group 0 to 4.7152e-09.\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3228:\n",
      "train loss: 2.7056004897045543e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3229:\n",
      "train loss: 2.4820596843671045e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3230:\n",
      "train loss: 1.7635052206016543e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3231:\n",
      "train loss: 1.5306541674325343e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3232:\n",
      "train loss: 2.7328364013969885e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3233:\n",
      "train loss: 2.515779777665799e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3234:\n",
      "train loss: 1.7336498971941277e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3235:\n",
      "train loss: 1.5040761699238495e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3236:\n",
      "train loss: 2.7564677532422426e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3237:\n",
      "train loss: 2.5368603271927314e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3238:\n",
      "train loss: 1.7149938368973396e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3239:\n",
      "train loss: 1.487852138916857e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3240:\n",
      "train loss: 2.769866429705585e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3241:\n",
      "train loss: 2.5474863400143715e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3242:\n",
      "train loss: 1.7071069618095774e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3243:\n",
      "train loss: 1.4828026842992892e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3244:\n",
      "train loss: 2.771658592784739e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3245:\n",
      "train loss: 2.5460878358658356e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3246:\n",
      "train loss: 1.7115744482679085e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3247:\n",
      "train loss: 1.4903248202102102e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3248:\n",
      "train loss: 2.7608909087439645e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3249:\n",
      "train loss: 2.532206432681962e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3250:\n",
      "train loss: 1.7283666319641598e-08\n",
      "lr: 4.715241078495213e-09\n",
      "Epoch 3251:\n",
      "train loss: 1.5098585955358424e-08\n",
      "Epoch 03253: reducing learning rate of group 0 to 4.4795e-09.\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3252:\n",
      "train loss: 2.7387067971770838e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3253:\n",
      "train loss: 2.507531990117473e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3254:\n",
      "train loss: 1.5423487061167344e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3255:\n",
      "train loss: 1.3377235767603869e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3256:\n",
      "train loss: 2.6944698572607557e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3257:\n",
      "train loss: 2.471204890165086e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3258:\n",
      "train loss: 1.581956184827409e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3259:\n",
      "train loss: 1.3802874723782883e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3260:\n",
      "train loss: 2.649159671838381e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3261:\n",
      "train loss: 2.4233813169026788e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3262:\n",
      "train loss: 1.632027550085538e-08\n",
      "lr: 4.479479024570452e-09\n",
      "Epoch 3263:\n",
      "train loss: 1.4322481715918941e-08\n",
      "Epoch 03265: reducing learning rate of group 0 to 4.2555e-09.\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3264:\n",
      "train loss: 2.595670929736548e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3265:\n",
      "train loss: 2.368444247116423e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3266:\n",
      "train loss: 1.4856479339342657e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3267:\n",
      "train loss: 1.2980199923891231e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3268:\n",
      "train loss: 2.5252784717057613e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3269:\n",
      "train loss: 2.3063707060253765e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3270:\n",
      "train loss: 1.5503915923635037e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3271:\n",
      "train loss: 1.3647560914932431e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3272:\n",
      "train loss: 2.4573020629561494e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3273:\n",
      "train loss: 2.237257472054244e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3274:\n",
      "train loss: 1.6204054676258057e-08\n",
      "lr: 4.255505073341929e-09\n",
      "Epoch 3275:\n",
      "train loss: 1.4350143250376681e-08\n",
      "Epoch 03277: reducing learning rate of group 0 to 4.0427e-09.\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3276:\n",
      "train loss: 2.38770083034298e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3277:\n",
      "train loss: 2.1682938595289405e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3278:\n",
      "train loss: 1.496003747824722e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3279:\n",
      "train loss: 1.3197476399306332e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3280:\n",
      "train loss: 2.3116453713218582e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3281:\n",
      "train loss: 2.1030478042236465e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3282:\n",
      "train loss: 1.561154268769726e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3283:\n",
      "train loss: 1.3841603805019125e-08\n",
      "lr: 4.042729819674833e-09\n",
      "Epoch 3284:\n",
      "train loss: 2.2489494327897137e-08\n",
      "Epoch 03286: reducing learning rate of group 0 to 3.8406e-09.\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3285:\n",
      "train loss: 2.0421819510765858e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3286:\n",
      "train loss: 1.6200780244173533e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3287:\n",
      "train loss: 1.4496933332622036e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3288:\n",
      "train loss: 2.0048376031912998e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3289:\n",
      "train loss: 1.8115168163876594e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3290:\n",
      "train loss: 1.664492273157894e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3291:\n",
      "train loss: 1.4910817558526496e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3292:\n",
      "train loss: 1.9665274788789363e-08\n",
      "lr: 3.840593328691091e-09\n",
      "Epoch 3293:\n",
      "train loss: 1.7763513545199764e-08\n",
      "Epoch 03295: reducing learning rate of group 0 to 3.6486e-09.\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3294:\n",
      "train loss: 1.696614816615168e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3295:\n",
      "train loss: 1.520532943823122e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3296:\n",
      "train loss: 1.766590277814392e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3297:\n",
      "train loss: 1.58833678151948e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3298:\n",
      "train loss: 1.708670981107773e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3299:\n",
      "train loss: 1.5393914391977282e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3300:\n",
      "train loss: 1.7495022272532288e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3301:\n",
      "train loss: 1.572962103176049e-08\n",
      "lr: 3.648563662256536e-09\n",
      "Epoch 3302:\n",
      "train loss: 1.722439572431665e-08\n",
      "Epoch 03304: reducing learning rate of group 0 to 3.4661e-09.\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3303:\n",
      "train loss: 1.5517861524001305e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3304:\n",
      "train loss: 1.7382944823953562e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3305:\n",
      "train loss: 1.5716396833931207e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3306:\n",
      "train loss: 1.5580051584000737e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3307:\n",
      "train loss: 1.3950254168502728e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3308:\n",
      "train loss: 1.7312823144139137e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3309:\n",
      "train loss: 1.5652991405485248e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3310:\n",
      "train loss: 1.56371804540276e-08\n",
      "lr: 3.466135479143709e-09\n",
      "Epoch 3311:\n",
      "train loss: 1.4001948008281848e-08\n",
      "Epoch 03313: reducing learning rate of group 0 to 3.2928e-09.\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3312:\n",
      "train loss: 1.726570003313638e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3313:\n",
      "train loss: 1.5610156994916395e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3314:\n",
      "train loss: 1.4111662626419873e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3315:\n",
      "train loss: 1.2555112060305503e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3316:\n",
      "train loss: 1.7151457417068994e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3317:\n",
      "train loss: 1.5580987179579288e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3318:\n",
      "train loss: 1.4138469267756041e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3319:\n",
      "train loss: 1.2580104037174068e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3320:\n",
      "train loss: 1.7127870996127923e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3321:\n",
      "train loss: 1.5559087583973705e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3322:\n",
      "train loss: 1.4158454346433561e-08\n",
      "lr: 3.2928287051865235e-09\n",
      "Epoch 3323:\n",
      "train loss: 1.2598712164499051e-08\n",
      "Epoch 03325: reducing learning rate of group 0 to 3.1282e-09.\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3324:\n",
      "train loss: 1.7110258127380526e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3325:\n",
      "train loss: 1.5542833737281745e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3326:\n",
      "train loss: 1.268730167095731e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3327:\n",
      "train loss: 1.1204564963169415e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3328:\n",
      "train loss: 1.7019510666854728e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3329:\n",
      "train loss: 1.5531356927439726e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3330:\n",
      "train loss: 1.2697583537582046e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3331:\n",
      "train loss: 1.1214087794458152e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3332:\n",
      "train loss: 1.7010400590092424e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3333:\n",
      "train loss: 1.5523018156232542e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3334:\n",
      "train loss: 1.2704857944683578e-08\n",
      "lr: 3.128187269927197e-09\n",
      "Epoch 3335:\n",
      "train loss: 1.1220734537813554e-08\n",
      "Epoch 03337: reducing learning rate of group 0 to 2.9718e-09.\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3336:\n",
      "train loss: 1.700401375587786e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3337:\n",
      "train loss: 1.5517230280385362e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3338:\n",
      "train loss: 1.1298408601573608e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3339:\n",
      "train loss: 9.888050251217141e-09\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3340:\n",
      "train loss: 1.6925545776598527e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3341:\n",
      "train loss: 1.551350639662164e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3342:\n",
      "train loss: 1.1301447402260013e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3343:\n",
      "train loss: 9.890787613785905e-09\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3344:\n",
      "train loss: 1.6922764445909923e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3345:\n",
      "train loss: 1.5511003167533353e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3346:\n",
      "train loss: 1.1303382443055356e-08\n",
      "lr: 2.971777906430837e-09\n",
      "Epoch 3347:\n",
      "train loss: 9.892528839056534e-09\n",
      "Epoch 03349: reducing learning rate of group 0 to 2.8232e-09.\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3348:\n",
      "train loss: 1.6920879052819414e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3349:\n",
      "train loss: 1.550930410285359e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3350:\n",
      "train loss: 9.96390414725819e-09\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3351:\n",
      "train loss: 8.623484111892167e-09\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3352:\n",
      "train loss: 1.6849050104244213e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3353:\n",
      "train loss: 1.5508168124804938e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3354:\n",
      "train loss: 9.96464124566689e-09\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3355:\n",
      "train loss: 8.624160745948733e-09\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3356:\n",
      "train loss: 1.6848130445754376e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3357:\n",
      "train loss: 1.550732428067261e-08\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3358:\n",
      "train loss: 9.96511989460841e-09\n",
      "lr: 2.823189011109295e-09\n",
      "Epoch 3359:\n",
      "train loss: 8.624607745297016e-09\n",
      "Epoch 03361: reducing learning rate of group 0 to 2.6820e-09.\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3360:\n",
      "train loss: 1.684741423573809e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3361:\n",
      "train loss: 1.550666288565633e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3362:\n",
      "train loss: 8.69183559517294e-09\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3363:\n",
      "train loss: 7.418342084296585e-09\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3364:\n",
      "train loss: 1.6779798744319605e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3365:\n",
      "train loss: 1.550611086161143e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3366:\n",
      "train loss: 8.692085915244135e-09\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3367:\n",
      "train loss: 7.418602313644408e-09\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3368:\n",
      "train loss: 1.6779245660913775e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3369:\n",
      "train loss: 1.550557090908573e-08\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3370:\n",
      "train loss: 8.692337803059746e-09\n",
      "lr: 2.6820295605538303e-09\n",
      "Epoch 3371:\n",
      "train loss: 7.418878384706864e-09\n",
      "Epoch 03373: reducing learning rate of group 0 to 2.5479e-09.\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3372:\n",
      "train loss: 1.6778660254386303e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3373:\n",
      "train loss: 1.5504981606764802e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3374:\n",
      "train loss: 7.482778917889872e-09\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3375:\n",
      "train loss: 6.2730614120407296e-09\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3376:\n",
      "train loss: 1.6714217150500863e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3377:\n",
      "train loss: 1.5504141082834965e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3378:\n",
      "train loss: 7.483439618446337e-09\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3379:\n",
      "train loss: 6.2738868857944e-09\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3380:\n",
      "train loss: 1.6712860901331962e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3381:\n",
      "train loss: 1.550257196749607e-08\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3382:\n",
      "train loss: 7.48496128212632e-09\n",
      "lr: 2.5479280825261385e-09\n",
      "Epoch 3383:\n",
      "train loss: 6.275805058310054e-09\n",
      "Epoch 03385: reducing learning rate of group 0 to 2.4205e-09.\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3384:\n",
      "train loss: 1.6709989334901113e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3385:\n",
      "train loss: 1.5499115370087686e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3386:\n",
      "train loss: 6.3393813956402235e-09\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3387:\n",
      "train loss: 5.192270169011302e-09\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3388:\n",
      "train loss: 1.6640622194397888e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3389:\n",
      "train loss: 1.5487522052834546e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3390:\n",
      "train loss: 6.353377137458129e-09\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3391:\n",
      "train loss: 5.212294971807817e-09\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3392:\n",
      "train loss: 1.6607957659250575e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3393:\n",
      "train loss: 1.544468417053646e-08\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3394:\n",
      "train loss: 6.405744853392387e-09\n",
      "lr: 2.4205316783998313e-09\n",
      "Epoch 3395:\n",
      "train loss: 5.287713057688014e-09\n",
      "Epoch 03397: reducing learning rate of group 0 to 2.2995e-09.\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3396:\n",
      "train loss: 1.6484497749923588e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3397:\n",
      "train loss: 1.5281792405892763e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3398:\n",
      "train loss: 5.514284462741231e-09\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3399:\n",
      "train loss: 4.579988183368264e-09\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3400:\n",
      "train loss: 1.581137963534689e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3401:\n",
      "train loss: 1.4437390457294236e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3402:\n",
      "train loss: 6.563215875094651e-09\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3403:\n",
      "train loss: 5.981189962191211e-09\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3404:\n",
      "train loss: 1.3815859634317117e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3405:\n",
      "train loss: 1.1929244847552356e-08\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3406:\n",
      "train loss: 9.477224598650832e-09\n",
      "lr: 2.2995050944798397e-09\n",
      "Epoch 3407:\n",
      "train loss: 9.121972636501505e-09\n",
      "Epoch 03409: reducing learning rate of group 0 to 2.1845e-09.\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3408:\n",
      "train loss: 1.0756102070950802e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3409:\n",
      "train loss: 8.912583048115054e-09\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3410:\n",
      "train loss: 1.1252962580395501e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3411:\n",
      "train loss: 1.0553911048148615e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3412:\n",
      "train loss: 8.920843988855109e-09\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3413:\n",
      "train loss: 7.788902706561261e-09\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3414:\n",
      "train loss: 1.1662089297383877e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3415:\n",
      "train loss: 1.0221220627615441e-08\n",
      "lr: 2.1845298397558475e-09\n",
      "Epoch 3416:\n",
      "train loss: 9.903921388880466e-09\n",
      "Epoch 03418: reducing learning rate of group 0 to 2.0753e-09.\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3417:\n",
      "train loss: 9.189365018637902e-09\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3418:\n",
      "train loss: 1.0204481945483197e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3419:\n",
      "train loss: 8.840535018427318e-09\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3420:\n",
      "train loss: 1.0226155312317932e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3421:\n",
      "train loss: 9.447951771032237e-09\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3422:\n",
      "train loss: 9.13309019381991e-09\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3423:\n",
      "train loss: 7.993677435990271e-09\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3424:\n",
      "train loss: 1.0834951469334148e-08\n",
      "lr: 2.0753033477680553e-09\n",
      "Epoch 3425:\n",
      "train loss: 9.88916445735218e-09\n",
      "Epoch 03427: reducing learning rate of group 0 to 1.9715e-09.\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3426:\n",
      "train loss: 8.817036562771088e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3427:\n",
      "train loss: 7.818454670888623e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3428:\n",
      "train loss: 9.969417850051121e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3429:\n",
      "train loss: 9.028589519763217e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3430:\n",
      "train loss: 8.758552367195038e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3431:\n",
      "train loss: 7.8286951298779e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3432:\n",
      "train loss: 9.94538375877685e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3433:\n",
      "train loss: 9.00475712031486e-09\n",
      "lr: 1.9715381803796525e-09\n",
      "Epoch 3434:\n",
      "train loss: 8.779433579791139e-09\n",
      "Epoch 03436: reducing learning rate of group 0 to 1.8730e-09.\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3435:\n",
      "train loss: 7.84669902075526e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3436:\n",
      "train loss: 9.928602118891892e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3437:\n",
      "train loss: 9.029321421126098e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3438:\n",
      "train loss: 7.878835015940178e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3439:\n",
      "train loss: 7.001737833114613e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3440:\n",
      "train loss: 9.872708610721015e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3441:\n",
      "train loss: 8.952982172215113e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3442:\n",
      "train loss: 7.978574626390089e-09\n",
      "lr: 1.8729612713606697e-09\n",
      "Epoch 3443:\n",
      "train loss: 7.118645098551301e-09\n",
      "Epoch 03445: reducing learning rate of group 0 to 1.7793e-09.\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3444:\n",
      "train loss: 9.74429743638462e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3445:\n",
      "train loss: 8.811308172711357e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3446:\n",
      "train loss: 7.287575578894431e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3447:\n",
      "train loss: 6.484408979048178e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3448:\n",
      "train loss: 9.522868310247004e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3449:\n",
      "train loss: 8.625606622208788e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3450:\n",
      "train loss: 7.481185445533722e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3451:\n",
      "train loss: 6.682737682603766e-09\n",
      "lr: 1.779313207792636e-09\n",
      "Epoch 3452:\n",
      "train loss: 9.326518592639742e-09\n",
      "Epoch 03454: reducing learning rate of group 0 to 1.6903e-09.\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3453:\n",
      "train loss: 8.435505077364235e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3454:\n",
      "train loss: 7.662811188493446e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3455:\n",
      "train loss: 6.8963799568433145e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3456:\n",
      "train loss: 8.322710537682587e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3457:\n",
      "train loss: 7.489119129727136e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3458:\n",
      "train loss: 7.79072843813231e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3459:\n",
      "train loss: 7.013007554861189e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3460:\n",
      "train loss: 8.216106004879467e-09\n",
      "lr: 1.6903475474030041e-09\n",
      "Epoch 3461:\n",
      "train loss: 7.393085264788007e-09\n",
      "Epoch 03463: reducing learning rate of group 0 to 1.6058e-09.\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3462:\n",
      "train loss: 7.87664740221166e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3463:\n",
      "train loss: 7.090406388441169e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3464:\n",
      "train loss: 7.384301879751957e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3465:\n",
      "train loss: 6.609004389082734e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3466:\n",
      "train loss: 7.891146135421602e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3467:\n",
      "train loss: 7.138612025081119e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3468:\n",
      "train loss: 7.341205679271027e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3469:\n",
      "train loss: 6.570346140102753e-09\n",
      "lr: 1.605830170032854e-09\n",
      "Epoch 3470:\n",
      "train loss: 7.92576792660099e-09\n",
      "Epoch 03472: reducing learning rate of group 0 to 1.5255e-09.\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3471:\n",
      "train loss: 7.169347046503577e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3472:\n",
      "train loss: 7.314065526129713e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3473:\n",
      "train loss: 6.584667271582144e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3474:\n",
      "train loss: 7.184207455018259e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3475:\n",
      "train loss: 6.463485215714755e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3476:\n",
      "train loss: 7.297493545078864e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3477:\n",
      "train loss: 6.56973804325834e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3478:\n",
      "train loss: 7.197548319195004e-09\n",
      "lr: 1.5255386615312111e-09\n",
      "Epoch 3479:\n",
      "train loss: 6.475633831816248e-09\n",
      "Epoch 03481: reducing learning rate of group 0 to 1.4493e-09.\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3480:\n",
      "train loss: 7.286288851909576e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3481:\n",
      "train loss: 6.5596868517976164e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3482:\n",
      "train loss: 6.5180945801624665e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3483:\n",
      "train loss: 5.831556026829869e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3484:\n",
      "train loss: 7.242718358369158e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3485:\n",
      "train loss: 6.553105680329492e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3486:\n",
      "train loss: 6.523877387761777e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3487:\n",
      "train loss: 5.8368387376868536e-09\n",
      "lr: 1.4492617284546505e-09\n",
      "Epoch 3488:\n",
      "train loss: 7.237728916852382e-09\n",
      "Epoch 03490: reducing learning rate of group 0 to 1.3768e-09.\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3489:\n",
      "train loss: 6.548605097911184e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3490:\n",
      "train loss: 6.527755236192014e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3491:\n",
      "train loss: 5.874699738068033e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3492:\n",
      "train loss: 6.54631392573345e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3493:\n",
      "train loss: 5.891953829797641e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3494:\n",
      "train loss: 6.530162670479956e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3495:\n",
      "train loss: 5.876858516578623e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3496:\n",
      "train loss: 6.544235426324193e-09\n",
      "lr: 1.3767986420319179e-09\n",
      "Epoch 3497:\n",
      "train loss: 5.89007627858728e-09\n",
      "Epoch 03499: reducing learning rate of group 0 to 1.3080e-09.\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3498:\n",
      "train loss: 6.531719125352221e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3499:\n",
      "train loss: 5.8782566649185445e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3500:\n",
      "train loss: 5.921794519135388e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3501:\n",
      "train loss: 5.300488119751912e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3502:\n",
      "train loss: 6.4999413080539125e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3503:\n",
      "train loss: 5.879026583718152e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3504:\n",
      "train loss: 5.92100804670716e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3505:\n",
      "train loss: 5.299823775738658e-09\n",
      "lr: 1.307958709930322e-09\n",
      "Epoch 3506:\n",
      "train loss: 6.500365727065491e-09\n",
      "Epoch 03508: reducing learning rate of group 0 to 1.2426e-09.\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3507:\n",
      "train loss: 5.879378520796952e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3508:\n",
      "train loss: 5.920582669969005e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3509:\n",
      "train loss: 5.3305261129711865e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3510:\n",
      "train loss: 5.879472986012539e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3511:\n",
      "train loss: 5.2895144981514396e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3512:\n",
      "train loss: 5.9203295500857e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3513:\n",
      "train loss: 5.330301080105533e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3514:\n",
      "train loss: 5.879541797587564e-09\n",
      "lr: 1.2425607744338058e-09\n",
      "Epoch 3515:\n",
      "train loss: 5.289579866455383e-09\n",
      "Epoch 03517: reducing learning rate of group 0 to 1.1804e-09.\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3516:\n",
      "train loss: 5.920136968304819e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3517:\n",
      "train loss: 5.330129528797554e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3518:\n",
      "train loss: 5.319079561757974e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3519:\n",
      "train loss: 4.758615472631826e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3520:\n",
      "train loss: 5.890493876404306e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3521:\n",
      "train loss: 5.330000360369007e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3522:\n",
      "train loss: 5.319073082884235e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3523:\n",
      "train loss: 4.758609821137786e-09\n",
      "lr: 1.1804327357121154e-09\n",
      "Epoch 3524:\n",
      "train loss: 5.890376677582979e-09\n",
      "Epoch 03526: reducing learning rate of group 0 to 1.1214e-09.\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3525:\n",
      "train loss: 5.3298953883437225e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3526:\n",
      "train loss: 5.319046749286941e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3527:\n",
      "train loss: 4.786608437776688e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3528:\n",
      "train loss: 5.329811144720314e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3529:\n",
      "train loss: 4.797363326508292e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3530:\n",
      "train loss: 5.319008562684427e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3531:\n",
      "train loss: 4.786573621154737e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3532:\n",
      "train loss: 5.329728646381168e-09\n",
      "lr: 1.1214110989265095e-09\n",
      "Epoch 3533:\n",
      "train loss: 4.797289248932124e-09\n",
      "Epoch 03535: reducing learning rate of group 0 to 1.0653e-09.\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3534:\n",
      "train loss: 5.318961456937406e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3535:\n",
      "train loss: 4.786531734456424e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3536:\n",
      "train loss: 4.8238436212995315e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3537:\n",
      "train loss: 4.318033654058032e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3538:\n",
      "train loss: 5.2922893455252365e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3539:\n",
      "train loss: 4.786486188428844e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3540:\n",
      "train loss: 4.823777949513394e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3541:\n",
      "train loss: 4.317975067845637e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3542:\n",
      "train loss: 5.292236013896004e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3543:\n",
      "train loss: 4.786436986380152e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3544:\n",
      "train loss: 4.823714895891666e-09\n",
      "lr: 1.065340543980184e-09\n",
      "Epoch 3545:\n",
      "train loss: 4.317919035909511e-09\n",
      "Epoch 03547: reducing learning rate of group 0 to 1.0121e-09.\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3546:\n",
      "train loss: 5.292179608713825e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3547:\n",
      "train loss: 4.78638633433747e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3548:\n",
      "train loss: 4.343152810332645e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3549:\n",
      "train loss: 3.86265210607784e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3550:\n",
      "train loss: 5.266835106437744e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3551:\n",
      "train loss: 4.786336626131145e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3552:\n",
      "train loss: 4.343099310306005e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3553:\n",
      "train loss: 3.862604492373652e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3554:\n",
      "train loss: 5.266776087616732e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3555:\n",
      "train loss: 4.786282700707841e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3556:\n",
      "train loss: 4.34304920320557e-09\n",
      "lr: 1.0120735167811747e-09\n",
      "Epoch 3557:\n",
      "train loss: 3.862561778233926e-09\n",
      "Epoch 03559: reducing learning rate of group 0 to 9.6147e-10.\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3558:\n",
      "train loss: 5.266714767859916e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3559:\n",
      "train loss: 4.786224665890544e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3560:\n",
      "train loss: 3.8865430608674925e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3561:\n",
      "train loss: 3.4300864471958095e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3562:\n",
      "train loss: 5.242625825852927e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3563:\n",
      "train loss: 4.786163008017356e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3564:\n",
      "train loss: 3.8865096591162595e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3565:\n",
      "train loss: 3.4300598645072383e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3566:\n",
      "train loss: 5.2425530162116125e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3567:\n",
      "train loss: 4.7860938940418255e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3568:\n",
      "train loss: 3.886483377646314e-09\n",
      "lr: 9.614698409421158e-10\n",
      "Epoch 3569:\n",
      "train loss: 3.430039840286792e-09\n",
      "Epoch 03571: reducing learning rate of group 0 to 9.1340e-10.\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3570:\n",
      "train loss: 5.242474383521245e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3571:\n",
      "train loss: 4.786018991629546e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3572:\n",
      "train loss: 3.4528391404872397e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3573:\n",
      "train loss: 3.0192252266482424e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3574:\n",
      "train loss: 5.2195681706349165e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3575:\n",
      "train loss: 4.785937086917199e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3576:\n",
      "train loss: 3.45283315751343e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3577:\n",
      "train loss: 3.0192274128925025e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3578:\n",
      "train loss: 5.219472269909023e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3579:\n",
      "train loss: 4.78584245083851e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3580:\n",
      "train loss: 3.4528400959003025e-09\n",
      "lr: 9.1339634889501e-10\n",
      "Epoch 3581:\n",
      "train loss: 3.01924163315683e-09\n",
      "Epoch 03583: reducing learning rate of group 0 to 8.6773e-10.\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3582:\n",
      "train loss: 5.21936384595906e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3583:\n",
      "train loss: 4.785735909429951e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3584:\n",
      "train loss: 3.040930063767717e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3585:\n",
      "train loss: 2.6290219829449373e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3586:\n",
      "train loss: 5.197558911613797e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3587:\n",
      "train loss: 4.7856085393537404e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3588:\n",
      "train loss: 3.040981019916801e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3589:\n",
      "train loss: 2.6290861944203484e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3590:\n",
      "train loss: 5.197397555824381e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3591:\n",
      "train loss: 4.785439956615759e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3592:\n",
      "train loss: 3.0410769679504755e-09\n",
      "lr: 8.677265314502595e-10\n",
      "Epoch 3593:\n",
      "train loss: 2.629199328099933e-09\n",
      "Epoch 03595: reducing learning rate of group 0 to 8.2434e-10.\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3594:\n",
      "train loss: 5.197182112297096e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3595:\n",
      "train loss: 4.785211705674987e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3596:\n",
      "train loss: 2.649915326824141e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3597:\n",
      "train loss: 2.2586667459724628e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3598:\n",
      "train loss: 5.17626353503127e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3599:\n",
      "train loss: 4.784847419547521e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3600:\n",
      "train loss: 2.6502461094129237e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3601:\n",
      "train loss: 2.2590627813151876e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3602:\n",
      "train loss: 5.175694513463039e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3603:\n",
      "train loss: 4.78419963903225e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3604:\n",
      "train loss: 2.650896449180725e-09\n",
      "lr: 8.243402048777465e-10\n",
      "Epoch 3605:\n",
      "train loss: 2.25983313574889e-09\n",
      "Epoch 03607: reducing learning rate of group 0 to 7.8312e-10.\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3606:\n",
      "train loss: 5.174664332015339e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3607:\n",
      "train loss: 4.783007271858563e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3608:\n",
      "train loss: 2.280422253017627e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3609:\n",
      "train loss: 1.909259687336708e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3610:\n",
      "train loss: 5.152776519559174e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3611:\n",
      "train loss: 4.780133695029382e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3612:\n",
      "train loss: 2.283764876124094e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3613:\n",
      "train loss: 1.913528661077871e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3614:\n",
      "train loss: 5.146794280083781e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3615:\n",
      "train loss: 4.772740961808956e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3616:\n",
      "train loss: 2.292441011743828e-09\n",
      "lr: 7.831231946338591e-10\n",
      "Epoch 3617:\n",
      "train loss: 1.924631317929456e-09\n",
      "Epoch 03619: reducing learning rate of group 0 to 7.4397e-10.\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3618:\n",
      "train loss: 5.131225773149666e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3619:\n",
      "train loss: 4.7533691278389974e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3620:\n",
      "train loss: 1.962216039002197e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3621:\n",
      "train loss: 1.622546552321228e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3622:\n",
      "train loss: 5.059854431635126e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3623:\n",
      "train loss: 4.68383840394425e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3624:\n",
      "train loss: 2.0475733807111714e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3625:\n",
      "train loss: 1.7390125248052996e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3626:\n",
      "train loss: 4.884516802098274e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3627:\n",
      "train loss: 4.459058673737006e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3628:\n",
      "train loss: 2.317269564721653e-09\n",
      "lr: 7.439670349021661e-10\n",
      "Epoch 3629:\n",
      "train loss: 2.072489851913516e-09\n",
      "Epoch 03631: reducing learning rate of group 0 to 7.0677e-10.\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3630:\n",
      "train loss: 4.4550127120106955e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3631:\n",
      "train loss: 3.942088717449468e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3632:\n",
      "train loss: 2.571399053403996e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3633:\n",
      "train loss: 2.410912017794255e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3634:\n",
      "train loss: 3.7239792784555763e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3635:\n",
      "train loss: 3.1662844531833158e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3636:\n",
      "train loss: 3.3928036233017622e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3637:\n",
      "train loss: 3.2202315677695285e-09\n",
      "lr: 7.067686831570578e-10\n",
      "Epoch 3638:\n",
      "train loss: 3.0074782766183246e-09\n",
      "Epoch 03640: reducing learning rate of group 0 to 6.7143e-10.\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3639:\n",
      "train loss: 2.5537124396108177e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3640:\n",
      "train loss: 3.866603107775425e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3641:\n",
      "train loss: 3.5543434329161423e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3642:\n",
      "train loss: 2.5286082218858e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3643:\n",
      "train loss: 2.2779919786350643e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3644:\n",
      "train loss: 3.6357813160335822e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3645:\n",
      "train loss: 3.154494913456089e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3646:\n",
      "train loss: 3.0574146874725266e-09\n",
      "lr: 6.714302489992048e-10\n",
      "Epoch 3647:\n",
      "train loss: 2.8701280035499077e-09\n",
      "Epoch 03649: reducing learning rate of group 0 to 6.3786e-10.\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3648:\n",
      "train loss: 3.0660157749024188e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3649:\n",
      "train loss: 2.628651624826497e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3650:\n",
      "train loss: 3.2171891788367746e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3651:\n",
      "train loss: 2.9800625636096416e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3652:\n",
      "train loss: 2.7229763498624995e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3653:\n",
      "train loss: 2.374274367687979e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3654:\n",
      "train loss: 3.4097470608911923e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3655:\n",
      "train loss: 3.123040519001693e-09\n",
      "lr: 6.378587365492445e-10\n",
      "Epoch 3656:\n",
      "train loss: 2.6210062441716514e-09\n",
      "Epoch 03658: reducing learning rate of group 0 to 6.0597e-10.\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3657:\n",
      "train loss: 2.31139853237363e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3658:\n",
      "train loss: 3.4432608052647533e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3659:\n",
      "train loss: 3.1528206183079993e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3660:\n",
      "train loss: 2.316772047454045e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3661:\n",
      "train loss: 2.0344634130864926e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3662:\n",
      "train loss: 3.4244301510474716e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3663:\n",
      "train loss: 3.130308407342883e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3664:\n",
      "train loss: 2.3407315418330044e-09\n",
      "lr: 6.059657997217823e-10\n",
      "Epoch 3665:\n",
      "train loss: 2.058579388672927e-09\n",
      "Epoch 03667: reducing learning rate of group 0 to 5.7567e-10.\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3666:\n",
      "train loss: 3.401470611708627e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3667:\n",
      "train loss: 3.1093707237379153e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3668:\n",
      "train loss: 2.086322353876149e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3669:\n",
      "train loss: 1.816870382693046e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3670:\n",
      "train loss: 3.3704636738328625e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3671:\n",
      "train loss: 3.0929198568176057e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3672:\n",
      "train loss: 2.103329623069726e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3673:\n",
      "train loss: 1.8350975338799593e-09\n",
      "lr: 5.756675097356932e-10\n",
      "Epoch 3674:\n",
      "train loss: 3.3495956706514784e-09\n",
      "Epoch 03676: reducing learning rate of group 0 to 5.4688e-10.\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3675:\n",
      "train loss: 3.069024918032442e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3676:\n",
      "train loss: 2.1304910326927347e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3677:\n",
      "train loss: 1.878621924615513e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3678:\n",
      "train loss: 3.0439681830203656e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3679:\n",
      "train loss: 2.7741306643829694e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3680:\n",
      "train loss: 2.168608495144555e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3681:\n",
      "train loss: 1.919198492961829e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3682:\n",
      "train loss: 3.001537632466533e-09\n",
      "lr: 5.468841342489085e-10\n",
      "Epoch 3683:\n",
      "train loss: 2.729827901630387e-09\n",
      "Epoch 03685: reducing learning rate of group 0 to 5.1954e-10.\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3684:\n",
      "train loss: 2.2145725169407848e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3685:\n",
      "train loss: 1.9662242804687313e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3686:\n",
      "train loss: 2.708518226275256e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3687:\n",
      "train loss: 2.4506280070950826e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3688:\n",
      "train loss: 2.2459851344735437e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3689:\n",
      "train loss: 2.008882081630669e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3690:\n",
      "train loss: 2.667815598227549e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3691:\n",
      "train loss: 2.4120618975245047e-09\n",
      "lr: 5.19539927536463e-10\n",
      "Epoch 3692:\n",
      "train loss: 2.282386345905577e-09\n",
      "Epoch 03694: reducing learning rate of group 0 to 4.9356e-10.\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3693:\n",
      "train loss: 2.0433015188161614e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3694:\n",
      "train loss: 2.635555651544488e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3695:\n",
      "train loss: 2.3948143753846937e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3696:\n",
      "train loss: 2.062782276420863e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3697:\n",
      "train loss: 1.8339433714481562e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3698:\n",
      "train loss: 2.6125269733843357e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3699:\n",
      "train loss: 2.37349646897973e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3700:\n",
      "train loss: 2.0823432956318646e-09\n",
      "lr: 4.935629311596398e-10\n",
      "Epoch 3701:\n",
      "train loss: 1.8519515229154237e-09\n",
      "Epoch 03703: reducing learning rate of group 0 to 4.6888e-10.\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3702:\n",
      "train loss: 2.5960026667579965e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3703:\n",
      "train loss: 2.358509585680094e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3704:\n",
      "train loss: 1.873123949844776e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3705:\n",
      "train loss: 1.6530254232467316e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3706:\n",
      "train loss: 2.5736124769967893e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3707:\n",
      "train loss: 2.3490353980244046e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3708:\n",
      "train loss: 1.8816185380527416e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3709:\n",
      "train loss: 1.6606790338511307e-09\n",
      "lr: 4.688847846016578e-10\n",
      "Epoch 3710:\n",
      "train loss: 2.5666532618799814e-09\n",
      "Epoch 03712: reducing learning rate of group 0 to 4.4544e-10.\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3711:\n",
      "train loss: 2.3427224205997283e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3712:\n",
      "train loss: 1.887332733022105e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3713:\n",
      "train loss: 1.6769613383845183e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3714:\n",
      "train loss: 2.3393945821597254e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3715:\n",
      "train loss: 2.127018021285802e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3716:\n",
      "train loss: 1.89118820724367e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3717:\n",
      "train loss: 1.6805292358431593e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3718:\n",
      "train loss: 2.3360522658477684e-09\n",
      "lr: 4.4544054537157487e-10\n",
      "Epoch 3719:\n",
      "train loss: 2.123902683397127e-09\n",
      "Epoch 03721: reducing learning rate of group 0 to 4.2317e-10.\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3720:\n",
      "train loss: 1.8940677464827478e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3721:\n",
      "train loss: 1.6832272409680068e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3722:\n",
      "train loss: 2.1326615165589176e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3723:\n",
      "train loss: 1.9312828846654645e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3724:\n",
      "train loss: 1.88560086896991e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3725:\n",
      "train loss: 1.6851561461851828e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3726:\n",
      "train loss: 2.1308464020511288e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3727:\n",
      "train loss: 1.9296130977549657e-09\n",
      "lr: 4.231685181029961e-10\n",
      "Epoch 3728:\n",
      "train loss: 1.8871004791433146e-09\n",
      "Epoch 03730: reducing learning rate of group 0 to 4.0201e-10.\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3729:\n",
      "train loss: 1.6865414345427196e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3730:\n",
      "train loss: 2.1295352410167373e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3731:\n",
      "train loss: 1.9384703534327143e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3732:\n",
      "train loss: 1.687277069201607e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3733:\n",
      "train loss: 1.496667994106148e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3734:\n",
      "train loss: 2.1286402769262128e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3735:\n",
      "train loss: 1.937649126799623e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3736:\n",
      "train loss: 1.6879914936998334e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3737:\n",
      "train loss: 1.4973224234462648e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3738:\n",
      "train loss: 2.1280098676130533e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3739:\n",
      "train loss: 1.9370780706867415e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3740:\n",
      "train loss: 1.6884726334550643e-09\n",
      "lr: 4.020100921978463e-10\n",
      "Epoch 3741:\n",
      "train loss: 1.4977571657078243e-09\n",
      "Epoch 03743: reducing learning rate of group 0 to 3.8191e-10.\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3742:\n",
      "train loss: 2.12758349534258e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3743:\n",
      "train loss: 1.9366934377786473e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3744:\n",
      "train loss: 1.5075116359659952e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3745:\n",
      "train loss: 1.3263010807784629e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3746:\n",
      "train loss: 2.1177649728678546e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3747:\n",
      "train loss: 1.9364451997180766e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3748:\n",
      "train loss: 1.507705087170724e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3749:\n",
      "train loss: 1.3264796063389663e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3750:\n",
      "train loss: 2.117567441471928e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3751:\n",
      "train loss: 1.9362623638213547e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3752:\n",
      "train loss: 1.50784321153425e-09\n",
      "lr: 3.8190958758795396e-10\n",
      "Epoch 3753:\n",
      "train loss: 1.3266102233479827e-09\n",
      "Epoch 03755: reducing learning rate of group 0 to 3.6281e-10.\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3754:\n",
      "train loss: 2.117411075636333e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3755:\n",
      "train loss: 1.936113210206512e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3756:\n",
      "train loss: 1.3357518583999083e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3757:\n",
      "train loss: 1.1635796113671132e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3758:\n",
      "train loss: 2.1082076027784597e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3759:\n",
      "train loss: 1.9359753195230522e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3760:\n",
      "train loss: 1.3358602173169848e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3761:\n",
      "train loss: 1.1636923635297795e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3762:\n",
      "train loss: 2.108060329418491e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3763:\n",
      "train loss: 1.9358262606791598e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3764:\n",
      "train loss: 1.3359812499763887e-09\n",
      "lr: 3.6281410820855625e-10\n",
      "Epoch 3765:\n",
      "train loss: 1.1638174221341842e-09\n",
      "Epoch 03767: reducing learning rate of group 0 to 3.4467e-10.\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3766:\n",
      "train loss: 2.107899789033369e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3767:\n",
      "train loss: 1.935663477926319e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3768:\n",
      "train loss: 1.172527678679799e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3769:\n",
      "train loss: 1.0089834076473797e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3770:\n",
      "train loss: 2.0991038585442433e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3771:\n",
      "train loss: 1.935466257172598e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3772:\n",
      "train loss: 1.172708765027109e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3773:\n",
      "train loss: 1.0091803706731416e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3774:\n",
      "train loss: 2.098856246417212e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3775:\n",
      "train loss: 1.9352038543882677e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3776:\n",
      "train loss: 1.1729600981559378e-09\n",
      "lr: 3.446734027981284e-10\n",
      "Epoch 3777:\n",
      "train loss: 1.0094547965693128e-09\n",
      "Epoch 03779: reducing learning rate of group 0 to 3.2744e-10.\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3778:\n",
      "train loss: 2.098522353110969e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3779:\n",
      "train loss: 1.9348418893013705e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3780:\n",
      "train loss: 1.0179128157298598e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3781:\n",
      "train loss: 8.62638771018409e-10\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3782:\n",
      "train loss: 2.089814309790752e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3783:\n",
      "train loss: 1.934231264462096e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3784:\n",
      "train loss: 1.0185790544905254e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3785:\n",
      "train loss: 8.634242324995235e-10\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3786:\n",
      "train loss: 2.0888150150490515e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3787:\n",
      "train loss: 1.933068194703381e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3788:\n",
      "train loss: 1.0198732610277417e-09\n",
      "lr: 3.27439732658222e-10\n",
      "Epoch 3789:\n",
      "train loss: 8.649617900309326e-10\n",
      "Epoch 03791: reducing learning rate of group 0 to 3.1107e-10.\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3790:\n",
      "train loss: 2.086853334856141e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3791:\n",
      "train loss: 1.930753929548682e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3792:\n",
      "train loss: 8.748636057011816e-10\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3793:\n",
      "train loss: 7.284851838016555e-10\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3794:\n",
      "train loss: 2.0741544568824517e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3795:\n",
      "train loss: 1.9245314528876927e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3796:\n",
      "train loss: 8.823130550316541e-10\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3797:\n",
      "train loss: 7.382078338460155e-10\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3798:\n",
      "train loss: 2.0602372042861818e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3799:\n",
      "train loss: 1.907070083645376e-09\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3800:\n",
      "train loss: 9.031088335022262e-10\n",
      "lr: 3.1106774602531087e-10\n",
      "Epoch 3801:\n",
      "train loss: 7.65056692897535e-10\n",
      "Epoch 03803: reducing learning rate of group 0 to 2.9551e-10.\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3802:\n",
      "train loss: 2.0224151048657603e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3803:\n",
      "train loss: 1.859800727358272e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3804:\n",
      "train loss: 8.188398474099852e-10\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3805:\n",
      "train loss: 7.075464635035827e-10\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3806:\n",
      "train loss: 1.9020629531987976e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3807:\n",
      "train loss: 1.7149384900227005e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3808:\n",
      "train loss: 9.921849130699807e-10\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3809:\n",
      "train loss: 9.1112895830519e-10\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3810:\n",
      "train loss: 1.6646366858906336e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3811:\n",
      "train loss: 1.4447337672437179e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3812:\n",
      "train loss: 1.288469912097335e-09\n",
      "lr: 2.955143587240453e-10\n",
      "Epoch 3813:\n",
      "train loss: 1.2164522975736514e-09\n",
      "Epoch 03815: reducing learning rate of group 0 to 2.8074e-10.\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3814:\n",
      "train loss: 1.3753423275872088e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3815:\n",
      "train loss: 1.1693516825426332e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3816:\n",
      "train loss: 1.4072565932206993e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3817:\n",
      "train loss: 1.3079093618922399e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3818:\n",
      "train loss: 1.1965271598964723e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3819:\n",
      "train loss: 1.0421667526686202e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3820:\n",
      "train loss: 1.4905365570586877e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3821:\n",
      "train loss: 1.3462460982843744e-09\n",
      "lr: 2.80738640787843e-10\n",
      "Epoch 3822:\n",
      "train loss: 1.2012274958485524e-09\n",
      "Epoch 03824: reducing learning rate of group 0 to 2.6670e-10.\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3823:\n",
      "train loss: 1.0829471952890356e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3824:\n",
      "train loss: 1.4263594614686473e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3825:\n",
      "train loss: 1.2710040350629186e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3826:\n",
      "train loss: 1.164756457789511e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3827:\n",
      "train loss: 1.0644607688500427e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3828:\n",
      "train loss: 1.3141413229713544e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3829:\n",
      "train loss: 1.1587737736138083e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3830:\n",
      "train loss: 1.272836979854209e-09\n",
      "lr: 2.6670170874845085e-10\n",
      "Epoch 3831:\n",
      "train loss: 1.1677113082503593e-09\n",
      "Epoch 03833: reducing learning rate of group 0 to 2.5337e-10.\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3832:\n",
      "train loss: 1.2185205659725053e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3833:\n",
      "train loss: 1.074100107838492e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3834:\n",
      "train loss: 1.2256586776220845e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3835:\n",
      "train loss: 1.1182740277057342e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3836:\n",
      "train loss: 1.1548273547748641e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3837:\n",
      "train loss: 1.0241933164759005e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3838:\n",
      "train loss: 1.2695630121720482e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3839:\n",
      "train loss: 1.1576900055150335e-09\n",
      "lr: 2.533666233110283e-10\n",
      "Epoch 3840:\n",
      "train loss: 1.1191426786490324e-09\n",
      "Epoch 03842: reducing learning rate of group 0 to 2.4070e-10.\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3841:\n",
      "train loss: 9.92151311129589e-10\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3842:\n",
      "train loss: 1.2982216956252744e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3843:\n",
      "train loss: 1.1890627403300505e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3844:\n",
      "train loss: 9.767121400860291e-10\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3845:\n",
      "train loss: 8.585943171275559e-10\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3846:\n",
      "train loss: 1.3145785251279228e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3847:\n",
      "train loss: 1.2026049728008387e-09\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3848:\n",
      "train loss: 9.660310937472493e-10\n",
      "lr: 2.4069829214547685e-10\n",
      "Epoch 3849:\n",
      "train loss: 8.505410106832084e-10\n",
      "Epoch 03851: reducing learning rate of group 0 to 2.2866e-10.\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3850:\n",
      "train loss: 1.319965203820578e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3851:\n",
      "train loss: 1.2052383394199401e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3852:\n",
      "train loss: 8.576720307117963e-10\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3853:\n",
      "train loss: 7.508613590162272e-10\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3854:\n",
      "train loss: 1.3077223128375673e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3855:\n",
      "train loss: 1.1954224856691976e-09\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3856:\n",
      "train loss: 8.705458168893892e-10\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3857:\n",
      "train loss: 7.663960546917047e-10\n",
      "lr: 2.28663377538203e-10\n",
      "Epoch 3858:\n",
      "train loss: 1.2899611313378679e-09\n",
      "Epoch 03860: reducing learning rate of group 0 to 2.1723e-10.\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3859:\n",
      "train loss: 1.1756320675222954e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3860:\n",
      "train loss: 8.921134348704254e-10\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3861:\n",
      "train loss: 7.943023157444301e-10\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3862:\n",
      "train loss: 1.1589388643444477e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3863:\n",
      "train loss: 1.050106907489806e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3864:\n",
      "train loss: 9.142783708391568e-10\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3865:\n",
      "train loss: 8.161888520367326e-10\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3866:\n",
      "train loss: 1.1377223314712143e-09\n",
      "lr: 2.1723020866129282e-10\n",
      "Epoch 3867:\n",
      "train loss: 1.029692896732143e-09\n",
      "Epoch 03869: reducing learning rate of group 0 to 2.0637e-10.\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3868:\n",
      "train loss: 9.338287148944084e-10\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3869:\n",
      "train loss: 8.348805246699304e-10\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3870:\n",
      "train loss: 1.022362121659044e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3871:\n",
      "train loss: 9.208812200739969e-10\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3872:\n",
      "train loss: 9.432849391339437e-10\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3873:\n",
      "train loss: 8.481800814940511e-10\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3874:\n",
      "train loss: 1.0101626470537334e-09\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3875:\n",
      "train loss: 9.098170044972595e-10\n",
      "lr: 2.0636869822822817e-10\n",
      "Epoch 3876:\n",
      "train loss: 9.532692919783032e-10\n",
      "Epoch 03878: reducing learning rate of group 0 to 1.9605e-10.\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3877:\n",
      "train loss: 8.572613143644627e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3878:\n",
      "train loss: 1.001889746297756e-09\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3879:\n",
      "train loss: 9.073359485977956e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3880:\n",
      "train loss: 8.618651591782958e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3881:\n",
      "train loss: 7.70049304956961e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3882:\n",
      "train loss: 9.966727122174666e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3883:\n",
      "train loss: 9.026343811537513e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3884:\n",
      "train loss: 8.660806240024368e-10\n",
      "lr: 1.9605026331681675e-10\n",
      "Epoch 3885:\n",
      "train loss: 7.738439854714848e-10\n",
      "Epoch 03887: reducing learning rate of group 0 to 1.8625e-10.\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3886:\n",
      "train loss: 9.932396186428453e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3887:\n",
      "train loss: 8.995379249847096e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3888:\n",
      "train loss: 7.80446562950602e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3889:\n",
      "train loss: 6.925694578275848e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3890:\n",
      "train loss: 9.86364566392346e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3891:\n",
      "train loss: 8.975312758208941e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3892:\n",
      "train loss: 7.822840059050175e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3893:\n",
      "train loss: 6.942633073787708e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3894:\n",
      "train loss: 9.84781984780263e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3895:\n",
      "train loss: 8.96053830970381e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3896:\n",
      "train loss: 7.836575683305279e-10\n",
      "lr: 1.862477501509759e-10\n",
      "Epoch 3897:\n",
      "train loss: 6.955542451835794e-10\n",
      "Epoch 03899: reducing learning rate of group 0 to 1.7694e-10.\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3898:\n",
      "train loss: 9.835552487830101e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3899:\n",
      "train loss: 8.948967857645548e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3900:\n",
      "train loss: 7.00760235794184e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3901:\n",
      "train loss: 6.170222769987987e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3902:\n",
      "train loss: 9.781482550360491e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3903:\n",
      "train loss: 8.939502341721009e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3904:\n",
      "train loss: 7.016673497190021e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3905:\n",
      "train loss: 6.179033658366416e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3906:\n",
      "train loss: 9.772861315821942e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3907:\n",
      "train loss: 8.931213665390396e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3908:\n",
      "train loss: 7.024489258885455e-10\n",
      "lr: 1.769353626434271e-10\n",
      "Epoch 3909:\n",
      "train loss: 6.186526093207263e-10\n",
      "Epoch 03911: reducing learning rate of group 0 to 1.6809e-10.\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3910:\n",
      "train loss: 9.765590796460817e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3911:\n",
      "train loss: 8.924328267039585e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3912:\n",
      "train loss: 6.233115120087238e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3913:\n",
      "train loss: 5.436781917490973e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3914:\n",
      "train loss: 9.717834024325254e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3915:\n",
      "train loss: 8.918888698829176e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3916:\n",
      "train loss: 6.238189887882918e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3917:\n",
      "train loss: 5.441619889207227e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3918:\n",
      "train loss: 9.713116396047912e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3919:\n",
      "train loss: 8.914432325228871e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3920:\n",
      "train loss: 6.242262969465836e-10\n",
      "lr: 1.6808859451125574e-10\n",
      "Epoch 3921:\n",
      "train loss: 5.445470384920609e-10\n",
      "Epoch 03923: reducing learning rate of group 0 to 1.5968e-10.\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3922:\n",
      "train loss: 9.709368116357102e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3923:\n",
      "train loss: 8.910917281773694e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3924:\n",
      "train loss: 5.487614257605752e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3925:\n",
      "train loss: 4.730496672704328e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3926:\n",
      "train loss: 9.666601315090026e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3927:\n",
      "train loss: 8.908214106953103e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3928:\n",
      "train loss: 5.490071088876255e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3929:\n",
      "train loss: 4.732853231902818e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3930:\n",
      "train loss: 9.66422810939173e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3931:\n",
      "train loss: 8.905941170372765e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3932:\n",
      "train loss: 5.492124591933178e-10\n",
      "lr: 1.5968416478569294e-10\n",
      "Epoch 3933:\n",
      "train loss: 4.734832534458174e-10\n",
      "Epoch 03935: reducing learning rate of group 0 to 1.5170e-10.\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3934:\n",
      "train loss: 9.662180632482657e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3935:\n",
      "train loss: 8.903956840024625e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3936:\n",
      "train loss: 4.774039855263918e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3937:\n",
      "train loss: 4.054624347498857e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3938:\n",
      "train loss: 9.622342767985227e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3939:\n",
      "train loss: 8.90196744109062e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3940:\n",
      "train loss: 4.775982460222789e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3941:\n",
      "train loss: 4.05668821991221e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3942:\n",
      "train loss: 9.61996068666173e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3943:\n",
      "train loss: 8.899411599766905e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3944:\n",
      "train loss: 4.778591303634511e-10\n",
      "lr: 1.516999565464083e-10\n",
      "Epoch 3945:\n",
      "train loss: 4.059574214331861e-10\n",
      "Epoch 03947: reducing learning rate of group 0 to 1.4411e-10.\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3946:\n",
      "train loss: 9.616485462955902e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3947:\n",
      "train loss: 8.895521164641499e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3948:\n",
      "train loss: 4.0988894701984237e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3949:\n",
      "train loss: 3.416791111849328e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3950:\n",
      "train loss: 9.573461632370504e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3951:\n",
      "train loss: 8.886929907633817e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3952:\n",
      "train loss: 4.108908540607975e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3953:\n",
      "train loss: 3.4295616512624063e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3954:\n",
      "train loss: 9.555610315587193e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3955:\n",
      "train loss: 8.864818791605578e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3956:\n",
      "train loss: 4.134970749381305e-10\n",
      "lr: 1.4411495871908787e-10\n",
      "Epoch 3957:\n",
      "train loss: 3.463197358287747e-10\n",
      "Epoch 03959: reducing learning rate of group 0 to 1.3691e-10.\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3958:\n",
      "train loss: 9.507846779230957e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3959:\n",
      "train loss: 8.805049267085052e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3960:\n",
      "train loss: 3.556336239293199e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3961:\n",
      "train loss: 2.9495970206196123e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3962:\n",
      "train loss: 9.305979954847455e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3963:\n",
      "train loss: 8.58341052513176e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3964:\n",
      "train loss: 3.8281499872888307e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3965:\n",
      "train loss: 3.313904378614573e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3966:\n",
      "train loss: 8.774262463924633e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3967:\n",
      "train loss: 7.90998096151948e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3968:\n",
      "train loss: 4.62593264611483e-10\n",
      "lr: 1.3690921078313347e-10\n",
      "Epoch 3969:\n",
      "train loss: 4.248977732010607e-10\n",
      "Epoch 03971: reducing learning rate of group 0 to 1.3006e-10.\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3970:\n",
      "train loss: 7.680320592796436e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3971:\n",
      "train loss: 6.660796979068488e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3972:\n",
      "train loss: 5.375290988387052e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3973:\n",
      "train loss: 5.087443500797975e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3974:\n",
      "train loss: 6.25729177926101e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3975:\n",
      "train loss: 5.286614453121537e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3976:\n",
      "train loss: 6.700024707347019e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3977:\n",
      "train loss: 6.281085838655771e-10\n",
      "lr: 1.3006375024397679e-10\n",
      "Epoch 3978:\n",
      "train loss: 5.296741187457976e-10\n",
      "Epoch 03980: reducing learning rate of group 0 to 1.2356e-10.\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3979:\n",
      "train loss: 4.5891484332094443e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3980:\n",
      "train loss: 7.091064624940012e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3981:\n",
      "train loss: 6.397809272208043e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3982:\n",
      "train loss: 4.891949565438868e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3983:\n",
      "train loss: 4.4820639278169926e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3984:\n",
      "train loss: 6.410905330597558e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3985:\n",
      "train loss: 5.545836797903494e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3986:\n",
      "train loss: 5.854960672568413e-10\n",
      "lr: 1.2356056273177793e-10\n",
      "Epoch 3987:\n",
      "train loss: 5.476456595398979e-10\n",
      "Epoch 03989: reducing learning rate of group 0 to 1.1738e-10.\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3988:\n",
      "train loss: 5.486916529615463e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3989:\n",
      "train loss: 4.729602631149521e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3990:\n",
      "train loss: 5.985533726764707e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3991:\n",
      "train loss: 5.520405569518064e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3992:\n",
      "train loss: 4.993279455741143e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3993:\n",
      "train loss: 4.372103112975823e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3994:\n",
      "train loss: 6.258274161219181e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3995:\n",
      "train loss: 5.729880090139909e-10\n",
      "lr: 1.1738253459518904e-10\n",
      "Epoch 3996:\n",
      "train loss: 4.833519377572847e-10\n",
      "Epoch 03998: reducing learning rate of group 0 to 1.1151e-10.\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 3997:\n",
      "train loss: 4.2593957286280864e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 3998:\n",
      "train loss: 6.33602044866068e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 3999:\n",
      "train loss: 5.813652123598679e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4000:\n",
      "train loss: 4.235398343196148e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4001:\n",
      "train loss: 3.7025569909963773e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4002:\n",
      "train loss: 6.354311890246641e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4003:\n",
      "train loss: 5.82753485176167e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4004:\n",
      "train loss: 4.2242480444414055e-10\n",
      "lr: 1.1151340786542958e-10\n",
      "Epoch 4005:\n",
      "train loss: 3.693298339839301e-10\n",
      "Epoch 04007: reducing learning rate of group 0 to 1.0594e-10.\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4006:\n",
      "train loss: 6.361382962170988e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4007:\n",
      "train loss: 5.832642523775118e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4008:\n",
      "train loss: 3.7191700850042605e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4009:\n",
      "train loss: 3.2190640986234604e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4010:\n",
      "train loss: 6.324524780283261e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4011:\n",
      "train loss: 5.813575177022245e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4012:\n",
      "train loss: 3.747325040621973e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4013:\n",
      "train loss: 3.257910912333615e-10\n",
      "lr: 1.059377374721581e-10\n",
      "Epoch 4014:\n",
      "train loss: 6.271381748632048e-10\n",
      "Epoch 04016: reducing learning rate of group 0 to 1.0064e-10.\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4015:\n",
      "train loss: 5.746166726720166e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4016:\n",
      "train loss: 3.8287303861640553e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4017:\n",
      "train loss: 3.3761699120995575e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4018:\n",
      "train loss: 5.665153710232044e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4019:\n",
      "train loss: 5.153980552047489e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4020:\n",
      "train loss: 3.953613127730258e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4021:\n",
      "train loss: 3.509656483869879e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4022:\n",
      "train loss: 5.525650617671353e-10\n",
      "lr: 1.0064085059855019e-10\n",
      "Epoch 4023:\n",
      "train loss: 5.008468837245716e-10\n",
      "Epoch 04025: reducing learning rate of group 0 to 9.5609e-11.\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4024:\n",
      "train loss: 4.1043360911747423e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4025:\n",
      "train loss: 3.663105956846296e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4026:\n",
      "train loss: 4.921783184108235e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4027:\n",
      "train loss: 4.432236425920585e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4028:\n",
      "train loss: 4.2220975580441617e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4029:\n",
      "train loss: 3.797941708862875e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4030:\n",
      "train loss: 4.794584152119436e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4031:\n",
      "train loss: 4.313052781366185e-10\n",
      "lr: 9.560880806862267e-11\n",
      "Epoch 4032:\n",
      "train loss: 4.3334061948475653e-10\n",
      "Epoch 04034: reducing learning rate of group 0 to 9.0828e-11.\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4033:\n",
      "train loss: 3.902033102811468e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4034:\n",
      "train loss: 4.698136900564591e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4035:\n",
      "train loss: 4.248271135114168e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4036:\n",
      "train loss: 3.9588507069773835e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4037:\n",
      "train loss: 3.5434367398687944e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4038:\n",
      "train loss: 4.63173629008043e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4039:\n",
      "train loss: 4.187266676314693e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4040:\n",
      "train loss: 4.0144526277890347e-10\n",
      "lr: 9.082836766519154e-11\n",
      "Epoch 4041:\n",
      "train loss: 3.594404203821259e-10\n",
      "Epoch 04043: reducing learning rate of group 0 to 8.6287e-11.\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4042:\n",
      "train loss: 4.585113827243347e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4043:\n",
      "train loss: 4.145115083467131e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4044:\n",
      "train loss: 3.642484503803341e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4045:\n",
      "train loss: 3.2399406010550283e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4046:\n",
      "train loss: 4.5336848479692786e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4047:\n",
      "train loss: 4.1186077677362283e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4048:\n",
      "train loss: 3.666299421573709e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4049:\n",
      "train loss: 3.2613935278484986e-10\n",
      "lr: 8.628694928193196e-11\n",
      "Epoch 4050:\n",
      "train loss: 4.5142669568877455e-10\n",
      "Epoch 04052: reducing learning rate of group 0 to 8.1973e-11.\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4051:\n",
      "train loss: 4.1009902611146544e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4052:\n",
      "train loss: 3.682297200844952e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4053:\n",
      "train loss: 3.2962567754479597e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4054:\n",
      "train loss: 4.0918334818015944e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4055:\n",
      "train loss: 3.700296733977688e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4056:\n",
      "train loss: 3.692834708637502e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4057:\n",
      "train loss: 3.3058984766134875e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4058:\n",
      "train loss: 4.0829747713016063e-10\n",
      "lr: 8.197260181783536e-11\n",
      "Epoch 4059:\n",
      "train loss: 3.692165714051399e-10\n",
      "Epoch 04061: reducing learning rate of group 0 to 7.7874e-11.\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4060:\n",
      "train loss: 3.700269396010698e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4061:\n",
      "train loss: 3.3127559062278684e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4062:\n",
      "train loss: 3.707179999737341e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4063:\n",
      "train loss: 3.336429397606294e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4064:\n",
      "train loss: 3.6858280405979696e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4065:\n",
      "train loss: 3.317225830000261e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4066:\n",
      "train loss: 3.703079902436058e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4067:\n",
      "train loss: 3.332752412613414e-10\n",
      "lr: 7.787397172694359e-11\n",
      "Epoch 4068:\n",
      "train loss: 3.689074545227489e-10\n",
      "Epoch 04070: reducing learning rate of group 0 to 7.3980e-11.\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4069:\n",
      "train loss: 3.3201630234964873e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4070:\n",
      "train loss: 3.7003556533441934e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4071:\n",
      "train loss: 3.348807833663383e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4072:\n",
      "train loss: 3.321625040499426e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4073:\n",
      "train loss: 2.9709746517789513e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4074:\n",
      "train loss: 3.6986245216402293e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4075:\n",
      "train loss: 3.347243303357285e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4076:\n",
      "train loss: 3.3229762674024206e-10\n",
      "lr: 7.39802731405964e-11\n",
      "Epoch 4077:\n",
      "train loss: 2.972188063490053e-10\n",
      "Epoch 04079: reducing learning rate of group 0 to 7.0281e-11.\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4078:\n",
      "train loss: 3.6974600895035893e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4079:\n",
      "train loss: 3.3461986733669207e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4080:\n",
      "train loss: 2.990354583250516e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4081:\n",
      "train loss: 2.6570268842798674e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4082:\n",
      "train loss: 3.679155664172545e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4083:\n",
      "train loss: 3.3455275080698804e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4084:\n",
      "train loss: 2.990907124870869e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4085:\n",
      "train loss: 2.6575358779950105e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4086:\n",
      "train loss: 3.678632683385407e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4087:\n",
      "train loss: 3.34503939306071e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4088:\n",
      "train loss: 2.9913176059346054e-10\n",
      "lr: 7.028125948356658e-11\n",
      "Epoch 4089:\n",
      "train loss: 2.6579183924823385e-10\n",
      "Epoch 04091: reducing learning rate of group 0 to 6.6767e-11.\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4090:\n",
      "train loss: 3.678213389556656e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4091:\n",
      "train loss: 3.344639944929916e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4092:\n",
      "train loss: 2.6748315423171236e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4093:\n",
      "train loss: 2.358103834128056e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4094:\n",
      "train loss: 3.661162024565746e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4095:\n",
      "train loss: 3.344265190318201e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4096:\n",
      "train loss: 2.6751639468574456e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4097:\n",
      "train loss: 2.358441300205618e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4098:\n",
      "train loss: 3.6607727483744485e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4099:\n",
      "train loss: 3.3438716961375766e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4100:\n",
      "train loss: 2.6755096445836454e-10\n",
      "lr: 6.676719650938824e-11\n",
      "Epoch 4101:\n",
      "train loss: 2.3587901310187963e-10\n",
      "Epoch 04103: reducing learning rate of group 0 to 6.3429e-11.\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4102:\n",
      "train loss: 3.6603657173913295e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4103:\n",
      "train loss: 3.343463020801536e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4104:\n",
      "train loss: 2.3748910828049576e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4105:\n",
      "train loss: 2.0740215854208278e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4106:\n",
      "train loss: 3.644120613544176e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4107:\n",
      "train loss: 3.3430531895954994e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4108:\n",
      "train loss: 2.3752689794354623e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4109:\n",
      "train loss: 2.0744121962152676e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4110:\n",
      "train loss: 3.643670869062959e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4111:\n",
      "train loss: 3.3426035564354e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4112:\n",
      "train loss: 2.375670745610055e-10\n",
      "lr: 6.342883668391883e-11\n",
      "Epoch 4113:\n",
      "train loss: 2.074828037489052e-10\n",
      "Epoch 04115: reducing learning rate of group 0 to 6.0257e-11.\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4114:\n",
      "train loss: 3.6432021913555814e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4115:\n",
      "train loss: 3.342133441768867e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4116:\n",
      "train loss: 2.0901859631480626e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4117:\n",
      "train loss: 1.8043974618806936e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4118:\n",
      "train loss: 3.6276505169380704e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4119:\n",
      "train loss: 3.3416083167257806e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4120:\n",
      "train loss: 2.0906976422904885e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4121:\n",
      "train loss: 1.8049551930050254e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4122:\n",
      "train loss: 3.627012656392127e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4123:\n",
      "train loss: 3.340932325450569e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4124:\n",
      "train loss: 2.0913649463014965e-10\n",
      "lr: 6.025739484972289e-11\n",
      "Epoch 4125:\n",
      "train loss: 1.8056582181676565e-10\n",
      "Epoch 04127: reducing learning rate of group 0 to 5.7245e-11.\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4126:\n",
      "train loss: 3.6261951391810535e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4127:\n",
      "train loss: 3.340051968323783e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4128:\n",
      "train loss: 1.8206407901960912e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4129:\n",
      "train loss: 1.5493448269604838e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4130:\n",
      "train loss: 3.6106684972026846e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4131:\n",
      "train loss: 3.3386517971683275e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4132:\n",
      "train loss: 1.8221755808977253e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4133:\n",
      "train loss: 1.551118625262635e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4134:\n",
      "train loss: 3.6084743437634273e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4135:\n",
      "train loss: 3.336129801510518e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4136:\n",
      "train loss: 1.824973490589902e-10\n",
      "lr: 5.7244525107236744e-11\n",
      "Epoch 4137:\n",
      "train loss: 1.5543783795966152e-10\n",
      "Epoch 04139: reducing learning rate of group 0 to 5.4382e-11.\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4138:\n",
      "train loss: 3.604440191092041e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4139:\n",
      "train loss: 3.3314416328666167e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4140:\n",
      "train loss: 1.572214112630802e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4141:\n",
      "train loss: 1.316531447268292e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4142:\n",
      "train loss: 3.5816342127802426e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4143:\n",
      "train loss: 3.319992030551483e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4144:\n",
      "train loss: 1.5857667498313703e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4145:\n",
      "train loss: 1.3337144537955369e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4146:\n",
      "train loss: 3.5580403640835527e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4147:\n",
      "train loss: 3.2909219707186064e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4148:\n",
      "train loss: 1.6199702732760703e-10\n",
      "lr: 5.4382298851874906e-11\n",
      "Epoch 4149:\n",
      "train loss: 1.3766318614186691e-10\n",
      "Epoch 04151: reducing learning rate of group 0 to 5.1663e-11.\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4150:\n",
      "train loss: 3.4999633699931626e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4151:\n",
      "train loss: 3.219654237602712e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4152:\n",
      "train loss: 1.4582036890390058e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4153:\n",
      "train loss: 1.2537628337884664e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4154:\n",
      "train loss: 3.328028298002725e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4155:\n",
      "train loss: 3.018240921567668e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4156:\n",
      "train loss: 1.698013396237537e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4157:\n",
      "train loss: 1.5372783861596912e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4158:\n",
      "train loss: 2.9906506874118806e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4159:\n",
      "train loss: 2.630297669293993e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4160:\n",
      "train loss: 2.128059515882643e-10\n",
      "lr: 5.166318390928116e-11\n",
      "Epoch 4161:\n",
      "train loss: 1.9898841356514997e-10\n",
      "Epoch 04163: reducing learning rate of group 0 to 4.9080e-11.\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4162:\n",
      "train loss: 2.543515595481179e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4163:\n",
      "train loss: 2.1842430238388426e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4164:\n",
      "train loss: 2.3286232002185453e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4165:\n",
      "train loss: 2.1721083625556346e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4166:\n",
      "train loss: 2.1805981139772105e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4167:\n",
      "train loss: 1.8838429081779682e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4168:\n",
      "train loss: 2.576293776700524e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4169:\n",
      "train loss: 2.360012033688133e-10\n",
      "lr: 4.90800247138171e-11\n",
      "Epoch 4170:\n",
      "train loss: 2.0582122468620753e-10\n",
      "Epoch 04172: reducing learning rate of group 0 to 4.6626e-11.\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4171:\n",
      "train loss: 1.824650390152992e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4172:\n",
      "train loss: 2.579944357926533e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4173:\n",
      "train loss: 2.3258518945373362e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4174:\n",
      "train loss: 1.9172700574524552e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4175:\n",
      "train loss: 1.7360656654810032e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4176:\n",
      "train loss: 2.418181106768458e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4177:\n",
      "train loss: 2.1418621353579483e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4178:\n",
      "train loss: 2.1143328693015124e-10\n",
      "lr: 4.662602347812624e-11\n",
      "Epoch 4179:\n",
      "train loss: 1.9376810489624036e-10\n",
      "Epoch 04181: reducing learning rate of group 0 to 4.4295e-11.\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4180:\n",
      "train loss: 2.2245691823491817e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4181:\n",
      "train loss: 1.9624375751907786e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4182:\n",
      "train loss: 2.0662103739099145e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4183:\n",
      "train loss: 1.885911925702639e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4184:\n",
      "train loss: 2.0804501935444534e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4185:\n",
      "train loss: 1.8448550226195937e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4186:\n",
      "train loss: 2.17099052793713e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4187:\n",
      "train loss: 1.9805049270656225e-10\n",
      "lr: 4.429472230421993e-11\n",
      "Epoch 4188:\n",
      "train loss: 1.9949181663829562e-10\n",
      "Epoch 04190: reducing learning rate of group 0 to 4.2080e-11.\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4189:\n",
      "train loss: 1.7688942624359281e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4190:\n",
      "train loss: 2.2381911334873426e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4191:\n",
      "train loss: 2.0506557222199049e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4192:\n",
      "train loss: 1.7315960350742119e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4193:\n",
      "train loss: 1.5221764111992293e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4194:\n",
      "train loss: 2.2796794178449637e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4195:\n",
      "train loss: 2.087862680989196e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4196:\n",
      "train loss: 1.6983335482184773e-10\n",
      "lr: 4.207998618900893e-11\n",
      "Epoch 4197:\n",
      "train loss: 1.492525639841237e-10\n",
      "Epoch 04199: reducing learning rate of group 0 to 3.9976e-11.\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4198:\n",
      "train loss: 2.3056820089894503e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4199:\n",
      "train loss: 2.110277469610287e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4200:\n",
      "train loss: 1.490064458044237e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4201:\n",
      "train loss: 1.298133333207455e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4202:\n",
      "train loss: 2.3058801314314064e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4203:\n",
      "train loss: 2.1160052528778796e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4204:\n",
      "train loss: 1.4884151934868672e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4205:\n",
      "train loss: 1.3004734564263524e-10\n",
      "lr: 3.997598687955848e-11\n",
      "Epoch 4206:\n",
      "train loss: 2.299370475285625e-10\n",
      "Epoch 04208: reducing learning rate of group 0 to 3.7977e-11.\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4207:\n",
      "train loss: 2.1055071995040426e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4208:\n",
      "train loss: 1.502658546865471e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4209:\n",
      "train loss: 1.3271757568083557e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4210:\n",
      "train loss: 2.09026378779997e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4211:\n",
      "train loss: 1.903681549175893e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4212:\n",
      "train loss: 1.5262183191992492e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4213:\n",
      "train loss: 1.352311205548701e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4214:\n",
      "train loss: 2.0640739032532152e-10\n",
      "lr: 3.7977187535580556e-11\n",
      "Epoch 4215:\n",
      "train loss: 1.8765839154351705e-10\n",
      "Epoch 04217: reducing learning rate of group 0 to 3.6078e-11.\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4216:\n",
      "train loss: 1.5540553244774197e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4217:\n",
      "train loss: 1.3805693553166494e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4218:\n",
      "train loss: 1.8652149492044876e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4219:\n",
      "train loss: 1.6874493479973413e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4220:\n",
      "train loss: 1.5711019099127544e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4221:\n",
      "train loss: 1.4054430522106196e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4222:\n",
      "train loss: 1.8415889974184804e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4223:\n",
      "train loss: 1.665161830903501e-10\n",
      "lr: 3.6078328158801524e-11\n",
      "Epoch 4224:\n",
      "train loss: 1.5921042890238806e-10\n",
      "Epoch 04226: reducing learning rate of group 0 to 3.4274e-11.\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4225:\n",
      "train loss: 1.4252517851405829e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4226:\n",
      "train loss: 1.8230578305247878e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4227:\n",
      "train loss: 1.656709577576902e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4228:\n",
      "train loss: 1.4365138822616805e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4229:\n",
      "train loss: 1.27706647642867e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4230:\n",
      "train loss: 1.8096714368672571e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4231:\n",
      "train loss: 1.6442390929935401e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4232:\n",
      "train loss: 1.4480467043408022e-10\n",
      "lr: 3.427441175086145e-11\n",
      "Epoch 4233:\n",
      "train loss: 1.2877568322758692e-10\n",
      "Epoch 04235: reducing learning rate of group 0 to 3.2561e-11.\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4234:\n",
      "train loss: 1.7997971664441247e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4235:\n",
      "train loss: 1.635224502698316e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4236:\n",
      "train loss: 1.3016480528905648e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4237:\n",
      "train loss: 1.148690859178665e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4238:\n",
      "train loss: 1.7851130647053846e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4239:\n",
      "train loss: 1.6293894571469024e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4240:\n",
      "train loss: 1.306910628400267e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4241:\n",
      "train loss: 1.1534384120465053e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4242:\n",
      "train loss: 1.7808105845673475e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4243:\n",
      "train loss: 1.6255152492475447e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4244:\n",
      "train loss: 1.310372667304322e-10\n",
      "lr: 3.256069116331837e-11\n",
      "Epoch 4245:\n",
      "train loss: 1.1565673032677604e-10\n",
      "Epoch 04247: reducing learning rate of group 0 to 3.0933e-11.\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4246:\n",
      "train loss: 1.7779644313492344e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4247:\n",
      "train loss: 1.6229240439675845e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4248:\n",
      "train loss: 1.1659254845781247e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4249:\n",
      "train loss: 1.0196163058297637e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4250:\n",
      "train loss: 1.7683173546358603e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4251:\n",
      "train loss: 1.6211626350283984e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4252:\n",
      "train loss: 1.1675542215165154e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4253:\n",
      "train loss: 1.021161173063437e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4254:\n",
      "train loss: 1.7668208993632352e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4255:\n",
      "train loss: 1.6197204236212733e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4256:\n",
      "train loss: 1.1689376225206257e-10\n",
      "lr: 3.0932656605152455e-11\n",
      "Epoch 4257:\n",
      "train loss: 1.0224966128895386e-10\n",
      "Epoch 04259: reducing learning rate of group 0 to 2.9386e-11.\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4258:\n",
      "train loss: 1.7654972447163333e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4259:\n",
      "train loss: 1.6184211576196442e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4260:\n",
      "train loss: 1.0307839724851998e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4261:\n",
      "train loss: 8.916801861522626e-11\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4262:\n",
      "train loss: 1.756822688350656e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4263:\n",
      "train loss: 1.6170544367740244e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4264:\n",
      "train loss: 1.0321697678697779e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4265:\n",
      "train loss: 8.931351310121256e-11\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4266:\n",
      "train loss: 1.7552799346448036e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4267:\n",
      "train loss: 1.6154341012869442e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4268:\n",
      "train loss: 1.0338412118874255e-10\n",
      "lr: 2.938602377489483e-11\n",
      "Epoch 4269:\n",
      "train loss: 8.948820609963656e-11\n",
      "Epoch 04271: reducing learning rate of group 0 to 2.7917e-11.\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4270:\n",
      "train loss: 1.7534067263696457e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4271:\n",
      "train loss: 1.6134665018966472e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4272:\n",
      "train loss: 9.034237735479996e-11\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4273:\n",
      "train loss: 7.716071836030039e-11\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4274:\n",
      "train loss: 1.743907168754077e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4275:\n",
      "train loss: 1.6106571150379381e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4276:\n",
      "train loss: 9.065095962743225e-11\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4277:\n",
      "train loss: 7.750650270371807e-11\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4278:\n",
      "train loss: 1.7398994610064073e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4279:\n",
      "train loss: 1.6061668417747546e-10\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4280:\n",
      "train loss: 9.114330013514298e-11\n",
      "lr: 2.7916722586150086e-11\n",
      "Epoch 4281:\n",
      "train loss: 7.806057067903693e-11\n",
      "Epoch 04283: reducing learning rate of group 0 to 2.6521e-11.\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4282:\n",
      "train loss: 1.73341387381768e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4283:\n",
      "train loss: 1.5988283345028644e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4284:\n",
      "train loss: 7.937123177558658e-11\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4285:\n",
      "train loss: 6.710754297221513e-11\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4286:\n",
      "train loss: 1.7141927252295618e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4287:\n",
      "train loss: 1.5836723073310573e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4288:\n",
      "train loss: 8.113316339635599e-11\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4289:\n",
      "train loss: 6.923149044504597e-11\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4290:\n",
      "train loss: 1.6871567663539824e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4291:\n",
      "train loss: 1.5515581327359494e-10\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4292:\n",
      "train loss: 8.481871752481138e-11\n",
      "lr: 2.652088645684258e-11\n",
      "Epoch 4293:\n",
      "train loss: 7.357061879711236e-11\n",
      "Epoch 04295: reducing learning rate of group 0 to 2.5195e-11.\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4294:\n",
      "train loss: 1.6339281935964745e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4295:\n",
      "train loss: 1.4894073670375941e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4296:\n",
      "train loss: 7.987477661316558e-11\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4297:\n",
      "train loss: 7.050717416810014e-11\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4298:\n",
      "train loss: 1.5248908442657642e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4299:\n",
      "train loss: 1.3683217576994016e-10\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4300:\n",
      "train loss: 9.366660041738239e-11\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4301:\n",
      "train loss: 8.563632964003668e-11\n",
      "lr: 2.5194842134000447e-11\n",
      "Epoch 4302:\n",
      "train loss: 1.3643967725954556e-10\n",
      "Epoch 04304: reducing learning rate of group 0 to 2.3935e-11.\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4303:\n",
      "train loss: 1.198046787976284e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4304:\n",
      "train loss: 1.1142889888349461e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4305:\n",
      "train loss: 1.0368897901194479e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4306:\n",
      "train loss: 1.085795508617268e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4307:\n",
      "train loss: 9.407375741859474e-11\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4308:\n",
      "train loss: 1.2390335205333033e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4309:\n",
      "train loss: 1.1412864364550776e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4310:\n",
      "train loss: 1.0042578600119568e-10\n",
      "lr: 2.3935100027300423e-11\n",
      "Epoch 4311:\n",
      "train loss: 8.819480159905235e-11\n",
      "Epoch 04313: reducing learning rate of group 0 to 2.2738e-11.\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4312:\n",
      "train loss: 1.275903007495982e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4313:\n",
      "train loss: 1.1578869283398509e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4314:\n",
      "train loss: 8.98795071524663e-11\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4315:\n",
      "train loss: 8.008534072115731e-11\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4316:\n",
      "train loss: 1.2319897382799665e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4317:\n",
      "train loss: 1.1047470556756172e-10\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4318:\n",
      "train loss: 9.642752274305129e-11\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4319:\n",
      "train loss: 8.743299191129312e-11\n",
      "lr: 2.27383450259354e-11\n",
      "Epoch 4320:\n",
      "train loss: 1.1564470945047468e-10\n",
      "Epoch 04322: reducing learning rate of group 0 to 2.1601e-11.\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4321:\n",
      "train loss: 1.0290815030891993e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4322:\n",
      "train loss: 1.0387176001237489e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4323:\n",
      "train loss: 9.50645762367879e-11\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4324:\n",
      "train loss: 9.838832141708053e-11\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4325:\n",
      "train loss: 8.693197455210744e-11\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4326:\n",
      "train loss: 1.0882391920530827e-10\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4327:\n",
      "train loss: 9.940573252373623e-11\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4328:\n",
      "train loss: 9.460130258956619e-11\n",
      "lr: 2.1601427774638628e-11\n",
      "Epoch 4329:\n",
      "train loss: 8.37008452204909e-11\n",
      "Epoch 04331: reducing learning rate of group 0 to 2.0521e-11.\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4330:\n",
      "train loss: 1.1157231580739637e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4331:\n",
      "train loss: 1.0178075517341168e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4332:\n",
      "train loss: 8.282621225832602e-11\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4333:\n",
      "train loss: 7.276406714811053e-11\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4334:\n",
      "train loss: 1.1248963015848237e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4335:\n",
      "train loss: 1.0298821691249938e-10\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4336:\n",
      "train loss: 8.178090345120672e-11\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4337:\n",
      "train loss: 7.18612856596764e-11\n",
      "lr: 2.0521356385906695e-11\n",
      "Epoch 4338:\n",
      "train loss: 1.1328003151597691e-10\n",
      "Epoch 04340: reducing learning rate of group 0 to 1.9495e-11.\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4339:\n",
      "train loss: 1.0369150591597118e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4340:\n",
      "train loss: 8.11505210248694e-11\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4341:\n",
      "train loss: 7.178833566443283e-11\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4342:\n",
      "train loss: 1.0403751863379837e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4343:\n",
      "train loss: 9.487745228314462e-11\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4344:\n",
      "train loss: 8.076791654242737e-11\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4345:\n",
      "train loss: 7.144886047142118e-11\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4346:\n",
      "train loss: 1.0433328682595852e-10\n",
      "lr: 1.949528856661136e-11\n",
      "Epoch 4347:\n",
      "train loss: 9.513163049149638e-11\n",
      "Epoch 04349: reducing learning rate of group 0 to 1.8521e-11.\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4348:\n",
      "train loss: 8.05550368958852e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4349:\n",
      "train loss: 7.127322535420667e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4350:\n",
      "train loss: 9.568480374235743e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4351:\n",
      "train loss: 8.690873693266354e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4352:\n",
      "train loss: 8.002237750157021e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4353:\n",
      "train loss: 7.122924921925586e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4354:\n",
      "train loss: 9.5705758811041e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4355:\n",
      "train loss: 8.690925530643526e-11\n",
      "lr: 1.852052413828079e-11\n",
      "Epoch 4356:\n",
      "train loss: 8.004168631035066e-11\n",
      "Epoch 04358: reducing learning rate of group 0 to 1.7594e-11.\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4357:\n",
      "train loss: 7.126606307477914e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4358:\n",
      "train loss: 9.565421449965685e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4359:\n",
      "train loss: 8.72851063432046e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4360:\n",
      "train loss: 7.133150429298439e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4361:\n",
      "train loss: 6.301047118723691e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4362:\n",
      "train loss: 9.554600437830011e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4363:\n",
      "train loss: 8.716240023297386e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4364:\n",
      "train loss: 7.146455544312567e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4365:\n",
      "train loss: 6.315372315732865e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4366:\n",
      "train loss: 9.539676091965592e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4367:\n",
      "train loss: 8.701027252695933e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4368:\n",
      "train loss: 7.16174008600864e-11\n",
      "lr: 1.759449793136675e-11\n",
      "Epoch 4369:\n",
      "train loss: 6.330747880516925e-11\n",
      "Epoch 04371: reducing learning rate of group 0 to 1.6715e-11.\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4370:\n",
      "train loss: 9.524326129249974e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4371:\n",
      "train loss: 8.685962346246953e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4372:\n",
      "train loss: 6.38327664815908e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4373:\n",
      "train loss: 5.593638456396233e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4374:\n",
      "train loss: 9.46861966697373e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4375:\n",
      "train loss: 8.67224150804524e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4376:\n",
      "train loss: 6.39666220827955e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4377:\n",
      "train loss: 5.6069269838741106e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4378:\n",
      "train loss: 9.455498820477431e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4379:\n",
      "train loss: 8.659556210466033e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4380:\n",
      "train loss: 6.408954055196401e-11\n",
      "lr: 1.671477303479841e-11\n",
      "Epoch 4381:\n",
      "train loss: 5.6188039103758945e-11\n",
      "Epoch 04383: reducing learning rate of group 0 to 1.5879e-11.\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4382:\n",
      "train loss: 9.444003392918143e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4383:\n",
      "train loss: 8.648391362351143e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4384:\n",
      "train loss: 5.666195530466669e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4385:\n",
      "train loss: 4.91532039853427e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4386:\n",
      "train loss: 9.394267256539716e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4387:\n",
      "train loss: 8.638593777985277e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4388:\n",
      "train loss: 5.675814882017261e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4389:\n",
      "train loss: 4.9248871460496487e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4390:\n",
      "train loss: 9.38459127263987e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4391:\n",
      "train loss: 8.628977640505276e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4392:\n",
      "train loss: 5.685291697436901e-11\n",
      "lr: 1.587903438305849e-11\n",
      "Epoch 4393:\n",
      "train loss: 4.9344754582426734e-11\n",
      "Epoch 04395: reducing learning rate of group 0 to 1.5085e-11.\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4394:\n",
      "train loss: 9.374964703007288e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4395:\n",
      "train loss: 8.619221253528543e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4396:\n",
      "train loss: 4.9793635379468325e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4397:\n",
      "train loss: 4.266440671234486e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4398:\n",
      "train loss: 9.32631165292602e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4399:\n",
      "train loss: 8.607564365293776e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4400:\n",
      "train loss: 4.991796013043728e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4401:\n",
      "train loss: 4.279896377574498e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4402:\n",
      "train loss: 9.311289889778276e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4403:\n",
      "train loss: 8.591080545807411e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4404:\n",
      "train loss: 5.0094138772351446e-11\n",
      "lr: 1.5085082663905565e-11\n",
      "Epoch 4405:\n",
      "train loss: 4.299399409892746e-11\n",
      "Epoch 04407: reducing learning rate of group 0 to 1.4331e-11.\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4406:\n",
      "train loss: 9.288928419900535e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4407:\n",
      "train loss: 8.56630821730156e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4408:\n",
      "train loss: 4.356495091048771e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4409:\n",
      "train loss: 3.6868746504686387e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4410:\n",
      "train loss: 9.212864329971266e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4411:\n",
      "train loss: 8.518384484233494e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4412:\n",
      "train loss: 4.4117232462073853e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4413:\n",
      "train loss: 3.752871608593017e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4414:\n",
      "train loss: 9.129352279910462e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4415:\n",
      "train loss: 8.419682442059364e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4416:\n",
      "train loss: 4.524513373541774e-11\n",
      "lr: 1.4330828530710286e-11\n",
      "Epoch 4417:\n",
      "train loss: 3.886510244097439e-11\n",
      "Epoch 04419: reducing learning rate of group 0 to 1.3614e-11.\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4418:\n",
      "train loss: 8.96283833082299e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4419:\n",
      "train loss: 8.2238597306147e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4420:\n",
      "train loss: 4.100921225718824e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4421:\n",
      "train loss: 3.546463823707151e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4422:\n",
      "train loss: 8.568200777677965e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4423:\n",
      "train loss: 7.785384521869076e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4424:\n",
      "train loss: 4.611040358889973e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4425:\n",
      "train loss: 4.133802453087187e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4426:\n",
      "train loss: 7.889884606161662e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4427:\n",
      "train loss: 7.021337673489411e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4428:\n",
      "train loss: 5.4483199137984435e-11\n",
      "lr: 1.3614287104174771e-11\n",
      "Epoch 4429:\n",
      "train loss: 5.017592683215103e-11\n",
      "Epoch 04431: reducing learning rate of group 0 to 1.2934e-11.\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4430:\n",
      "train loss: 6.995104703441016e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4431:\n",
      "train loss: 6.108813213567259e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4432:\n",
      "train loss: 5.748096194600037e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4433:\n",
      "train loss: 5.3226290858398917e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4434:\n",
      "train loss: 6.137168305908661e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4435:\n",
      "train loss: 5.335651981015503e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4436:\n",
      "train loss: 6.467747156761989e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4437:\n",
      "train loss: 5.965334930322854e-11\n",
      "lr: 1.2933572748966032e-11\n",
      "Epoch 4438:\n",
      "train loss: 5.5993912831745255e-11\n",
      "Epoch 04440: reducing learning rate of group 0 to 1.2287e-11.\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4439:\n",
      "train loss: 4.90624995638223e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4440:\n",
      "train loss: 6.787463327223189e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4441:\n",
      "train loss: 6.207198431930867e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4442:\n",
      "train loss: 4.886447519525559e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4443:\n",
      "train loss: 4.338706901166718e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4444:\n",
      "train loss: 6.663108396782342e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4445:\n",
      "train loss: 5.990628493207911e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4446:\n",
      "train loss: 5.176044233307924e-11\n",
      "lr: 1.228689411151773e-11\n",
      "Epoch 4447:\n",
      "train loss: 4.678784448405527e-11\n",
      "Epoch 04449: reducing learning rate of group 0 to 1.1673e-11.\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4448:\n",
      "train loss: 6.304423900576411e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4449:\n",
      "train loss: 5.6259777674244684e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4450:\n",
      "train loss: 4.9825631179909136e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4451:\n",
      "train loss: 4.507861425051978e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4452:\n",
      "train loss: 5.935232036016562e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4453:\n",
      "train loss: 5.3039197741446166e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4454:\n",
      "train loss: 5.289040037314134e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4455:\n",
      "train loss: 4.7981586488668905e-11\n",
      "lr: 1.1672549405941844e-11\n",
      "Epoch 4456:\n",
      "train loss: 5.665345620717828e-11\n",
      "Epoch 04458: reducing learning rate of group 0 to 1.1089e-11.\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4457:\n",
      "train loss: 5.0573825715893953e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4458:\n",
      "train loss: 5.5125570541406335e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4459:\n",
      "train loss: 5.027282358358291e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4460:\n",
      "train loss: 4.931202950746154e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4461:\n",
      "train loss: 4.3724095654818084e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4462:\n",
      "train loss: 5.65134827137968e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4463:\n",
      "train loss: 5.1513445305078346e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4464:\n",
      "train loss: 4.819742936744959e-11\n",
      "lr: 1.1088921935644751e-11\n",
      "Epoch 4465:\n",
      "train loss: 4.27265759207421e-11\n",
      "Epoch 04467: reducing learning rate of group 0 to 1.0534e-11.\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4466:\n",
      "train loss: 5.740496313463567e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4467:\n",
      "train loss: 5.2314875749116264e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4468:\n",
      "train loss: 4.248583621177515e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4469:\n",
      "train loss: 3.7358593937860903e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4470:\n",
      "train loss: 5.7701600881041804e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4471:\n",
      "train loss: 5.280516675568051e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4472:\n",
      "train loss: 4.205068131748997e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4473:\n",
      "train loss: 3.697521077816055e-11\n",
      "lr: 1.0534475838862512e-11\n",
      "Epoch 4474:\n",
      "train loss: 5.803201216753912e-11\n",
      "Epoch 04476: reducing learning rate of group 0 to 1.0008e-11.\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4475:\n",
      "train loss: 5.308658233360938e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4476:\n",
      "train loss: 4.1817604513637734e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4477:\n",
      "train loss: 3.7035935482105484e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4478:\n",
      "train loss: 5.318417103062284e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4479:\n",
      "train loss: 4.8450504342000504e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4480:\n",
      "train loss: 4.1740698055568364e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4481:\n",
      "train loss: 3.698809786908355e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4482:\n",
      "train loss: 5.3207010332083634e-11\n",
      "lr: 1.0007752046919386e-11\n",
      "Epoch 4483:\n",
      "train loss: 4.844915237229631e-11\n",
      "Epoch 04485: reducing learning rate of group 0 to 9.5074e-12.\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4484:\n",
      "train loss: 4.176316148622286e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4485:\n",
      "train loss: 3.7028417301404774e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4486:\n",
      "train loss: 4.8643229828452677e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4487:\n",
      "train loss: 4.4111563536669664e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4488:\n",
      "train loss: 4.1598263468270786e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4489:\n",
      "train loss: 3.710721747401293e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4490:\n",
      "train loss: 4.856128713324441e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4491:\n",
      "train loss: 4.4029102409789464e-11\n",
      "lr: 9.507364444573416e-12\n",
      "Epoch 4492:\n",
      "train loss: 4.1681565408169085e-11\n",
      "Epoch 04494: reducing learning rate of group 0 to 9.0320e-12.\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4493:\n",
      "train loss: 3.719122186775996e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4494:\n",
      "train loss: 4.847844016580116e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4495:\n",
      "train loss: 4.4174727272323784e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4496:\n",
      "train loss: 3.7249845959547355e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4497:\n",
      "train loss: 3.298389762878991e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4498:\n",
      "train loss: 4.840140332207148e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4499:\n",
      "train loss: 4.409823731596326e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4500:\n",
      "train loss: 3.732360063540468e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4501:\n",
      "train loss: 3.305566911472009e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4502:\n",
      "train loss: 4.8330848468424336e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4503:\n",
      "train loss: 4.4031154884015536e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4504:\n",
      "train loss: 3.738613766782657e-11\n",
      "lr: 9.031996222344745e-12\n",
      "Epoch 4505:\n",
      "train loss: 3.3115007534122513e-11\n",
      "Epoch 04507: reducing learning rate of group 0 to 8.5804e-12.\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4506:\n",
      "train loss: 4.827462845112432e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4507:\n",
      "train loss: 4.397976219226525e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4508:\n",
      "train loss: 3.336344303722283e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4509:\n",
      "train loss: 2.9302597343727056e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4510:\n",
      "train loss: 4.802095213858648e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4511:\n",
      "train loss: 4.394228271775453e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4512:\n",
      "train loss: 3.3397661515444425e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4513:\n",
      "train loss: 2.933478942064613e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4514:\n",
      "train loss: 4.799154870862542e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4515:\n",
      "train loss: 4.39148062608881e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4516:\n",
      "train loss: 3.3421268674206956e-11\n",
      "lr: 8.580396411227507e-12\n",
      "Epoch 4517:\n",
      "train loss: 2.935620228993688e-11\n",
      "Epoch 04519: reducing learning rate of group 0 to 8.1514e-12.\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4518:\n",
      "train loss: 4.797103024848466e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4519:\n",
      "train loss: 4.389676215520261e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4520:\n",
      "train loss: 2.957024454178491e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4521:\n",
      "train loss: 2.5707080632384782e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4522:\n",
      "train loss: 4.775429963064284e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4523:\n",
      "train loss: 4.388525150403362e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4524:\n",
      "train loss: 2.958115008360247e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4525:\n",
      "train loss: 2.571659007901471e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4526:\n",
      "train loss: 4.7744505239044583e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4527:\n",
      "train loss: 4.387649923445341e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4528:\n",
      "train loss: 2.9588637088035204e-11\n",
      "lr: 8.151376590666132e-12\n",
      "Epoch 4529:\n",
      "train loss: 2.5723064776866435e-11\n",
      "Epoch 04531: reducing learning rate of group 0 to 7.7438e-12.\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4530:\n",
      "train loss: 4.773881447211252e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4531:\n",
      "train loss: 4.387160334478742e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4532:\n",
      "train loss: 2.5918828236769656e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4533:\n",
      "train loss: 2.2246879703233086e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4534:\n",
      "train loss: 4.754116721088349e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4535:\n",
      "train loss: 4.3867256167819603e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4536:\n",
      "train loss: 2.5921308636138602e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4537:\n",
      "train loss: 2.224948205394219e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4538:\n",
      "train loss: 4.7539132874084695e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4539:\n",
      "train loss: 4.386556816289631e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4540:\n",
      "train loss: 2.592349825201696e-11\n",
      "lr: 7.743807761132825e-12\n",
      "Epoch 4541:\n",
      "train loss: 2.2250709418770903e-11\n",
      "Epoch 04543: reducing learning rate of group 0 to 7.3566e-12.\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4542:\n",
      "train loss: 4.75372053137778e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4543:\n",
      "train loss: 4.386358398614474e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4544:\n",
      "train loss: 2.243567934261464e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4545:\n",
      "train loss: 1.894663215078782e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4546:\n",
      "train loss: 4.7351384791595995e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4547:\n",
      "train loss: 4.386183816239374e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4548:\n",
      "train loss: 2.2436914215655635e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4549:\n",
      "train loss: 1.8947698854258676e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4550:\n",
      "train loss: 4.734941200728439e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4551:\n",
      "train loss: 4.385945389128623e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4552:\n",
      "train loss: 2.2439005042624668e-11\n",
      "lr: 7.356617373076184e-12\n",
      "Epoch 4553:\n",
      "train loss: 1.895031691436593e-11\n",
      "Epoch 04555: reducing learning rate of group 0 to 6.9888e-12.\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4554:\n",
      "train loss: 4.7344960946686674e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4555:\n",
      "train loss: 4.385451987820577e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4556:\n",
      "train loss: 1.9129843897344004e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4557:\n",
      "train loss: 1.5817534185384664e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4558:\n",
      "train loss: 4.716019206889732e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4559:\n",
      "train loss: 4.384053142642401e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4560:\n",
      "train loss: 1.914584997428783e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4561:\n",
      "train loss: 1.583914057589759e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4562:\n",
      "train loss: 4.71256888781259e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4563:\n",
      "train loss: 4.379744910376312e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4564:\n",
      "train loss: 1.919741150098447e-11\n",
      "lr: 6.988786504422374e-12\n",
      "Epoch 4565:\n",
      "train loss: 1.5909304411531976e-11\n",
      "Epoch 04567: reducing learning rate of group 0 to 6.6393e-12.\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4566:\n",
      "train loss: 4.702010535242172e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4567:\n",
      "train loss: 4.366243961928583e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4568:\n",
      "train loss: 1.62095437628151e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4569:\n",
      "train loss: 1.317665902597021e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4570:\n",
      "train loss: 4.640515322157624e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4571:\n",
      "train loss: 4.305584062046918e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4572:\n",
      "train loss: 1.695890170273862e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4573:\n",
      "train loss: 1.4265520062071392e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4574:\n",
      "train loss: 4.462369939076853e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4575:\n",
      "train loss: 4.072115644933115e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4576:\n",
      "train loss: 1.9770175660880575e-11\n",
      "lr: 6.639347179201255e-12\n",
      "Epoch 4577:\n",
      "train loss: 1.7836857843867274e-11\n",
      "Epoch 04579: reducing learning rate of group 0 to 6.3074e-12.\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4578:\n",
      "train loss: 3.9840043326653515e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4579:\n",
      "train loss: 3.4890989543157626e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4580:\n",
      "train loss: 2.3416011075835614e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4581:\n",
      "train loss: 2.232698429989971e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4582:\n",
      "train loss: 3.189379549767923e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4583:\n",
      "train loss: 2.660407847613563e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4584:\n",
      "train loss: 3.1898071909197385e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4585:\n",
      "train loss: 3.037800574566976e-11\n",
      "lr: 6.307379820241192e-12\n",
      "Epoch 4586:\n",
      "train loss: 2.5208263944463836e-11\n",
      "Epoch 04588: reducing learning rate of group 0 to 5.9920e-12.\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4587:\n",
      "train loss: 2.1604323999038163e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4588:\n",
      "train loss: 3.453865191191723e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4589:\n",
      "train loss: 3.072598513398891e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4590:\n",
      "train loss: 2.4436169766447187e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4591:\n",
      "train loss: 2.2955419545792066e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4592:\n",
      "train loss: 2.917701021846079e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4593:\n",
      "train loss: 2.4334076615393792e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4594:\n",
      "train loss: 3.11947790821761e-11\n",
      "lr: 5.992010829229132e-12\n",
      "Epoch 4595:\n",
      "train loss: 2.932998997900988e-11\n",
      "Epoch 04597: reducing learning rate of group 0 to 5.6924e-12.\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4596:\n",
      "train loss: 2.410570728930265e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4597:\n",
      "train loss: 2.0970642994920962e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4598:\n",
      "train loss: 3.001678418598847e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4599:\n",
      "train loss: 2.6634311470019414e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4600:\n",
      "train loss: 2.543528919596338e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4601:\n",
      "train loss: 2.3326214179127387e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4602:\n",
      "train loss: 2.7394422449130055e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4603:\n",
      "train loss: 2.3989854444894704e-11\n",
      "lr: 5.6924102877676755e-12\n",
      "Epoch 4604:\n",
      "train loss: 2.795317584918549e-11\n",
      "Epoch 04606: reducing learning rate of group 0 to 5.4078e-12.\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4605:\n",
      "train loss: 2.5700916795673304e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4606:\n",
      "train loss: 2.524591060507691e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4607:\n",
      "train loss: 2.2334420675232792e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4608:\n",
      "train loss: 2.6711928529538945e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4609:\n",
      "train loss: 2.438861922072739e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4610:\n",
      "train loss: 2.4136272380745656e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4611:\n",
      "train loss: 2.1382391601162727e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4612:\n",
      "train loss: 2.7516774161493144e-11\n",
      "lr: 5.407789773379292e-12\n",
      "Epoch 4613:\n",
      "train loss: 2.5102518565149655e-11\n",
      "Epoch 04615: reducing learning rate of group 0 to 5.1374e-12.\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4614:\n",
      "train loss: 2.348823582156485e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4615:\n",
      "train loss: 2.0801810389818063e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4616:\n",
      "train loss: 2.558997629999579e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4617:\n",
      "train loss: 2.3240758432519918e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4618:\n",
      "train loss: 2.2975742003929808e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4619:\n",
      "train loss: 2.0465338986321708e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4620:\n",
      "train loss: 2.588803088646117e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4621:\n",
      "train loss: 2.348582163939413e-11\n",
      "lr: 5.137400284710327e-12\n",
      "Epoch 4622:\n",
      "train loss: 2.279095655807449e-11\n",
      "Epoch 04624: reducing learning rate of group 0 to 4.8805e-12.\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4623:\n",
      "train loss: 2.0324789715971038e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4624:\n",
      "train loss: 2.5992568216832642e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4625:\n",
      "train loss: 2.3663369125420742e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4626:\n",
      "train loss: 2.0357994411275302e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4627:\n",
      "train loss: 1.8070133406342e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4628:\n",
      "train loss: 2.587617444960511e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4629:\n",
      "train loss: 2.3493787497328106e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4630:\n",
      "train loss: 2.0572422874296254e-11\n",
      "lr: 4.88053027047481e-12\n",
      "Epoch 4631:\n",
      "train loss: 1.8321543613032522e-11\n",
      "Epoch 04633: reducing learning rate of group 0 to 4.6365e-12.\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4632:\n",
      "train loss: 2.5603997250498247e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4633:\n",
      "train loss: 2.321084625151476e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4634:\n",
      "train loss: 1.8659421266922675e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4635:\n",
      "train loss: 1.6530123799284564e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4636:\n",
      "train loss: 2.519308483526727e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4637:\n",
      "train loss: 2.2918738589753632e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4638:\n",
      "train loss: 1.894686503232272e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4639:\n",
      "train loss: 1.681075106305711e-11\n",
      "lr: 4.636503756951069e-12\n",
      "Epoch 4640:\n",
      "train loss: 2.4923279946196373e-11\n",
      "Epoch 04642: reducing learning rate of group 0 to 4.4047e-12.\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4641:\n",
      "train loss: 2.2666019982361835e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4642:\n",
      "train loss: 1.918158576226327e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4643:\n",
      "train loss: 1.7136750204627726e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4644:\n",
      "train loss: 2.2527712778848704e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4645:\n",
      "train loss: 2.040057278545793e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4646:\n",
      "train loss: 1.9338098038756984e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4647:\n",
      "train loss: 1.7277859721848505e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4648:\n",
      "train loss: 2.239918370190036e-11\n",
      "lr: 4.404678569103516e-12\n",
      "Epoch 4649:\n",
      "train loss: 2.0285092729682594e-11\n",
      "Epoch 04651: reducing learning rate of group 0 to 4.1844e-12.\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4650:\n",
      "train loss: 1.9441255575647135e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4651:\n",
      "train loss: 1.7371779887599183e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4652:\n",
      "train loss: 2.0330897782362393e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4653:\n",
      "train loss: 1.8329931357479645e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4654:\n",
      "train loss: 1.940349004569502e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4655:\n",
      "train loss: 1.7431189901243912e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4656:\n",
      "train loss: 2.027792688987708e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4657:\n",
      "train loss: 1.8282384762874325e-11\n",
      "lr: 4.18444464064834e-12\n",
      "Epoch 4658:\n",
      "train loss: 1.9447238584583586e-11\n",
      "Epoch 04660: reducing learning rate of group 0 to 3.9752e-12.\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4659:\n",
      "train loss: 1.7470567785830568e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4660:\n",
      "train loss: 2.0241763939144128e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4661:\n",
      "train loss: 1.8349337799540865e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4662:\n",
      "train loss: 1.7490026838759213e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4663:\n",
      "train loss: 1.5610139101314898e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4664:\n",
      "train loss: 2.0217117232005304e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4665:\n",
      "train loss: 1.832620382329346e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4666:\n",
      "train loss: 1.7511410707100492e-11\n",
      "lr: 3.975222408615923e-12\n",
      "Epoch 4667:\n",
      "train loss: 1.562960616681387e-11\n",
      "Epoch 04669: reducing learning rate of group 0 to 3.7765e-12.\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4668:\n",
      "train loss: 2.0198980900024633e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4669:\n",
      "train loss: 1.8309328323116696e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4670:\n",
      "train loss: 1.573442859074003e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4671:\n",
      "train loss: 1.3945681062523767e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4672:\n",
      "train loss: 2.0092470467751856e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4673:\n",
      "train loss: 1.8297564538471802e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4674:\n",
      "train loss: 1.574458291380455e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4675:\n",
      "train loss: 1.3955263976981044e-11\n",
      "lr: 3.776461288185127e-12\n",
      "Epoch 4676:\n",
      "train loss: 2.008279029357784e-11\n",
      "Epoch 04678: reducing learning rate of group 0 to 3.5876e-12.\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4677:\n",
      "train loss: 1.828972794431842e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4678:\n",
      "train loss: 1.575119014232232e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4679:\n",
      "train loss: 1.405115126889435e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4680:\n",
      "train loss: 1.8286258173934425e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4681:\n",
      "train loss: 1.658293427000601e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4682:\n",
      "train loss: 1.5755985687518758e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4683:\n",
      "train loss: 1.405498377072739e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4684:\n",
      "train loss: 1.8281660661824984e-11\n",
      "lr: 3.5876382237758705e-12\n",
      "Epoch 4685:\n",
      "train loss: 1.6579019443509392e-11\n",
      "Epoch 04687: reducing learning rate of group 0 to 3.4083e-12.\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4686:\n",
      "train loss: 1.575906854988785e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4687:\n",
      "train loss: 1.4058145079172363e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4688:\n",
      "train loss: 1.666247843137506e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4689:\n",
      "train loss: 1.5044681739144676e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4690:\n",
      "train loss: 1.567562639392979e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4691:\n",
      "train loss: 1.405981423327979e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4692:\n",
      "train loss: 1.6660779554229122e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4693:\n",
      "train loss: 1.50436480397742e-11\n",
      "lr: 3.4082563125870767e-12\n",
      "Epoch 4694:\n",
      "train loss: 1.5677196349030925e-11\n",
      "Epoch 04696: reducing learning rate of group 0 to 3.2378e-12.\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4695:\n",
      "train loss: 1.4060696590899218e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4696:\n",
      "train loss: 1.665963448711959e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4697:\n",
      "train loss: 1.5123245451195676e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4698:\n",
      "train loss: 1.4061268700282796e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4699:\n",
      "train loss: 1.2525651959575918e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4700:\n",
      "train loss: 1.665822473888213e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4701:\n",
      "train loss: 1.512219259758045e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4702:\n",
      "train loss: 1.406139841025981e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4703:\n",
      "train loss: 1.2525713347180367e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4704:\n",
      "train loss: 1.6657932690443863e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4705:\n",
      "train loss: 1.512139725381278e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4706:\n",
      "train loss: 1.406220128865431e-11\n",
      "lr: 3.237843496957723e-12\n",
      "Epoch 4707:\n",
      "train loss: 1.2526258047784902e-11\n",
      "Epoch 04709: reducing learning rate of group 0 to 3.0760e-12.\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4708:\n",
      "train loss: 1.665711782470185e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4709:\n",
      "train loss: 1.512150893298878e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4710:\n",
      "train loss: 1.2603357675205022e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4711:\n",
      "train loss: 1.1144355711588484e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4712:\n",
      "train loss: 1.6579772708409503e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4713:\n",
      "train loss: 1.5120945200145178e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4714:\n",
      "train loss: 1.2603767698343913e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4715:\n",
      "train loss: 1.1144766075814575e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4716:\n",
      "train loss: 1.6579216205340574e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4717:\n",
      "train loss: 1.51202839450322e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4718:\n",
      "train loss: 1.2603949007695397e-11\n",
      "lr: 3.0759513221098366e-12\n",
      "Epoch 4719:\n",
      "train loss: 1.114460924967185e-11\n",
      "Epoch 04721: reducing learning rate of group 0 to 2.9222e-12.\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4720:\n",
      "train loss: 1.6579033059115264e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4721:\n",
      "train loss: 1.5120472170140386e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4722:\n",
      "train loss: 1.121761030694868e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4723:\n",
      "train loss: 9.83114423253721e-12\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4724:\n",
      "train loss: 1.6506201862270862e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4725:\n",
      "train loss: 1.5120157734504064e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4726:\n",
      "train loss: 1.1216896821633282e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4727:\n",
      "train loss: 9.831151660745127e-12\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4728:\n",
      "train loss: 1.650614062504811e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4729:\n",
      "train loss: 1.5120274206373393e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4730:\n",
      "train loss: 1.1217166662125985e-11\n",
      "lr: 2.9221537560043447e-12\n",
      "Epoch 4731:\n",
      "train loss: 9.830587464377472e-12\n",
      "Epoch 04733: reducing learning rate of group 0 to 2.7760e-12.\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4732:\n",
      "train loss: 1.6505831439231565e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4733:\n",
      "train loss: 1.512020580773822e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4734:\n",
      "train loss: 9.900745341962895e-12\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4735:\n",
      "train loss: 8.58347860005761e-12\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4736:\n",
      "train loss: 1.6436512831069156e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4737:\n",
      "train loss: 1.5119490129912003e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4738:\n",
      "train loss: 9.900841703827163e-12\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4739:\n",
      "train loss: 8.583519437100849e-12\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4740:\n",
      "train loss: 1.643621009024454e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4741:\n",
      "train loss: 1.5119397332418142e-11\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4742:\n",
      "train loss: 9.900053747700632e-12\n",
      "lr: 2.7760460682041275e-12\n",
      "Epoch 4743:\n",
      "train loss: 8.58380799664807e-12\n",
      "Epoch 04745: reducing learning rate of group 0 to 2.6372e-12.\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4744:\n",
      "train loss: 1.6435961733239767e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4745:\n",
      "train loss: 1.5119544515541412e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4746:\n",
      "train loss: 8.649432606277782e-12\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4747:\n",
      "train loss: 7.398878878687891e-12\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4748:\n",
      "train loss: 1.6368961135576307e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4749:\n",
      "train loss: 1.5119317586587934e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4750:\n",
      "train loss: 8.64894086390204e-12\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4751:\n",
      "train loss: 7.398598195511826e-12\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4752:\n",
      "train loss: 1.6369129511567784e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4753:\n",
      "train loss: 1.5119583358784347e-11\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4754:\n",
      "train loss: 8.648980850120437e-12\n",
      "lr: 2.637243764793921e-12\n",
      "Epoch 4755:\n",
      "train loss: 7.398252905129855e-12\n",
      "Epoch 04757: reducing learning rate of group 0 to 2.5054e-12.\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4756:\n",
      "train loss: 1.6369426028305215e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4757:\n",
      "train loss: 1.5118857668782296e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4758:\n",
      "train loss: 7.460680617297734e-12\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4759:\n",
      "train loss: 6.272576421959192e-12\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4760:\n",
      "train loss: 1.6306261109314558e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4761:\n",
      "train loss: 1.5118107043744207e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4762:\n",
      "train loss: 7.46144709493875e-12\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4763:\n",
      "train loss: 6.273851748469019e-12\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4764:\n",
      "train loss: 1.6304630001169697e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4765:\n",
      "train loss: 1.5115827465988314e-11\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4766:\n",
      "train loss: 7.463929681847665e-12\n",
      "lr: 2.5053815765542248e-12\n",
      "Epoch 4767:\n",
      "train loss: 6.2766375177202524e-12\n",
      "Epoch 04769: reducing learning rate of group 0 to 2.3801e-12.\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4768:\n",
      "train loss: 1.630095551215319e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4769:\n",
      "train loss: 1.5111610235678877e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4770:\n",
      "train loss: 6.340315244206191e-12\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4771:\n",
      "train loss: 5.214487546376401e-12\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4772:\n",
      "train loss: 1.6228879387154965e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4773:\n",
      "train loss: 1.509533272005447e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4774:\n",
      "train loss: 6.360610444054419e-12\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4775:\n",
      "train loss: 5.243034418809226e-12\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4776:\n",
      "train loss: 1.618425810330055e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4777:\n",
      "train loss: 1.503654748499443e-11\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4778:\n",
      "train loss: 6.431879289355232e-12\n",
      "lr: 2.3801124977265133e-12\n",
      "Epoch 4779:\n",
      "train loss: 5.3428839590281725e-12\n",
      "Epoch 04781: reducing learning rate of group 0 to 2.2611e-12.\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4780:\n",
      "train loss: 1.602660866481947e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4781:\n",
      "train loss: 1.4830562933879564e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4782:\n",
      "train loss: 5.6100256656693865e-12\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4783:\n",
      "train loss: 4.714633662116262e-12\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4784:\n",
      "train loss: 1.5280846224029725e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4785:\n",
      "train loss: 1.3899601876854919e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4786:\n",
      "train loss: 6.7558859091651825e-12\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4787:\n",
      "train loss: 6.1811046534143086e-12\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4788:\n",
      "train loss: 1.332009334085487e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4789:\n",
      "train loss: 1.1501499311018936e-11\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4790:\n",
      "train loss: 9.498696856556167e-12\n",
      "lr: 2.2611068728401875e-12\n",
      "Epoch 4791:\n",
      "train loss: 9.099398685917255e-12\n",
      "Epoch 04793: reducing learning rate of group 0 to 2.1481e-12.\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4792:\n",
      "train loss: 1.0506195521093959e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4793:\n",
      "train loss: 8.76393105791831e-12\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4794:\n",
      "train loss: 1.100458086420471e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4795:\n",
      "train loss: 1.028655857065102e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4796:\n",
      "train loss: 8.857168532451106e-12\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4797:\n",
      "train loss: 7.733865922064427e-12\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4798:\n",
      "train loss: 1.1434466625755652e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4799:\n",
      "train loss: 1.0095353267546088e-11\n",
      "lr: 2.148051529198178e-12\n",
      "Epoch 4800:\n",
      "train loss: 9.604786008607701e-12\n",
      "Epoch 04802: reducing learning rate of group 0 to 2.0406e-12.\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4801:\n",
      "train loss: 8.85879234677032e-12\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4802:\n",
      "train loss: 1.0212012897863982e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4803:\n",
      "train loss: 8.891871355725297e-12\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4804:\n",
      "train loss: 9.83189085492287e-12\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4805:\n",
      "train loss: 9.084658223891871e-12\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4806:\n",
      "train loss: 9.137078085505318e-12\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4807:\n",
      "train loss: 7.985203986562233e-12\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4808:\n",
      "train loss: 1.0547175225593304e-11\n",
      "lr: 2.0406489527382687e-12\n",
      "Epoch 4809:\n",
      "train loss: 9.655850617321565e-12\n",
      "Epoch 04811: reducing learning rate of group 0 to 1.9386e-12.\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4810:\n",
      "train loss: 8.684204708747874e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4811:\n",
      "train loss: 7.670667098286539e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4812:\n",
      "train loss: 9.831020878416006e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4813:\n",
      "train loss: 8.929608014103457e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4814:\n",
      "train loss: 8.524637014514045e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4815:\n",
      "train loss: 7.593455401286309e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4816:\n",
      "train loss: 9.884360906306972e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4817:\n",
      "train loss: 8.974182530175179e-12\n",
      "lr: 1.9386165051013554e-12\n",
      "Epoch 4818:\n",
      "train loss: 8.483146527829661e-12\n",
      "Epoch 04820: reducing learning rate of group 0 to 1.8417e-12.\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4819:\n",
      "train loss: 7.5555261680046e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4820:\n",
      "train loss: 9.916733779125877e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4821:\n",
      "train loss: 9.045056875322806e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4822:\n",
      "train loss: 7.552806255858206e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4823:\n",
      "train loss: 6.68266746577086e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4824:\n",
      "train loss: 9.901323488914627e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4825:\n",
      "train loss: 9.009069584802614e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4826:\n",
      "train loss: 7.613399544753253e-12\n",
      "lr: 1.8416856798462876e-12\n",
      "Epoch 4827:\n",
      "train loss: 6.761653595680822e-12\n",
      "Epoch 04829: reducing learning rate of group 0 to 1.7496e-12.\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4828:\n",
      "train loss: 9.805366976857534e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4829:\n",
      "train loss: 8.893760067471268e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4830:\n",
      "train loss: 6.916607366197002e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4831:\n",
      "train loss: 6.127314424806424e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4832:\n",
      "train loss: 9.591751819762415e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4833:\n",
      "train loss: 8.706296756062115e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4834:\n",
      "train loss: 7.120493409949083e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4835:\n",
      "train loss: 6.342278535120656e-12\n",
      "lr: 1.749601395853973e-12\n",
      "Epoch 4836:\n",
      "train loss: 9.373064502676424e-12\n",
      "Epoch 04838: reducing learning rate of group 0 to 1.6621e-12.\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4837:\n",
      "train loss: 8.48810911419234e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4838:\n",
      "train loss: 7.336881749481384e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4839:\n",
      "train loss: 6.593994868629974e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4840:\n",
      "train loss: 8.346346050731174e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4841:\n",
      "train loss: 7.516469287602908e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4842:\n",
      "train loss: 7.503197660103838e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4843:\n",
      "train loss: 6.747861033479292e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4844:\n",
      "train loss: 8.204420807541988e-12\n",
      "lr: 1.6621213260612743e-12\n",
      "Epoch 4845:\n",
      "train loss: 7.387821672565667e-12\n",
      "Epoch 04847: reducing learning rate of group 0 to 1.5790e-12.\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4846:\n",
      "train loss: 7.619655286375222e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4847:\n",
      "train loss: 6.853805503473495e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4848:\n",
      "train loss: 7.359781206981416e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4849:\n",
      "train loss: 6.593146354215603e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4850:\n",
      "train loss: 7.655300103855791e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4851:\n",
      "train loss: 6.920007012082061e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4852:\n",
      "train loss: 7.300148869082269e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4853:\n",
      "train loss: 6.539167524348352e-12\n",
      "lr: 1.5790152597582105e-12\n",
      "Epoch 4854:\n",
      "train loss: 7.703539003366833e-12\n",
      "Epoch 04856: reducing learning rate of group 0 to 1.5001e-12.\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4855:\n",
      "train loss: 6.9631928476360915e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4856:\n",
      "train loss: 7.2617409501156e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4857:\n",
      "train loss: 6.542519943099113e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4858:\n",
      "train loss: 6.984913659441463e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4859:\n",
      "train loss: 6.278381437321971e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4860:\n",
      "train loss: 7.237648195898975e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4861:\n",
      "train loss: 6.520928435908172e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4862:\n",
      "train loss: 7.003991009400282e-12\n",
      "lr: 1.5000644967703e-12\n",
      "Epoch 4863:\n",
      "train loss: 6.2964754636621815e-12\n",
      "Epoch 04865: reducing learning rate of group 0 to 1.4251e-12.\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4864:\n",
      "train loss: 7.220821625458194e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4865:\n",
      "train loss: 6.505879096393627e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4866:\n",
      "train loss: 6.342001920095623e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4867:\n",
      "train loss: 5.668443407282665e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4868:\n",
      "train loss: 7.1735861275974846e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4869:\n",
      "train loss: 6.495888844203172e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4870:\n",
      "train loss: 6.351483370013883e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4871:\n",
      "train loss: 5.6772570721932186e-12\n",
      "lr: 1.4250612719317849e-12\n",
      "Epoch 4872:\n",
      "train loss: 7.166297917646794e-12\n",
      "Epoch 04874: reducing learning rate of group 0 to 1.3538e-12.\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4873:\n",
      "train loss: 6.488500954971419e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4874:\n",
      "train loss: 6.3579387377198846e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4875:\n",
      "train loss: 5.716959222575438e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4876:\n",
      "train loss: 6.48475853160744e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4877:\n",
      "train loss: 5.8415019373219025e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4878:\n",
      "train loss: 6.361807792713451e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4879:\n",
      "train loss: 5.7204251364236136e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4880:\n",
      "train loss: 6.481257936954682e-12\n",
      "lr: 1.3538082083351955e-12\n",
      "Epoch 4881:\n",
      "train loss: 5.8386321311456706e-12\n",
      "Epoch 04883: reducing learning rate of group 0 to 1.2861e-12.\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4882:\n",
      "train loss: 6.363697729551058e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4883:\n",
      "train loss: 5.721917901343748e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4884:\n",
      "train loss: 5.869417760335905e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4885:\n",
      "train loss: 5.258754841588469e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4886:\n",
      "train loss: 6.333778602567103e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4887:\n",
      "train loss: 5.723590620549238e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4888:\n",
      "train loss: 5.867975634407944e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4889:\n",
      "train loss: 5.25769237595769e-12\n",
      "lr: 1.2861177979184357e-12\n",
      "Epoch 4890:\n",
      "train loss: 6.33519567331425e-12\n",
      "Epoch 04892: reducing learning rate of group 0 to 1.2218e-12.\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4891:\n",
      "train loss: 5.724691289974461e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4892:\n",
      "train loss: 5.867262581492107e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4893:\n",
      "train loss: 5.287209976107165e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4894:\n",
      "train loss: 5.724930568414009e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4895:\n",
      "train loss: 5.145812649265385e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4896:\n",
      "train loss: 5.867018420054292e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4897:\n",
      "train loss: 5.287315427200264e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4898:\n",
      "train loss: 5.72510099333447e-12\n",
      "lr: 1.2218119080225139e-12\n",
      "Epoch 4899:\n",
      "train loss: 5.145966265908808e-12\n",
      "Epoch 04901: reducing learning rate of group 0 to 1.1607e-12.\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4900:\n",
      "train loss: 5.8665110210698294e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4901:\n",
      "train loss: 5.287018083973853e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4902:\n",
      "train loss: 5.17504996896509e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4903:\n",
      "train loss: 4.6243658545351195e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4904:\n",
      "train loss: 5.836838535648835e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4905:\n",
      "train loss: 5.2863432236963174e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4906:\n",
      "train loss: 5.175396195428716e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4907:\n",
      "train loss: 4.62431691305221e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4908:\n",
      "train loss: 5.836623179325806e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4909:\n",
      "train loss: 5.286111466222043e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4910:\n",
      "train loss: 5.1754016229921524e-12\n",
      "lr: 1.160721312621388e-12\n",
      "Epoch 4911:\n",
      "train loss: 4.624234183788064e-12\n",
      "Epoch 04913: reducing learning rate of group 0 to 1.1027e-12.\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4912:\n",
      "train loss: 5.836149955449182e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4913:\n",
      "train loss: 5.285536772414218e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4914:\n",
      "train loss: 4.6524574858967135e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4915:\n",
      "train loss: 4.129699226000745e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4916:\n",
      "train loss: 5.808491191928548e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4917:\n",
      "train loss: 5.286110530978996e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4918:\n",
      "train loss: 4.652090728725163e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4919:\n",
      "train loss: 4.129905893669042e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4920:\n",
      "train loss: 5.808343578674779e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4921:\n",
      "train loss: 5.285647404215645e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4922:\n",
      "train loss: 4.652012916457036e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4923:\n",
      "train loss: 4.1291415268693595e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4924:\n",
      "train loss: 5.808450287424247e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4925:\n",
      "train loss: 5.28553248108256e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4926:\n",
      "train loss: 4.65231719219263e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4927:\n",
      "train loss: 4.129840875625814e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4928:\n",
      "train loss: 5.808322932465507e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4929:\n",
      "train loss: 5.285870934478809e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4930:\n",
      "train loss: 4.652484907207408e-12\n",
      "lr: 1.1026852469903186e-12\n",
      "Epoch 4931:\n",
      "train loss: 4.129472023222615e-12\n",
      "Epoch 04933: reducing learning rate of group 0 to 1.0476e-12.\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4932:\n",
      "train loss: 5.8086732844645735e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4933:\n",
      "train loss: 5.28580155743615e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4934:\n",
      "train loss: 4.155174045844826e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4935:\n",
      "train loss: 3.6589986812671215e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4936:\n",
      "train loss: 5.782762976376655e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4937:\n",
      "train loss: 5.285551063526832e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4938:\n",
      "train loss: 4.155468395048009e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4939:\n",
      "train loss: 3.658618769912492e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4940:\n",
      "train loss: 5.78317009161649e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4941:\n",
      "train loss: 5.2853495482018265e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4942:\n",
      "train loss: 4.155142767453676e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4943:\n",
      "train loss: 3.658268174340449e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4944:\n",
      "train loss: 5.782583633961786e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4945:\n",
      "train loss: 5.285210738529252e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4946:\n",
      "train loss: 4.155391618091951e-12\n",
      "lr: 1.0475509846408026e-12\n",
      "Epoch 4947:\n",
      "train loss: 3.658444071010297e-12\n",
      "Epoch 04949: reducing learning rate of group 0 to 9.9517e-13.\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4948:\n",
      "train loss: 5.781966265791657e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4949:\n",
      "train loss: 5.285697998449709e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4950:\n",
      "train loss: 3.683757452443675e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4951:\n",
      "train loss: 3.212202516002807e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4952:\n",
      "train loss: 5.757340005648737e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4953:\n",
      "train loss: 5.285549268917102e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4954:\n",
      "train loss: 3.683438137901577e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4955:\n",
      "train loss: 3.2120379614004687e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4956:\n",
      "train loss: 5.75713385427385e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4957:\n",
      "train loss: 5.285467834110724e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4958:\n",
      "train loss: 3.683685520061558e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4959:\n",
      "train loss: 3.2118518468467634e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4960:\n",
      "train loss: 5.757482352545597e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4961:\n",
      "train loss: 5.285603605093037e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4962:\n",
      "train loss: 3.6836047662851955e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4963:\n",
      "train loss: 3.2114745844547367e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4964:\n",
      "train loss: 5.757282717592178e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4965:\n",
      "train loss: 5.28537637595305e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4966:\n",
      "train loss: 3.6829498411276055e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4967:\n",
      "train loss: 3.211887708744418e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4968:\n",
      "train loss: 5.757436008028699e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4969:\n",
      "train loss: 5.285086493067133e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4970:\n",
      "train loss: 3.683595924310937e-12\n",
      "lr: 9.951734354087625e-13\n",
      "Epoch 4971:\n",
      "train loss: 3.2113436364640767e-12\n",
      "Epoch 04973: reducing learning rate of group 0 to 9.4541e-13.\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4972:\n",
      "train loss: 5.757715583129707e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4973:\n",
      "train loss: 5.285898299400101e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4974:\n",
      "train loss: 3.2349141470022405e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4975:\n",
      "train loss: 2.785776229692317e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4976:\n",
      "train loss: 5.733210942161682e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4977:\n",
      "train loss: 5.285643725768018e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4978:\n",
      "train loss: 3.2347041587082153e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4979:\n",
      "train loss: 2.7860676120318213e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4980:\n",
      "train loss: 5.733478066485445e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4981:\n",
      "train loss: 5.285441801578699e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4982:\n",
      "train loss: 3.234609137581002e-12\n",
      "lr: 9.454147636383243e-13\n",
      "Epoch 4983:\n",
      "train loss: 2.7857800420366114e-12\n",
      "Epoch 04985: reducing learning rate of group 0 to 8.9814e-13.\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4984:\n",
      "train loss: 5.733101220644204e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4985:\n",
      "train loss: 5.284641699678975e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4986:\n",
      "train loss: 2.809411859952817e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4987:\n",
      "train loss: 2.383282134745584e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4988:\n",
      "train loss: 5.710305130983914e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4989:\n",
      "train loss: 5.284293295582589e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4990:\n",
      "train loss: 2.8096970619094213e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4991:\n",
      "train loss: 2.3840890222019748e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4992:\n",
      "train loss: 5.709372483674976e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4993:\n",
      "train loss: 5.282510771776012e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4994:\n",
      "train loss: 2.8112534953299965e-12\n",
      "lr: 8.981440254564081e-13\n",
      "Epoch 4995:\n",
      "train loss: 2.3854290230393535e-12\n",
      "Epoch 04997: reducing learning rate of group 0 to 8.5324e-13.\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 4996:\n",
      "train loss: 5.706955376843868e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 4997:\n",
      "train loss: 5.280294799386493e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 4998:\n",
      "train loss: 2.4086073169980174e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 4999:\n",
      "train loss: 2.005551436728775e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5000:\n",
      "train loss: 5.6808233741011854e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5001:\n",
      "train loss: 5.274216754018915e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5002:\n",
      "train loss: 2.417060002105243e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5003:\n",
      "train loss: 2.016326612979498e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5004:\n",
      "train loss: 5.666212308455616e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5005:\n",
      "train loss: 5.2551431202653514e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5006:\n",
      "train loss: 2.4391447334413547e-12\n",
      "lr: 8.532368241835876e-13\n",
      "Epoch 5007:\n",
      "train loss: 2.044416652490721e-12\n",
      "Epoch 05009: reducing learning rate of group 0 to 8.1057e-13.\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5008:\n",
      "train loss: 5.624831642122635e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5009:\n",
      "train loss: 5.2038266564661195e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5010:\n",
      "train loss: 2.1165005887537276e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5011:\n",
      "train loss: 1.7686185414388236e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5012:\n",
      "train loss: 5.46025544203586e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5013:\n",
      "train loss: 5.0132863336888734e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5014:\n",
      "train loss: 2.348869160904599e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5015:\n",
      "train loss: 2.0708005910475898e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5016:\n",
      "train loss: 5.0421920720851705e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5017:\n",
      "train loss: 4.494013377593154e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5018:\n",
      "train loss: 2.9544330019835986e-12\n",
      "lr: 8.105749829744081e-13\n",
      "Epoch 5019:\n",
      "train loss: 2.7508442012958766e-12\n",
      "Epoch 05021: reducing learning rate of group 0 to 7.7005e-13.\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5020:\n",
      "train loss: 4.30389553785778e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5021:\n",
      "train loss: 3.69345162864794e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5022:\n",
      "train loss: 3.428202653149997e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5023:\n",
      "train loss: 3.240010680967544e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5024:\n",
      "train loss: 3.514665121842971e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5025:\n",
      "train loss: 2.983395378085446e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5026:\n",
      "train loss: 4.0596166283124756e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5027:\n",
      "train loss: 3.759101619154862e-12\n",
      "lr: 7.700462338256877e-13\n",
      "Epoch 5028:\n",
      "train loss: 3.1416946670677378e-12\n",
      "Epoch 05030: reducing learning rate of group 0 to 7.3154e-13.\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5029:\n",
      "train loss: 2.764075253584503e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5030:\n",
      "train loss: 4.121759860649731e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5031:\n",
      "train loss: 3.6919258019826736e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5032:\n",
      "train loss: 3.001016263376006e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5033:\n",
      "train loss: 2.755892502726471e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5034:\n",
      "train loss: 3.7109374763872006e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5035:\n",
      "train loss: 3.223565193457412e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5036:\n",
      "train loss: 3.4980506783476197e-12\n",
      "lr: 7.315439221344033e-13\n",
      "Epoch 5037:\n",
      "train loss: 3.2534153262558898e-12\n",
      "Epoch 05039: reducing learning rate of group 0 to 6.9497e-13.\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5038:\n",
      "train loss: 3.2529698568937987e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5039:\n",
      "train loss: 2.8225936691347697e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5040:\n",
      "train loss: 3.5057965167995756e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5041:\n",
      "train loss: 3.2244496387532685e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5042:\n",
      "train loss: 3.0004732532444995e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5043:\n",
      "train loss: 2.635660066348096e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5044:\n",
      "train loss: 3.656308728149047e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5045:\n",
      "train loss: 3.3468776022595547e-12\n",
      "lr: 6.949667260276831e-13\n",
      "Epoch 5046:\n",
      "train loss: 2.8992946650578474e-12\n",
      "Epoch 05048: reducing learning rate of group 0 to 6.6022e-13.\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5047:\n",
      "train loss: 2.5552807781366063e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5048:\n",
      "train loss: 3.719831034177504e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5049:\n",
      "train loss: 3.416163395761673e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5050:\n",
      "train loss: 2.524661461049324e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5051:\n",
      "train loss: 2.2041227237903575e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5052:\n",
      "train loss: 3.751773160554106e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5053:\n",
      "train loss: 3.444459554381754e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5054:\n",
      "train loss: 2.4994960519202333e-12\n",
      "lr: 6.602183897262989e-13\n",
      "Epoch 5055:\n",
      "train loss: 2.181889679680371e-12\n",
      "Epoch 05057: reducing learning rate of group 0 to 6.2721e-13.\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5056:\n",
      "train loss: 3.7701808255501475e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5057:\n",
      "train loss: 3.459822758234611e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5058:\n",
      "train loss: 2.1910944666646026e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5059:\n",
      "train loss: 1.8937992255596642e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5060:\n",
      "train loss: 3.752928928367826e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5061:\n",
      "train loss: 3.4502093917558085e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5062:\n",
      "train loss: 2.208272288847247e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5063:\n",
      "train loss: 1.920525999020505e-12\n",
      "lr: 6.27207470239984e-13\n",
      "Epoch 5064:\n",
      "train loss: 3.7169792614873574e-12\n",
      "Epoch 05066: reducing learning rate of group 0 to 5.9585e-13.\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5065:\n",
      "train loss: 3.4028843432365943e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5066:\n",
      "train loss: 2.2653020529407555e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5067:\n",
      "train loss: 1.999870082037531e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5068:\n",
      "train loss: 3.3472095577537632e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5069:\n",
      "train loss: 3.0416328841844136e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5070:\n",
      "train loss: 2.3506461538851467e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5071:\n",
      "train loss: 2.09073405233449e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5072:\n",
      "train loss: 3.25277086301669e-12\n",
      "lr: 5.958470967279848e-13\n",
      "Epoch 5073:\n",
      "train loss: 2.9431315559804123e-12\n",
      "Epoch 05075: reducing learning rate of group 0 to 5.6605e-13.\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5074:\n",
      "train loss: 2.4522007501439743e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5075:\n",
      "train loss: 2.1933721842473498e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5076:\n",
      "train loss: 2.8852156756524674e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5077:\n",
      "train loss: 2.592928439688007e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5078:\n",
      "train loss: 2.5308271719951634e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5079:\n",
      "train loss: 2.2813740719240384e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5080:\n",
      "train loss: 2.8024651376317627e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5081:\n",
      "train loss: 2.5165223788039516e-12\n",
      "lr: 5.660547418915855e-13\n",
      "Epoch 5082:\n",
      "train loss: 2.602068061991261e-12\n",
      "Epoch 05084: reducing learning rate of group 0 to 5.3775e-13.\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5083:\n",
      "train loss: 2.3466644279034792e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5084:\n",
      "train loss: 2.7415967101573026e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5085:\n",
      "train loss: 2.4747547478725287e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5086:\n",
      "train loss: 2.3824830006290348e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5087:\n",
      "train loss: 2.137181788037655e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5088:\n",
      "train loss: 2.7012339438285832e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5089:\n",
      "train loss: 2.4374410337883186e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5090:\n",
      "train loss: 2.4159714029491944e-12\n",
      "lr: 5.377520047970062e-13\n",
      "Epoch 5091:\n",
      "train loss: 2.1678824369942207e-12\n",
      "Epoch 05093: reducing learning rate of group 0 to 5.1086e-13.\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5092:\n",
      "train loss: 2.6722492348303206e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5093:\n",
      "train loss: 2.4114330342431978e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5094:\n",
      "train loss: 2.1966151093597162e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5095:\n",
      "train loss: 1.958741638471883e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5096:\n",
      "train loss: 2.641077796371718e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5097:\n",
      "train loss: 2.3951781853887815e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5098:\n",
      "train loss: 2.2114432704080385e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5099:\n",
      "train loss: 1.9716186683819606e-12\n",
      "lr: 5.108644045571559e-13\n",
      "Epoch 5100:\n",
      "train loss: 2.6293990164239246e-12\n",
      "Epoch 05102: reducing learning rate of group 0 to 4.8532e-13.\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5101:\n",
      "train loss: 2.384704034555548e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5102:\n",
      "train loss: 2.221134287428145e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5103:\n",
      "train loss: 1.9934200279586888e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5104:\n",
      "train loss: 2.378974516554419e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5105:\n",
      "train loss: 2.146974187824104e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5106:\n",
      "train loss: 2.2270269703874486e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5107:\n",
      "train loss: 1.998547846578362e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5108:\n",
      "train loss: 2.3744414798550596e-12\n",
      "lr: 4.853211843292981e-13\n",
      "Epoch 5109:\n",
      "train loss: 2.1430797462552803e-12\n",
      "Epoch 05111: reducing learning rate of group 0 to 4.6106e-13.\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5110:\n",
      "train loss: 2.2318495726547436e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5111:\n",
      "train loss: 2.0026924696009425e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5112:\n",
      "train loss: 2.152159913751698e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5113:\n",
      "train loss: 1.9332199173985006e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5114:\n",
      "train loss: 2.222954814564295e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5115:\n",
      "train loss: 2.004842020676325e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5116:\n",
      "train loss: 2.1495335002012222e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5117:\n",
      "train loss: 1.9309964410512747e-12\n",
      "lr: 4.610551251128332e-13\n",
      "Epoch 5118:\n",
      "train loss: 2.2249203829302064e-12\n",
      "Epoch 05120: reducing learning rate of group 0 to 4.3800e-13.\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5119:\n",
      "train loss: 2.006640032029024e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5120:\n",
      "train loss: 2.1482557008698607e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5121:\n",
      "train loss: 1.9405290055080986e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5122:\n",
      "train loss: 2.00741774369781e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5123:\n",
      "train loss: 1.7999079293626618e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5124:\n",
      "train loss: 2.1471856094534863e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5125:\n",
      "train loss: 1.9397585755507458e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5126:\n",
      "train loss: 2.0084448724701253e-12\n",
      "lr: 4.380023688571915e-13\n",
      "Epoch 5127:\n",
      "train loss: 1.8004224215572566e-12\n",
      "Epoch 05129: reducing learning rate of group 0 to 4.1610e-13.\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5128:\n",
      "train loss: 2.146499854474582e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5129:\n",
      "train loss: 1.938765046514196e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5130:\n",
      "train loss: 1.8115036608155248e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5131:\n",
      "train loss: 1.6131904947677304e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5132:\n",
      "train loss: 2.13584282316725e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5133:\n",
      "train loss: 1.9384003523003385e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5134:\n",
      "train loss: 1.8112299193692704e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5135:\n",
      "train loss: 1.6136168549076486e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5136:\n",
      "train loss: 2.135838686185862e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5137:\n",
      "train loss: 1.9384682488435766e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5138:\n",
      "train loss: 1.8109866596672609e-12\n",
      "lr: 4.161022504143319e-13\n",
      "Epoch 5139:\n",
      "train loss: 1.6141755405332933e-12\n",
      "Epoch 05141: reducing learning rate of group 0 to 3.9530e-13.\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5140:\n",
      "train loss: 2.1350207151583956e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5141:\n",
      "train loss: 1.9379484220775626e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5142:\n",
      "train loss: 1.6238147477088818e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5143:\n",
      "train loss: 1.436436836336944e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5144:\n",
      "train loss: 2.1261441851615144e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5145:\n",
      "train loss: 1.9379719330892516e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5146:\n",
      "train loss: 1.6239714659132124e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5147:\n",
      "train loss: 1.436576060770946e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5148:\n",
      "train loss: 2.125785031624656e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5149:\n",
      "train loss: 1.9378508941188496e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5150:\n",
      "train loss: 1.6244540537414059e-12\n",
      "lr: 3.952971378936153e-13\n",
      "Epoch 5151:\n",
      "train loss: 1.4371491242393055e-12\n",
      "Epoch 05153: reducing learning rate of group 0 to 3.7553e-13.\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5152:\n",
      "train loss: 2.125037139350294e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5153:\n",
      "train loss: 1.937925055055864e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5154:\n",
      "train loss: 1.447041562804893e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5155:\n",
      "train loss: 1.2689691299593667e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5156:\n",
      "train loss: 2.115952810584929e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5157:\n",
      "train loss: 1.937334005052631e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5158:\n",
      "train loss: 1.4468868716442219e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5159:\n",
      "train loss: 1.2684693310656992e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5160:\n",
      "train loss: 2.115523138908251e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5161:\n",
      "train loss: 1.9372213077734436e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5162:\n",
      "train loss: 1.4473228024022417e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5163:\n",
      "train loss: 1.2684724125572923e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5164:\n",
      "train loss: 2.1154486297207935e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5165:\n",
      "train loss: 1.9371664185616618e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5166:\n",
      "train loss: 1.447097685753762e-12\n",
      "lr: 3.755322809989345e-13\n",
      "Epoch 5167:\n",
      "train loss: 1.2686865206340692e-12\n",
      "Epoch 05169: reducing learning rate of group 0 to 3.5676e-13.\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5168:\n",
      "train loss: 2.1153850500971326e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5169:\n",
      "train loss: 1.9372913130393247e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5170:\n",
      "train loss: 1.2776923407454776e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5171:\n",
      "train loss: 1.108019440367716e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5172:\n",
      "train loss: 2.106634131381529e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5173:\n",
      "train loss: 1.9373698062806157e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5174:\n",
      "train loss: 1.2781150360969878e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5175:\n",
      "train loss: 1.1077804171024622e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5176:\n",
      "train loss: 2.1065260533834252e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5177:\n",
      "train loss: 1.9369081975615417e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5178:\n",
      "train loss: 1.2776673586030338e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5179:\n",
      "train loss: 1.1087473421740743e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5180:\n",
      "train loss: 2.1064971714923115e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5181:\n",
      "train loss: 1.93713745687057e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5182:\n",
      "train loss: 1.277740814794366e-12\n",
      "lr: 3.5675566694898775e-13\n",
      "Epoch 5183:\n",
      "train loss: 1.1084094962167683e-12\n",
      "Epoch 05185: reducing learning rate of group 0 to 3.3892e-13.\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5184:\n",
      "train loss: 2.1060820468151755e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5185:\n",
      "train loss: 1.936799146877533e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5186:\n",
      "train loss: 1.1171724409537133e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5187:\n",
      "train loss: 9.559475439192731e-13\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5188:\n",
      "train loss: 2.0972268465474638e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5189:\n",
      "train loss: 1.9366321911027553e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5190:\n",
      "train loss: 1.1179055755525705e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5191:\n",
      "train loss: 9.568521210020482e-13\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5192:\n",
      "train loss: 2.096663343675119e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5193:\n",
      "train loss: 1.935761172971063e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5194:\n",
      "train loss: 1.1184414503582139e-12\n",
      "lr: 3.3891788360153834e-13\n",
      "Epoch 5195:\n",
      "train loss: 9.573448439498786e-13\n",
      "Epoch 05197: reducing learning rate of group 0 to 3.2197e-13.\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5196:\n",
      "train loss: 2.0962091448071907e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5197:\n",
      "train loss: 1.935822770332467e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5198:\n",
      "train loss: 9.65762567309998e-13\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5199:\n",
      "train loss: 8.126247629362987e-13\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5200:\n",
      "train loss: 2.087792565433925e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5201:\n",
      "train loss: 1.9346869019025396e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5202:\n",
      "train loss: 9.66776658261172e-13\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5203:\n",
      "train loss: 8.145161997357297e-13\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5204:\n",
      "train loss: 2.0855869420802494e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5205:\n",
      "train loss: 1.9321723236287942e-12\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5206:\n",
      "train loss: 9.69373912013096e-13\n",
      "lr: 3.2197198942146143e-13\n",
      "Epoch 5207:\n",
      "train loss: 8.185022407563301e-13\n",
      "Epoch 05209: reducing learning rate of group 0 to 3.0587e-13.\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5208:\n",
      "train loss: 2.081087425265354e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5209:\n",
      "train loss: 1.926607809579051e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5210:\n",
      "train loss: 8.314572151039478e-13\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5211:\n",
      "train loss: 6.90067142565332e-13\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5212:\n",
      "train loss: 2.059965512885085e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5213:\n",
      "train loss: 1.908509600988713e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5214:\n",
      "train loss: 8.529648133056974e-13\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5215:\n",
      "train loss: 7.178415484967564e-13\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5216:\n",
      "train loss: 2.0185309550685997e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5217:\n",
      "train loss: 1.8577611484316743e-12\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5218:\n",
      "train loss: 9.13668179337076e-13\n",
      "lr: 3.0587338995038834e-13\n",
      "Epoch 5219:\n",
      "train loss: 7.950423541443223e-13\n",
      "Epoch 05221: reducing learning rate of group 0 to 2.9058e-13.\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5220:\n",
      "train loss: 1.913778949243454e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5221:\n",
      "train loss: 1.7290079746106538e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5222:\n",
      "train loss: 9.245445504955444e-13\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5223:\n",
      "train loss: 8.433178212808299e-13\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5224:\n",
      "train loss: 1.6841404931318946e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5225:\n",
      "train loss: 1.4658794259702117e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5226:\n",
      "train loss: 1.2225729501898781e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5227:\n",
      "train loss: 1.1577701430977365e-12\n",
      "lr: 2.905797204528689e-13\n",
      "Epoch 5228:\n",
      "train loss: 1.3791520796235452e-12\n",
      "Epoch 05230: reducing learning rate of group 0 to 2.7605e-13.\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5229:\n",
      "train loss: 1.1686381881150422e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5230:\n",
      "train loss: 1.5017785852122568e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5231:\n",
      "train loss: 1.4078260687480999e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5232:\n",
      "train loss: 1.056240555295845e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5233:\n",
      "train loss: 9.235085217998605e-13\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5234:\n",
      "train loss: 1.5261620087775368e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5235:\n",
      "train loss: 1.3483629488262073e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5236:\n",
      "train loss: 1.187493629043768e-12\n",
      "lr: 2.7605073443022545e-13\n",
      "Epoch 5237:\n",
      "train loss: 1.101549677931409e-12\n",
      "Epoch 05239: reducing learning rate of group 0 to 2.6225e-13.\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5238:\n",
      "train loss: 1.334827432263325e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5239:\n",
      "train loss: 1.150091780878394e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5240:\n",
      "train loss: 1.2573596950294907e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5241:\n",
      "train loss: 1.1667917323698542e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5242:\n",
      "train loss: 1.1690189584035203e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5243:\n",
      "train loss: 1.016473150698344e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5244:\n",
      "train loss: 1.3661600814503663e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5245:\n",
      "train loss: 1.2517743672527743e-12\n",
      "lr: 2.622481977087142e-13\n",
      "Epoch 5246:\n",
      "train loss: 1.106550201294616e-12\n",
      "Epoch 05248: reducing learning rate of group 0 to 2.4914e-13.\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5247:\n",
      "train loss: 9.781693448061987e-13\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5248:\n",
      "train loss: 1.3851697648301941e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5249:\n",
      "train loss: 1.2633746891662311e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5250:\n",
      "train loss: 9.882804789585635e-13\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5251:\n",
      "train loss: 8.766904492457682e-13\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5252:\n",
      "train loss: 1.3604514703319102e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5253:\n",
      "train loss: 1.2331823715695495e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5254:\n",
      "train loss: 1.0203077028887793e-12\n",
      "lr: 2.4913578782327846e-13\n",
      "Epoch 5255:\n",
      "train loss: 9.098009216937627e-13\n",
      "Epoch 05257: reducing learning rate of group 0 to 2.3668e-13.\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5256:\n",
      "train loss: 1.3279962988511817e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5257:\n",
      "train loss: 1.2026234383977068e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5258:\n",
      "train loss: 9.357855295199835e-13\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5259:\n",
      "train loss: 8.284369283820452e-13\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5260:\n",
      "train loss: 1.299061406740894e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5261:\n",
      "train loss: 1.1828335548809746e-12\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5262:\n",
      "train loss: 9.538919354372084e-13\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5263:\n",
      "train loss: 8.455624626856778e-13\n",
      "lr: 2.3667899843211453e-13\n",
      "Epoch 5264:\n",
      "train loss: 1.2837163019502168e-12\n",
      "Epoch 05266: reducing learning rate of group 0 to 2.2485e-13.\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5265:\n",
      "train loss: 1.1682182882989913e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5266:\n",
      "train loss: 9.676852067260561e-13\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5267:\n",
      "train loss: 8.632803012155544e-13\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5268:\n",
      "train loss: 1.1598680990163612e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5269:\n",
      "train loss: 1.0506368657155136e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5270:\n",
      "train loss: 9.772032174684122e-13\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5271:\n",
      "train loss: 8.729302655934625e-13\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5272:\n",
      "train loss: 1.1502020856538004e-12\n",
      "lr: 2.2484504851050878e-13\n",
      "Epoch 5273:\n",
      "train loss: 1.042140119901335e-12\n",
      "Epoch 05275: reducing learning rate of group 0 to 2.1360e-13.\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5274:\n",
      "train loss: 9.862155071973181e-13\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5275:\n",
      "train loss: 8.820540550783561e-13\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5276:\n",
      "train loss: 1.041085713397215e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5277:\n",
      "train loss: 9.375814333083438e-13\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5278:\n",
      "train loss: 9.882425350182595e-13\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5279:\n",
      "train loss: 8.887295586139279e-13\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5280:\n",
      "train loss: 1.0354448470441117e-12\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5281:\n",
      "train loss: 9.320422872520684e-13\n",
      "lr: 2.1360279608498334e-13\n",
      "Epoch 5282:\n",
      "train loss: 9.932709209488458e-13\n",
      "Epoch 05284: reducing learning rate of group 0 to 2.0292e-13.\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5283:\n",
      "train loss: 8.932106725965126e-13\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5284:\n",
      "train loss: 1.0304317018773689e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5285:\n",
      "train loss: 9.336281623358445e-13\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5286:\n",
      "train loss: 8.964669680199596e-13\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5287:\n",
      "train loss: 8.009489189182524e-13\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5288:\n",
      "train loss: 1.0277396893934227e-12\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5289:\n",
      "train loss: 9.297897948421826e-13\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5290:\n",
      "train loss: 8.988890684068886e-13\n",
      "lr: 2.0292265628073415e-13\n",
      "Epoch 5291:\n",
      "train loss: 8.03032318689822e-13\n",
      "Epoch 05293: reducing learning rate of group 0 to 1.9278e-13.\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5292:\n",
      "train loss: 1.0253010995046336e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5293:\n",
      "train loss: 9.289376380365865e-13\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5294:\n",
      "train loss: 8.095361508112295e-13\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5295:\n",
      "train loss: 7.174704810217727e-13\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5296:\n",
      "train loss: 1.0195081938711863e-12\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5297:\n",
      "train loss: 9.279675610234482e-13\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5298:\n",
      "train loss: 8.101851682181462e-13\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5299:\n",
      "train loss: 7.18279247987548e-13\n",
      "lr: 1.9277652346669742e-13\n",
      "Epoch 5300:\n",
      "train loss: 1.0180424945783866e-12\n",
      "Epoch 05302: reducing learning rate of group 0 to 1.8314e-13.\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5301:\n",
      "train loss: 9.264422289547201e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5302:\n",
      "train loss: 8.107416508877579e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5303:\n",
      "train loss: 7.235496187557019e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5304:\n",
      "train loss: 9.26537308988087e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5305:\n",
      "train loss: 8.395256735519698e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5306:\n",
      "train loss: 8.110791046427498e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5307:\n",
      "train loss: 7.243851371710657e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5308:\n",
      "train loss: 9.26171675917495e-13\n",
      "lr: 1.8313769729336255e-13\n",
      "Epoch 5309:\n",
      "train loss: 8.392571466846215e-13\n",
      "Epoch 05311: reducing learning rate of group 0 to 1.7398e-13.\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5310:\n",
      "train loss: 8.106341167631789e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5311:\n",
      "train loss: 7.241501838007376e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5312:\n",
      "train loss: 8.429031930972921e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5313:\n",
      "train loss: 7.60511675427171e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5314:\n",
      "train loss: 8.076637132485347e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5315:\n",
      "train loss: 7.249269244058701e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5316:\n",
      "train loss: 8.425695669973998e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5317:\n",
      "train loss: 7.610370547704853e-13\n",
      "lr: 1.7398081242869442e-13\n",
      "Epoch 5318:\n",
      "train loss: 8.076162630559029e-13\n",
      "Epoch 05320: reducing learning rate of group 0 to 1.6528e-13.\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5319:\n",
      "train loss: 7.245447741000409e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5320:\n",
      "train loss: 8.42668897425944e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5321:\n",
      "train loss: 7.642622386928116e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5322:\n",
      "train loss: 7.245863033683943e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5323:\n",
      "train loss: 6.464007384864992e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5324:\n",
      "train loss: 8.427227719092257e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5325:\n",
      "train loss: 7.649313920644955e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5326:\n",
      "train loss: 7.248035250026835e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5327:\n",
      "train loss: 6.467360367723488e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5328:\n",
      "train loss: 8.431714759765422e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5329:\n",
      "train loss: 7.647001222818972e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5330:\n",
      "train loss: 7.245982660654386e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5331:\n",
      "train loss: 6.461117764407481e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5332:\n",
      "train loss: 8.427152578248407e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5333:\n",
      "train loss: 7.646395628461435e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5334:\n",
      "train loss: 7.252750372150782e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5335:\n",
      "train loss: 6.462837601144045e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5336:\n",
      "train loss: 8.42893734058305e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5337:\n",
      "train loss: 7.647881291425218e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5338:\n",
      "train loss: 7.244997925158444e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5339:\n",
      "train loss: 6.458882424692675e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5340:\n",
      "train loss: 8.424846268532377e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5341:\n",
      "train loss: 7.64402912463021e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5342:\n",
      "train loss: 7.245202849252832e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5343:\n",
      "train loss: 6.46093578031036e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5344:\n",
      "train loss: 8.427774126005497e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5345:\n",
      "train loss: 7.646951633724531e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5346:\n",
      "train loss: 7.245841387231084e-13\n",
      "lr: 1.6528177180725969e-13\n",
      "Epoch 5347:\n",
      "train loss: 6.461575168130901e-13\n",
      "Epoch 05349: reducing learning rate of group 0 to 1.5702e-13.\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5348:\n",
      "train loss: 8.425744887694934e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5349:\n",
      "train loss: 7.648280130623666e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5350:\n",
      "train loss: 6.506052073624563e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5351:\n",
      "train loss: 5.75323258676467e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5352:\n",
      "train loss: 8.395109509416768e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5353:\n",
      "train loss: 7.643226650348388e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5354:\n",
      "train loss: 6.503473203379723e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5355:\n",
      "train loss: 5.750991329568723e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5356:\n",
      "train loss: 8.390181810631335e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5357:\n",
      "train loss: 7.645559027658893e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5358:\n",
      "train loss: 6.502951162298798e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5359:\n",
      "train loss: 5.75403875441416e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5360:\n",
      "train loss: 8.388441244336603e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5361:\n",
      "train loss: 7.644337872472914e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5362:\n",
      "train loss: 6.505595164134251e-13\n",
      "lr: 1.570176832168967e-13\n",
      "Epoch 5363:\n",
      "train loss: 5.754403574781355e-13\n",
      "Epoch 05365: reducing learning rate of group 0 to 1.4917e-13.\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5364:\n",
      "train loss: 8.389439018127709e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5365:\n",
      "train loss: 7.647903976493474e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5366:\n",
      "train loss: 5.798101221129598e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5367:\n",
      "train loss: 5.079342711503342e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5368:\n",
      "train loss: 8.354057114515758e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5369:\n",
      "train loss: 7.645117309088671e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5370:\n",
      "train loss: 5.78930356756158e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5371:\n",
      "train loss: 5.077077528745091e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5372:\n",
      "train loss: 8.355972431712231e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5373:\n",
      "train loss: 7.649800138972098e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5374:\n",
      "train loss: 5.792718474320513e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5375:\n",
      "train loss: 5.083727921476544e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5376:\n",
      "train loss: 8.350385784886063e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5377:\n",
      "train loss: 7.645720514817687e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5378:\n",
      "train loss: 5.788555976799401e-13\n",
      "lr: 1.4916679905605187e-13\n",
      "Epoch 5379:\n",
      "train loss: 5.083689730099918e-13\n",
      "Epoch 05381: reducing learning rate of group 0 to 1.4171e-13.\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5380:\n",
      "train loss: 8.349883388816688e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5381:\n",
      "train loss: 7.644110001366914e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5382:\n",
      "train loss: 5.126592319086612e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5383:\n",
      "train loss: 4.4576297937473204e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5384:\n",
      "train loss: 8.311256737740558e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5385:\n",
      "train loss: 7.639351659601941e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5386:\n",
      "train loss: 5.123187715887426e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5387:\n",
      "train loss: 4.453291417497146e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5388:\n",
      "train loss: 8.309934033451865e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5389:\n",
      "train loss: 7.647870372497082e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5390:\n",
      "train loss: 5.124911894531182e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5391:\n",
      "train loss: 4.4549633273112144e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5392:\n",
      "train loss: 8.310965847673319e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5393:\n",
      "train loss: 7.63979281304416e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5394:\n",
      "train loss: 5.122773418340604e-13\n",
      "lr: 1.4170845910324926e-13\n",
      "Epoch 5395:\n",
      "train loss: 4.4574837066039474e-13\n",
      "Epoch 05397: reducing learning rate of group 0 to 1.3462e-13.\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5396:\n",
      "train loss: 8.311434864450678e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5397:\n",
      "train loss: 7.645630948267007e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5398:\n",
      "train loss: 4.4855260106332203e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5399:\n",
      "train loss: 3.849963601709812e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5400:\n",
      "train loss: 8.273928591594776e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5401:\n",
      "train loss: 7.64004618312582e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5402:\n",
      "train loss: 4.4909304113224957e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5403:\n",
      "train loss: 3.8499926076211923e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5404:\n",
      "train loss: 8.275394734070475e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5405:\n",
      "train loss: 7.644316780092925e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5406:\n",
      "train loss: 4.492634999305627e-13\n",
      "lr: 1.3462303614808678e-13\n",
      "Epoch 5407:\n",
      "train loss: 3.8566791660515053e-13\n",
      "Epoch 05409: reducing learning rate of group 0 to 1.2789e-13.\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5408:\n",
      "train loss: 8.274149492465064e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5409:\n",
      "train loss: 7.637910731847235e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5410:\n",
      "train loss: 3.887700156444708e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5411:\n",
      "train loss: 3.2869943222714524e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5412:\n",
      "train loss: 8.240391531438712e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5413:\n",
      "train loss: 7.632679848273948e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5414:\n",
      "train loss: 3.897494421764396e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5415:\n",
      "train loss: 3.2960514547345834e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5416:\n",
      "train loss: 8.228575344308064e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5417:\n",
      "train loss: 7.615957653505004e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5418:\n",
      "train loss: 3.911199270069611e-13\n",
      "lr: 1.2789188434068243e-13\n",
      "Epoch 5419:\n",
      "train loss: 3.310065628545072e-13\n",
      "Epoch 05421: reducing learning rate of group 0 to 1.2150e-13.\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5420:\n",
      "train loss: 8.205792721990605e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5421:\n",
      "train loss: 7.592119088234861e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5422:\n",
      "train loss: 3.3600146940132115e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5423:\n",
      "train loss: 2.800296138572349e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5424:\n",
      "train loss: 8.118804934047342e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5425:\n",
      "train loss: 7.521613877238172e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5426:\n",
      "train loss: 3.450984598011869e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5427:\n",
      "train loss: 2.903613772788926e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5428:\n",
      "train loss: 7.975053619974492e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5429:\n",
      "train loss: 7.337709846158116e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5430:\n",
      "train loss: 3.6575053072679017e-13\n",
      "lr: 1.2149729012364831e-13\n",
      "Epoch 5431:\n",
      "train loss: 3.1717533121191395e-13\n",
      "Epoch 05433: reducing learning rate of group 0 to 1.1542e-13.\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5432:\n",
      "train loss: 7.617132543280604e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5433:\n",
      "train loss: 6.911052433441329e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5434:\n",
      "train loss: 3.6055512555795417e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5435:\n",
      "train loss: 3.240756682446158e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5436:\n",
      "train loss: 6.839285401376104e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5437:\n",
      "train loss: 6.023706653766162e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5438:\n",
      "train loss: 4.614954700708663e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5439:\n",
      "train loss: 4.337319073120728e-13\n",
      "lr: 1.1542242561746588e-13\n",
      "Epoch 5440:\n",
      "train loss: 5.743939962537641e-13\n",
      "Epoch 05442: reducing learning rate of group 0 to 1.0965e-13.\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5441:\n",
      "train loss: 4.901149800750459e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5442:\n",
      "train loss: 5.712895575401765e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5443:\n",
      "train loss: 5.369253399091864e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5444:\n",
      "train loss: 4.3692159526001874e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5445:\n",
      "train loss: 3.7765766114148973e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5446:\n",
      "train loss: 6.041924512571138e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5447:\n",
      "train loss: 5.430745979743854e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5448:\n",
      "train loss: 4.554699183618628e-13\n",
      "lr: 1.0965130433659258e-13\n",
      "Epoch 5449:\n",
      "train loss: 4.1500014144128906e-13\n",
      "Epoch 05451: reducing learning rate of group 0 to 1.0417e-13.\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5450:\n",
      "train loss: 5.550217791294287e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5451:\n",
      "train loss: 4.841599107172762e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5452:\n",
      "train loss: 4.715406420299811e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5453:\n",
      "train loss: 4.3597794591002995e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5454:\n",
      "train loss: 4.885861858838674e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5455:\n",
      "train loss: 4.248060070067563e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5456:\n",
      "train loss: 5.265727847428326e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5457:\n",
      "train loss: 4.845749874420589e-13\n",
      "lr: 1.0416873911976295e-13\n",
      "Epoch 5458:\n",
      "train loss: 4.479828534931005e-13\n",
      "Epoch 05460: reducing learning rate of group 0 to 9.8960e-14.\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5459:\n",
      "train loss: 3.9305820661240524e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5460:\n",
      "train loss: 5.481433969278508e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5461:\n",
      "train loss: 5.02108450266224e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5462:\n",
      "train loss: 3.9039345037815885e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5463:\n",
      "train loss: 3.4543386338176654e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5464:\n",
      "train loss: 5.432617788781772e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5465:\n",
      "train loss: 4.9320227713119e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5466:\n",
      "train loss: 4.029138573305422e-13\n",
      "lr: 9.896030216377479e-14\n",
      "Epoch 5467:\n",
      "train loss: 3.606305488059273e-13\n",
      "Epoch 05469: reducing learning rate of group 0 to 9.4012e-14.\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5468:\n",
      "train loss: 5.273979551098472e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5469:\n",
      "train loss: 4.769564365802055e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5470:\n",
      "train loss: 3.7369667673029454e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5471:\n",
      "train loss: 3.328240331734766e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5472:\n",
      "train loss: 5.119870467634682e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5473:\n",
      "train loss: 4.641424846076243e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5474:\n",
      "train loss: 3.8554212349198003e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5475:\n",
      "train loss: 3.4350551133068554e-13\n",
      "lr: 9.401228705558605e-14\n",
      "Epoch 5476:\n",
      "train loss: 5.021329418277022e-13\n",
      "Epoch 05478: reducing learning rate of group 0 to 8.9312e-14.\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5477:\n",
      "train loss: 4.551516160825283e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5478:\n",
      "train loss: 3.9406122405292156e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5479:\n",
      "train loss: 3.5257091426725033e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5480:\n",
      "train loss: 4.50204378135943e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5481:\n",
      "train loss: 4.0730199582651756e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5482:\n",
      "train loss: 3.9919576576464174e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5483:\n",
      "train loss: 3.5724469466762576e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5484:\n",
      "train loss: 4.4600220903183473e-13\n",
      "lr: 8.931167270280675e-14\n",
      "Epoch 5485:\n",
      "train loss: 4.03202563197822e-13\n",
      "Epoch 05487: reducing learning rate of group 0 to 8.4846e-14.\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5486:\n",
      "train loss: 4.02036647609518e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5487:\n",
      "train loss: 3.6096984511139943e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5488:\n",
      "train loss: 4.0306111069267603e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5489:\n",
      "train loss: 3.630210264978855e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5490:\n",
      "train loss: 4.0259089026905845e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5491:\n",
      "train loss: 3.632405205235332e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5492:\n",
      "train loss: 4.0125615277347116e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5493:\n",
      "train loss: 3.60907027614284e-13\n",
      "lr: 8.484608906766641e-14\n",
      "Epoch 5494:\n",
      "train loss: 4.044083770662953e-13\n",
      "Epoch 05496: reducing learning rate of group 0 to 8.0604e-14.\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5495:\n",
      "train loss: 3.6425372944632245e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5496:\n",
      "train loss: 3.9995303039498727e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5497:\n",
      "train loss: 3.6190037034262344e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5498:\n",
      "train loss: 3.648702902070522e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5499:\n",
      "train loss: 3.2698048325927963e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5500:\n",
      "train loss: 3.992112995063176e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5501:\n",
      "train loss: 3.6140297852620664e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5502:\n",
      "train loss: 3.6565121461939395e-13\n",
      "lr: 8.060378461428309e-14\n",
      "Epoch 5503:\n",
      "train loss: 3.273695591875511e-13\n",
      "Epoch 05505: reducing learning rate of group 0 to 7.6574e-14.\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5504:\n",
      "train loss: 3.986757076647675e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5505:\n",
      "train loss: 3.6042006871685954e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5506:\n",
      "train loss: 3.2966290273826064e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5507:\n",
      "train loss: 2.928290654240356e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5508:\n",
      "train loss: 3.965211838128426e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5509:\n",
      "train loss: 3.605589696115781e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5510:\n",
      "train loss: 3.296450190161177e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5511:\n",
      "train loss: 2.9307025007379944e-13\n",
      "lr: 7.657359538356893e-14\n",
      "Epoch 5512:\n",
      "train loss: 3.96336031834197e-13\n",
      "Epoch 05514: reducing learning rate of group 0 to 7.2745e-14.\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5513:\n",
      "train loss: 3.606871485380132e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5514:\n",
      "train loss: 3.299151509816346e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5515:\n",
      "train loss: 2.9547545287304663e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5516:\n",
      "train loss: 3.5979166195821243e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5517:\n",
      "train loss: 3.257377973564592e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5518:\n",
      "train loss: 3.304414901336055e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5519:\n",
      "train loss: 2.9580487750036004e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5520:\n",
      "train loss: 3.5954973230819686e-13\n",
      "lr: 7.274491561439049e-14\n",
      "Epoch 5521:\n",
      "train loss: 3.251893202496991e-13\n",
      "Epoch 05523: reducing learning rate of group 0 to 6.9108e-14.\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5522:\n",
      "train loss: 3.298540886725569e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5523:\n",
      "train loss: 2.9542075281701345e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5524:\n",
      "train loss: 3.2686308799746104e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5525:\n",
      "train loss: 2.9412785750330854e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5526:\n",
      "train loss: 3.287396107545471e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5527:\n",
      "train loss: 2.9560788788463967e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5528:\n",
      "train loss: 3.265468217570768e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5529:\n",
      "train loss: 2.94037224659431e-13\n",
      "lr: 6.910766983367096e-14\n",
      "Epoch 5530:\n",
      "train loss: 3.290184585280023e-13\n",
      "Epoch 05532: reducing learning rate of group 0 to 6.5652e-14.\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5531:\n",
      "train loss: 2.96092223823685e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5532:\n",
      "train loss: 3.265193750145378e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5533:\n",
      "train loss: 2.950473907712915e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5534:\n",
      "train loss: 2.9657033184624254e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5535:\n",
      "train loss: 2.6522079657710064e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5536:\n",
      "train loss: 3.26200628662264e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5537:\n",
      "train loss: 2.950901267625877e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5538:\n",
      "train loss: 2.9698112838082964e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5539:\n",
      "train loss: 2.650692845957383e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5540:\n",
      "train loss: 3.262045734889655e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5541:\n",
      "train loss: 2.9515277400620535e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5542:\n",
      "train loss: 2.962179455964588e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5543:\n",
      "train loss: 2.6514110406368303e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5544:\n",
      "train loss: 3.262018580108032e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5545:\n",
      "train loss: 2.950972853338346e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5546:\n",
      "train loss: 2.962658478237135e-13\n",
      "lr: 6.56522863419874e-14\n",
      "Epoch 5547:\n",
      "train loss: 2.654816385508149e-13\n",
      "Epoch 05549: reducing learning rate of group 0 to 6.2370e-14.\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5548:\n",
      "train loss: 3.260009289536968e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5549:\n",
      "train loss: 2.9502111577403905e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5550:\n",
      "train loss: 2.6662344572830734e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5551:\n",
      "train loss: 2.3743291741244284e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5552:\n",
      "train loss: 3.2359045344841686e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5553:\n",
      "train loss: 2.945335731992127e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5554:\n",
      "train loss: 2.6688469428393625e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5555:\n",
      "train loss: 2.3789500066579675e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5556:\n",
      "train loss: 3.2397219059847846e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5557:\n",
      "train loss: 2.9454858503160877e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5558:\n",
      "train loss: 2.668502664488016e-13\n",
      "lr: 6.236967202488803e-14\n",
      "Epoch 5559:\n",
      "train loss: 2.3756325206663615e-13\n",
      "Epoch 05561: reducing learning rate of group 0 to 5.9251e-14.\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5560:\n",
      "train loss: 3.239136573831747e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5561:\n",
      "train loss: 2.9489968520580615e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5562:\n",
      "train loss: 2.391411460277662e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5563:\n",
      "train loss: 2.1046639300432217e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5564:\n",
      "train loss: 3.232125024783588e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5565:\n",
      "train loss: 2.946688252318853e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5566:\n",
      "train loss: 2.388451706696751e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5567:\n",
      "train loss: 2.105312631748187e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5568:\n",
      "train loss: 3.228249310077597e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5569:\n",
      "train loss: 2.9471140452203794e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5570:\n",
      "train loss: 2.3895764748099813e-13\n",
      "lr: 5.925118842364363e-14\n",
      "Epoch 5571:\n",
      "train loss: 2.1053359494904782e-13\n",
      "Epoch 05573: reducing learning rate of group 0 to 5.6289e-14.\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5572:\n",
      "train loss: 3.227443442769099e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5573:\n",
      "train loss: 2.944241262758217e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5574:\n",
      "train loss: 2.1257229879169558e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5575:\n",
      "train loss: 1.8586028431553306e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5576:\n",
      "train loss: 3.2141771330541754e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5577:\n",
      "train loss: 2.944946154144457e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5578:\n",
      "train loss: 2.1316358385877482e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5579:\n",
      "train loss: 1.8625075772439307e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5580:\n",
      "train loss: 3.2095692343298395e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5581:\n",
      "train loss: 2.943408660470062e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5582:\n",
      "train loss: 2.1286661695179987e-13\n",
      "lr: 5.628862900246144e-14\n",
      "Epoch 5583:\n",
      "train loss: 1.8631172608225547e-13\n",
      "Epoch 05585: reducing learning rate of group 0 to 5.3474e-14.\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5584:\n",
      "train loss: 3.211215093512231e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5585:\n",
      "train loss: 2.9426664205734754e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5586:\n",
      "train loss: 1.8767223500897727e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5587:\n",
      "train loss: 1.6243711203937514e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5588:\n",
      "train loss: 3.190738334305267e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5589:\n",
      "train loss: 2.9418439277702414e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5590:\n",
      "train loss: 1.8783070067601287e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5591:\n",
      "train loss: 1.6247618287656977e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5592:\n",
      "train loss: 3.1933677061982944e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5593:\n",
      "train loss: 2.9387719598825375e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5594:\n",
      "train loss: 1.8752362492925772e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5595:\n",
      "train loss: 1.6194215931028867e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5596:\n",
      "train loss: 3.191680406960049e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5597:\n",
      "train loss: 2.939013796235534e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5598:\n",
      "train loss: 1.879227813366977e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5599:\n",
      "train loss: 1.6289786213667184e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5600:\n",
      "train loss: 3.189662530899956e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5601:\n",
      "train loss: 2.940462093246981e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5602:\n",
      "train loss: 1.878632992047101e-13\n",
      "lr: 5.347419755233837e-14\n",
      "Epoch 5603:\n",
      "train loss: 1.6297403837360195e-13\n",
      "Epoch 05605: reducing learning rate of group 0 to 5.0800e-14.\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5604:\n",
      "train loss: 3.181037952719139e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5605:\n",
      "train loss: 2.938813658754068e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5606:\n",
      "train loss: 1.6440283209758947e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5607:\n",
      "train loss: 1.4058433644945899e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5608:\n",
      "train loss: 3.1685229674189503e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5609:\n",
      "train loss: 2.934935322618275e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5610:\n",
      "train loss: 1.6399323963362345e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5611:\n",
      "train loss: 1.4047748108922384e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5612:\n",
      "train loss: 3.1640077615125486e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5613:\n",
      "train loss: 2.9337959582882845e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5614:\n",
      "train loss: 1.6483635894823249e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5615:\n",
      "train loss: 1.415143568233379e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5616:\n",
      "train loss: 3.160553916053683e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5617:\n",
      "train loss: 2.917024531874391e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5618:\n",
      "train loss: 1.6663209660021563e-13\n",
      "lr: 5.080048767472145e-14\n",
      "Epoch 5619:\n",
      "train loss: 1.4259318095187363e-13\n",
      "Epoch 05621: reducing learning rate of group 0 to 4.8260e-14.\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5620:\n",
      "train loss: 3.145330299327296e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5621:\n",
      "train loss: 2.897968247083204e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5622:\n",
      "train loss: 1.4605324175833182e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5623:\n",
      "train loss: 1.2335340691711354e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5624:\n",
      "train loss: 3.0969186566701893e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5625:\n",
      "train loss: 2.8551267192022184e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5626:\n",
      "train loss: 1.5086066959903941e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5627:\n",
      "train loss: 1.2935362140748023e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5628:\n",
      "train loss: 3.025192566139332e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5629:\n",
      "train loss: 2.766360972962041e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5630:\n",
      "train loss: 1.6071212233650654e-13\n",
      "lr: 4.8260463290985374e-14\n",
      "Epoch 5631:\n",
      "train loss: 1.4106882508525751e-13\n",
      "Epoch 05633: reducing learning rate of group 0 to 4.5847e-14.\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5632:\n",
      "train loss: 2.8890766254911293e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5633:\n",
      "train loss: 2.6114365834009323e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5634:\n",
      "train loss: 1.5596521416283567e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5635:\n",
      "train loss: 1.390476007330505e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5636:\n",
      "train loss: 2.658835831772947e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5637:\n",
      "train loss: 2.358568296289953e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5638:\n",
      "train loss: 1.8236457555020237e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5639:\n",
      "train loss: 1.6830894660326356e-13\n",
      "lr: 4.5847440126436103e-14\n",
      "Epoch 5640:\n",
      "train loss: 2.365342740805045e-13\n",
      "Epoch 05642: reducing learning rate of group 0 to 4.3555e-14.\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5641:\n",
      "train loss: 2.0672422103487953e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5642:\n",
      "train loss: 2.1312670758497425e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5643:\n",
      "train loss: 1.9801386534729278e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5644:\n",
      "train loss: 1.8904722423832945e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5645:\n",
      "train loss: 1.640540567703899e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5646:\n",
      "train loss: 2.3012251725434125e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5647:\n",
      "train loss: 2.102841413444874e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5648:\n",
      "train loss: 1.8116261108630974e-13\n",
      "lr: 4.3555068120114294e-14\n",
      "Epoch 5649:\n",
      "train loss: 1.6013928275522933e-13\n",
      "Epoch 05651: reducing learning rate of group 0 to 4.1377e-14.\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5650:\n",
      "train loss: 2.3077334928232207e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5651:\n",
      "train loss: 2.0770274800300514e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5652:\n",
      "train loss: 1.6748010703742432e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5653:\n",
      "train loss: 1.5050951742857252e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5654:\n",
      "train loss: 2.1871370316231658e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5655:\n",
      "train loss: 1.9603837157784835e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5656:\n",
      "train loss: 1.8115843668208876e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5657:\n",
      "train loss: 1.6516923393516956e-13\n",
      "lr: 4.137731471410858e-14\n",
      "Epoch 5658:\n",
      "train loss: 2.0423786305739175e-13\n",
      "Epoch 05660: reducing learning rate of group 0 to 3.9308e-14.\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5659:\n",
      "train loss: 1.814563697922142e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5660:\n",
      "train loss: 1.9498942366131522e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5661:\n",
      "train loss: 1.7904037412439689e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5662:\n",
      "train loss: 1.7344577734242817e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5663:\n",
      "train loss: 1.5293628193296116e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5664:\n",
      "train loss: 2.0312438323126163e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5665:\n",
      "train loss: 1.858028820624973e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5666:\n",
      "train loss: 1.6723562363984666e-13\n",
      "lr: 3.930844897840315e-14\n",
      "Epoch 5667:\n",
      "train loss: 1.4804165316268207e-13\n",
      "Epoch 05669: reducing learning rate of group 0 to 3.7343e-14.\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5668:\n",
      "train loss: 2.0729171762413474e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5669:\n",
      "train loss: 1.8872992268651558e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5670:\n",
      "train loss: 1.4711732255788071e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5671:\n",
      "train loss: 1.2941989082754437e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5672:\n",
      "train loss: 2.0710377988157415e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5673:\n",
      "train loss: 1.8950899405201838e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5674:\n",
      "train loss: 1.4684521376308201e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5675:\n",
      "train loss: 1.2860675006763972e-13\n",
      "lr: 3.7343026529482985e-14\n",
      "Epoch 5676:\n",
      "train loss: 2.076438180102881e-13\n",
      "Epoch 05678: reducing learning rate of group 0 to 3.5476e-14.\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5677:\n",
      "train loss: 1.8874413827442117e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5678:\n",
      "train loss: 1.4653882410130616e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5679:\n",
      "train loss: 1.297059646614595e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5680:\n",
      "train loss: 1.8943829703369685e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5681:\n",
      "train loss: 1.7220468414472028e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5682:\n",
      "train loss: 1.4718162204074148e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5683:\n",
      "train loss: 1.2984951285493792e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5684:\n",
      "train loss: 1.8916239833968677e-13\n",
      "lr: 3.547587520300883e-14\n",
      "Epoch 5685:\n",
      "train loss: 1.725590948163121e-13\n",
      "Epoch 05687: reducing learning rate of group 0 to 3.3702e-14.\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5686:\n",
      "train loss: 1.4658708460542932e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5687:\n",
      "train loss: 1.2939133494647515e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5688:\n",
      "train loss: 1.7351695968930212e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5689:\n",
      "train loss: 1.584368278419168e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5690:\n",
      "train loss: 1.4530351603968778e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5691:\n",
      "train loss: 1.297544763259782e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5692:\n",
      "train loss: 1.7399770293230466e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5693:\n",
      "train loss: 1.5856754403489662e-13\n",
      "lr: 3.3702081442858386e-14\n",
      "Epoch 5694:\n",
      "train loss: 1.4545424920554304e-13\n",
      "Epoch 05696: reducing learning rate of group 0 to 3.2017e-14.\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5695:\n",
      "train loss: 1.2907312968577368e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5696:\n",
      "train loss: 1.7416963113146563e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5697:\n",
      "train loss: 1.5951169476829928e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5698:\n",
      "train loss: 1.292400939588092e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5699:\n",
      "train loss: 1.148583646868455e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5700:\n",
      "train loss: 1.7368313538340462e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5701:\n",
      "train loss: 1.5918555720826477e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5702:\n",
      "train loss: 1.2944389699743917e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5703:\n",
      "train loss: 1.1490625882111435e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5704:\n",
      "train loss: 1.7365447834625353e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5705:\n",
      "train loss: 1.5913237797633414e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5706:\n",
      "train loss: 1.3006145673349594e-13\n",
      "lr: 3.2016977370715464e-14\n",
      "Epoch 5707:\n",
      "train loss: 1.150744476142184e-13\n",
      "Epoch 05709: reducing learning rate of group 0 to 3.0416e-14.\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5708:\n",
      "train loss: 1.7323938987091592e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5709:\n",
      "train loss: 1.5889444499306862e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5710:\n",
      "train loss: 1.1617209660923806e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5711:\n",
      "train loss: 1.017162770838903e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5712:\n",
      "train loss: 1.7290379785937024e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5713:\n",
      "train loss: 1.5864923091416363e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5714:\n",
      "train loss: 1.160165547186878e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5715:\n",
      "train loss: 1.0191016104880079e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5716:\n",
      "train loss: 1.7235611667123132e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5717:\n",
      "train loss: 1.5817414885140153e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5718:\n",
      "train loss: 1.1659304261491913e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5719:\n",
      "train loss: 1.0162371949911838e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5720:\n",
      "train loss: 1.7252560494048631e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5721:\n",
      "train loss: 1.578344096718207e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5722:\n",
      "train loss: 1.1617044723028168e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5723:\n",
      "train loss: 1.0261715281986355e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5724:\n",
      "train loss: 1.719631527954418e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5725:\n",
      "train loss: 1.576960573751655e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5726:\n",
      "train loss: 1.167003997143472e-13\n",
      "lr: 3.041612850217969e-14\n",
      "Epoch 5727:\n",
      "train loss: 1.0254733227066436e-13\n",
      "Epoch 05729: reducing learning rate of group 0 to 2.8895e-14.\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5728:\n",
      "train loss: 1.7260088773793e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5729:\n",
      "train loss: 1.5752780598052252e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5730:\n",
      "train loss: 1.0315289069940417e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5731:\n",
      "train loss: 8.845599186175892e-14\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5732:\n",
      "train loss: 1.7087303140930435e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5733:\n",
      "train loss: 1.576303802349344e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5734:\n",
      "train loss: 1.0281146781141026e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5735:\n",
      "train loss: 8.892755356908899e-14\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5736:\n",
      "train loss: 1.7032367022054497e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5737:\n",
      "train loss: 1.5727043916206045e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5738:\n",
      "train loss: 1.0378026328350964e-13\n",
      "lr: 2.8895322077070705e-14\n",
      "Epoch 5739:\n",
      "train loss: 8.937780119614489e-14\n",
      "Epoch 05741: reducing learning rate of group 0 to 2.7451e-14.\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5740:\n",
      "train loss: 1.7043090210724249e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5741:\n",
      "train loss: 1.5632520957641092e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5742:\n",
      "train loss: 9.068105298573157e-14\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5743:\n",
      "train loss: 7.749195333345259e-14\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5744:\n",
      "train loss: 1.693699570317514e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5745:\n",
      "train loss: 1.5633681406821663e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5746:\n",
      "train loss: 9.154442059308949e-14\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5747:\n",
      "train loss: 7.829587327162977e-14\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5748:\n",
      "train loss: 1.687414264484208e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5749:\n",
      "train loss: 1.5524174398583076e-13\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5750:\n",
      "train loss: 9.172634678076774e-14\n",
      "lr: 2.745055597321717e-14\n",
      "Epoch 5751:\n",
      "train loss: 7.978971759828608e-14\n",
      "Epoch 05753: reducing learning rate of group 0 to 2.6078e-14.\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5752:\n",
      "train loss: 1.6756142678142575e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5753:\n",
      "train loss: 1.5389589406704692e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5754:\n",
      "train loss: 8.107134340490752e-14\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5755:\n",
      "train loss: 6.968537350280429e-14\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5756:\n",
      "train loss: 1.649201620354511e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5757:\n",
      "train loss: 1.51847407638004e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5758:\n",
      "train loss: 8.437775137565311e-14\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5759:\n",
      "train loss: 7.218014134421087e-14\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5760:\n",
      "train loss: 1.614246445726799e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5761:\n",
      "train loss: 1.4807892252676341e-13\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5762:\n",
      "train loss: 8.767484666463222e-14\n",
      "lr: 2.607802817455631e-14\n",
      "Epoch 5763:\n",
      "train loss: 7.693182403692884e-14\n",
      "Epoch 05765: reducing learning rate of group 0 to 2.4774e-14.\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5764:\n",
      "train loss: 1.5589690639978444e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5765:\n",
      "train loss: 1.4178316254162536e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5766:\n",
      "train loss: 8.270958374324554e-14\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5767:\n",
      "train loss: 7.317210275328484e-14\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5768:\n",
      "train loss: 1.468642955414155e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5769:\n",
      "train loss: 1.3190190032376796e-13\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5770:\n",
      "train loss: 9.34050217538686e-14\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5771:\n",
      "train loss: 8.424887922403795e-14\n",
      "lr: 2.4774126765828492e-14\n",
      "Epoch 5772:\n",
      "train loss: 1.3432904852674072e-13\n",
      "Epoch 05774: reducing learning rate of group 0 to 2.3535e-14.\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5773:\n",
      "train loss: 1.1860724922144756e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5774:\n",
      "train loss: 1.0811263787157039e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5775:\n",
      "train loss: 9.929167162711393e-14\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5776:\n",
      "train loss: 1.0975082493647498e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5777:\n",
      "train loss: 9.629266289008738e-14\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5778:\n",
      "train loss: 1.183485245864659e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5779:\n",
      "train loss: 1.0814128894268907e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5780:\n",
      "train loss: 1.0237472627987894e-13\n",
      "lr: 2.3535420427537068e-14\n",
      "Epoch 5781:\n",
      "train loss: 8.975703120027674e-14\n",
      "Epoch 05783: reducing learning rate of group 0 to 2.2359e-14.\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5782:\n",
      "train loss: 1.219704729684038e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5783:\n",
      "train loss: 1.1105653550963715e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5784:\n",
      "train loss: 9.04386554164871e-14\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5785:\n",
      "train loss: 8.009756325097367e-14\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5786:\n",
      "train loss: 1.2088700290529912e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5787:\n",
      "train loss: 1.0881313727953362e-13\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5788:\n",
      "train loss: 9.341052839250843e-14\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5789:\n",
      "train loss: 8.429691058459245e-14\n",
      "lr: 2.2358649406160214e-14\n",
      "Epoch 5790:\n",
      "train loss: 1.1570918000993098e-13\n",
      "Epoch 05792: reducing learning rate of group 0 to 2.1241e-14.\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5791:\n",
      "train loss: 1.0356408005224578e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5792:\n",
      "train loss: 9.976315944292332e-14\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5793:\n",
      "train loss: 9.077414283115267e-14\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5794:\n",
      "train loss: 1.0020666094389465e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5795:\n",
      "train loss: 8.849260208346238e-14\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5796:\n",
      "train loss: 1.0338194127560117e-13\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5797:\n",
      "train loss: 9.388174758757355e-14\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5798:\n",
      "train loss: 9.685639734879985e-14\n",
      "lr: 2.12407169358522e-14\n",
      "Epoch 5799:\n",
      "train loss: 8.571772392430066e-14\n",
      "Epoch 05801: reducing learning rate of group 0 to 2.0179e-14.\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5800:\n",
      "train loss: 1.0580281551182201e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5801:\n",
      "train loss: 9.62579356820055e-14\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5802:\n",
      "train loss: 8.482162975158208e-14\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5803:\n",
      "train loss: 7.525050565809955e-14\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5804:\n",
      "train loss: 1.0599877702046637e-13\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5805:\n",
      "train loss: 9.711017365107495e-14\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5806:\n",
      "train loss: 8.432581786207512e-14\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5807:\n",
      "train loss: 7.44875222439548e-14\n",
      "lr: 2.017868108905959e-14\n",
      "Epoch 5808:\n",
      "train loss: 1.0645368099183122e-13\n",
      "Epoch 05810: reducing learning rate of group 0 to 1.9170e-14.\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5809:\n",
      "train loss: 9.736056510923356e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5810:\n",
      "train loss: 8.414991043137024e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5811:\n",
      "train loss: 7.543313762811176e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5812:\n",
      "train loss: 9.738363661477788e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5813:\n",
      "train loss: 8.770472027564413e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5814:\n",
      "train loss: 8.468482990933975e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5815:\n",
      "train loss: 7.508135436257937e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5816:\n",
      "train loss: 9.758467705880654e-14\n",
      "lr: 1.9169747034606612e-14\n",
      "Epoch 5817:\n",
      "train loss: 8.819027489265827e-14\n",
      "Epoch 05819: reducing learning rate of group 0 to 1.8211e-14.\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5818:\n",
      "train loss: 8.420715831626457e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5819:\n",
      "train loss: 7.51494730480982e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5820:\n",
      "train loss: 8.90785180167869e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5821:\n",
      "train loss: 7.96745718570734e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5822:\n",
      "train loss: 8.35317065591988e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5823:\n",
      "train loss: 7.532102031943107e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5824:\n",
      "train loss: 8.868465957426419e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5825:\n",
      "train loss: 8.001999747441267e-14\n",
      "lr: 1.821125968287628e-14\n",
      "Epoch 5826:\n",
      "train loss: 8.384940100655303e-14\n",
      "Epoch 05828: reducing learning rate of group 0 to 1.7301e-14.\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5827:\n",
      "train loss: 7.519686891762104e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5828:\n",
      "train loss: 8.934989452988987e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5829:\n",
      "train loss: 8.039019879096878e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5830:\n",
      "train loss: 7.549452092255981e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5831:\n",
      "train loss: 6.72006524122228e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5832:\n",
      "train loss: 8.909036387089862e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5833:\n",
      "train loss: 8.111853794286023e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5834:\n",
      "train loss: 7.57053802822799e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5835:\n",
      "train loss: 6.666392573311845e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5836:\n",
      "train loss: 8.870563446052777e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5837:\n",
      "train loss: 8.030200436281049e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5838:\n",
      "train loss: 7.625410326986774e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5839:\n",
      "train loss: 6.720766551037077e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5840:\n",
      "train loss: 8.893301528690328e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5841:\n",
      "train loss: 8.02217369640743e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5842:\n",
      "train loss: 7.627020951369186e-14\n",
      "lr: 1.7300696698732465e-14\n",
      "Epoch 5843:\n",
      "train loss: 6.747618708998282e-14\n",
      "Epoch 05845: reducing learning rate of group 0 to 1.6436e-14.\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5844:\n",
      "train loss: 8.841548902650572e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5845:\n",
      "train loss: 7.984995673640186e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5846:\n",
      "train loss: 6.792068687436441e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5847:\n",
      "train loss: 5.980403330404819e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5848:\n",
      "train loss: 8.778535694574971e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5849:\n",
      "train loss: 7.951941821944056e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5850:\n",
      "train loss: 6.79642688416686e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5851:\n",
      "train loss: 5.986710655991741e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5852:\n",
      "train loss: 8.772269996440397e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5853:\n",
      "train loss: 8.020638193622133e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5854:\n",
      "train loss: 6.821911301366962e-14\n",
      "lr: 1.643566186379584e-14\n",
      "Epoch 5855:\n",
      "train loss: 5.995013873082604e-14\n",
      "Epoch 05857: reducing learning rate of group 0 to 1.5614e-14.\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5856:\n",
      "train loss: 8.794759996163476e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5857:\n",
      "train loss: 7.937852646100473e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5858:\n",
      "train loss: 6.144445814858554e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5859:\n",
      "train loss: 5.322362778520507e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5860:\n",
      "train loss: 8.725032051666576e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5861:\n",
      "train loss: 7.930861359833683e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5862:\n",
      "train loss: 6.141582146236763e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5863:\n",
      "train loss: 5.3992526778743296e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5864:\n",
      "train loss: 8.681677750735378e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5865:\n",
      "train loss: 7.852055283125156e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5866:\n",
      "train loss: 6.163963967519563e-14\n",
      "lr: 1.5613878770606047e-14\n",
      "Epoch 5867:\n",
      "train loss: 5.443567468514475e-14\n",
      "Epoch 05869: reducing learning rate of group 0 to 1.4833e-14.\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5868:\n",
      "train loss: 8.634236772418388e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5869:\n",
      "train loss: 7.859202717438673e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5870:\n",
      "train loss: 5.48095740798957e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5871:\n",
      "train loss: 4.7366708004827006e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5872:\n",
      "train loss: 8.541828814900658e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5873:\n",
      "train loss: 7.831934172489849e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5874:\n",
      "train loss: 5.473991455590297e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5875:\n",
      "train loss: 4.783906218607982e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5876:\n",
      "train loss: 8.541331608655254e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5877:\n",
      "train loss: 7.796421294442563e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5878:\n",
      "train loss: 5.574454894224152e-14\n",
      "lr: 1.4833184832075743e-14\n",
      "Epoch 5879:\n",
      "train loss: 4.850165874889655e-14\n",
      "Epoch 05881: reducing learning rate of group 0 to 1.4092e-14.\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5880:\n",
      "train loss: 8.503676371700294e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5881:\n",
      "train loss: 7.788172739086526e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5882:\n",
      "train loss: 4.8793055308408565e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5883:\n",
      "train loss: 4.3407159031855297e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5884:\n",
      "train loss: 8.324766216589118e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5885:\n",
      "train loss: 7.712485158406529e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5886:\n",
      "train loss: 5.0704355795806345e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5887:\n",
      "train loss: 4.393186763809739e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5888:\n",
      "train loss: 8.304358625474534e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5889:\n",
      "train loss: 7.689697164821131e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5890:\n",
      "train loss: 5.0962948399636945e-14\n",
      "lr: 1.4091525590471955e-14\n",
      "Epoch 5891:\n",
      "train loss: 4.4035819081666266e-14\n",
      "Epoch 05893: reducing learning rate of group 0 to 1.3387e-14.\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5892:\n",
      "train loss: 8.218699919881542e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5893:\n",
      "train loss: 7.593299536985462e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5894:\n",
      "train loss: 4.491181451087179e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5895:\n",
      "train loss: 3.8984571509250475e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5896:\n",
      "train loss: 8.066382198899073e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5897:\n",
      "train loss: 7.433474270057816e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5898:\n",
      "train loss: 4.655019372481404e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5899:\n",
      "train loss: 4.047141012787118e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5900:\n",
      "train loss: 7.93188875025338e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5901:\n",
      "train loss: 7.272572489032823e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5902:\n",
      "train loss: 4.8211180644642364e-14\n",
      "lr: 1.3386949310948356e-14\n",
      "Epoch 5903:\n",
      "train loss: 4.184155413997345e-14\n",
      "Epoch 05905: reducing learning rate of group 0 to 1.2718e-14.\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5904:\n",
      "train loss: 7.772251833039241e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5905:\n",
      "train loss: 7.043084015715303e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5906:\n",
      "train loss: 4.457383406582892e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5907:\n",
      "train loss: 3.904226501740467e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5908:\n",
      "train loss: 7.424902336649525e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5909:\n",
      "train loss: 6.802028532998985e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5910:\n",
      "train loss: 4.7672361486798326e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5911:\n",
      "train loss: 4.251509111676102e-14\n",
      "lr: 1.2717601845400937e-14\n",
      "Epoch 5912:\n",
      "train loss: 7.110908495864576e-14\n",
      "Epoch 05914: reducing learning rate of group 0 to 1.2082e-14.\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5913:\n",
      "train loss: 6.449435069244707e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5914:\n",
      "train loss: 5.0971968710443116e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5915:\n",
      "train loss: 4.640223312653122e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5916:\n",
      "train loss: 6.157640788119081e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5917:\n",
      "train loss: 5.536057307589065e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5918:\n",
      "train loss: 5.395915683723074e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5919:\n",
      "train loss: 4.9562959435515026e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5920:\n",
      "train loss: 5.845336998956701e-14\n",
      "lr: 1.208172175313089e-14\n",
      "Epoch 5921:\n",
      "train loss: 5.222316228047778e-14\n",
      "Epoch 05923: reducing learning rate of group 0 to 1.1478e-14.\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5922:\n",
      "train loss: 5.771097689140207e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5923:\n",
      "train loss: 5.264601293046028e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5924:\n",
      "train loss: 5.098482736530339e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5925:\n",
      "train loss: 4.4740853951905635e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5926:\n",
      "train loss: 5.906634483689078e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5927:\n",
      "train loss: 5.3379004615847406e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5928:\n",
      "train loss: 4.89945333134602e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5929:\n",
      "train loss: 4.442810048439472e-14\n",
      "lr: 1.1477635665474344e-14\n",
      "Epoch 5930:\n",
      "train loss: 5.949698026904893e-14\n",
      "Epoch 05932: reducing learning rate of group 0 to 1.0904e-14.\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5931:\n",
      "train loss: 5.352247352309795e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5932:\n",
      "train loss: 4.9951364267318794e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5933:\n",
      "train loss: 4.462784020505192e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5934:\n",
      "train loss: 5.3595010172968636e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5935:\n",
      "train loss: 4.826177123819799e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5936:\n",
      "train loss: 4.998723787479323e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5937:\n",
      "train loss: 4.522765470649499e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5938:\n",
      "train loss: 5.335244484966788e-14\n",
      "lr: 1.0903753882200628e-14\n",
      "Epoch 5939:\n",
      "train loss: 4.8632263992826244e-14\n",
      "Epoch 05941: reducing learning rate of group 0 to 1.0359e-14.\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5940:\n",
      "train loss: 5.04817530367329e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5941:\n",
      "train loss: 4.5183990845270895e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5942:\n",
      "train loss: 4.798094234210446e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5943:\n",
      "train loss: 4.29585303835611e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5944:\n",
      "train loss: 5.0072864097049547e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5945:\n",
      "train loss: 4.513358785654363e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5946:\n",
      "train loss: 4.828133018422258e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5947:\n",
      "train loss: 4.341365205246563e-14\n",
      "lr: 1.0358566188090596e-14\n",
      "Epoch 5948:\n",
      "train loss: 5.0041612276925974e-14\n",
      "Epoch 05950: reducing learning rate of group 0 to 9.8406e-15.\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5949:\n",
      "train loss: 4.5544504462964603e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5950:\n",
      "train loss: 4.8454099936145595e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5951:\n",
      "train loss: 4.3447524667185596e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5952:\n",
      "train loss: 4.552796743540783e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5953:\n",
      "train loss: 4.104805041905875e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5954:\n",
      "train loss: 4.808287595089133e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5955:\n",
      "train loss: 4.360132004117738e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5956:\n",
      "train loss: 4.542583150801093e-14\n",
      "lr: 9.840637878686066e-15\n",
      "Epoch 5957:\n",
      "train loss: 4.109459167148784e-14\n",
      "Epoch 05959: reducing learning rate of group 0 to 9.3486e-15.\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5958:\n",
      "train loss: 4.802671057302035e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5959:\n",
      "train loss: 4.29990401244104e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5960:\n",
      "train loss: 4.092467305843375e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5961:\n",
      "train loss: 3.6650864510721484e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5962:\n",
      "train loss: 4.82250407095746e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5963:\n",
      "train loss: 4.325097948837622e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5964:\n",
      "train loss: 4.0904910991434233e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5965:\n",
      "train loss: 3.65996671603266e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5966:\n",
      "train loss: 4.80487471318234e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5967:\n",
      "train loss: 4.342438841282249e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5968:\n",
      "train loss: 4.106693933376544e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5969:\n",
      "train loss: 3.6202489897528116e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5970:\n",
      "train loss: 4.7810113142190656e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5971:\n",
      "train loss: 4.336178675806435e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5972:\n",
      "train loss: 4.171329023106132e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5973:\n",
      "train loss: 3.666990702198881e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5974:\n",
      "train loss: 4.7807884840487607e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5975:\n",
      "train loss: 4.3073023663801926e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5976:\n",
      "train loss: 4.13780759294554e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5977:\n",
      "train loss: 3.618282505272437e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5978:\n",
      "train loss: 4.7756127099015605e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5979:\n",
      "train loss: 4.30763523938261e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5980:\n",
      "train loss: 4.160306228430009e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5981:\n",
      "train loss: 3.6318697936353924e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5982:\n",
      "train loss: 4.826845018332151e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5983:\n",
      "train loss: 4.314322539309331e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5984:\n",
      "train loss: 4.159458956941478e-14\n",
      "lr: 9.348605984751763e-15\n",
      "Epoch 5985:\n",
      "train loss: 3.651027765195188e-14\n",
      "Epoch 05987: reducing learning rate of group 0 to 8.8812e-15.\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5986:\n",
      "train loss: 4.762326216891765e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5987:\n",
      "train loss: 4.28526831845189e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5988:\n",
      "train loss: 3.726132023122711e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5989:\n",
      "train loss: 3.317484229895486e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5990:\n",
      "train loss: 4.731255958955944e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5991:\n",
      "train loss: 4.276280705612889e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5992:\n",
      "train loss: 3.763836989478906e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5993:\n",
      "train loss: 3.297907493030741e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5994:\n",
      "train loss: 4.693728310057702e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5995:\n",
      "train loss: 4.289280673044159e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5996:\n",
      "train loss: 3.732796507671665e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5997:\n",
      "train loss: 3.30181121946857e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5998:\n",
      "train loss: 4.732184002328266e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 5999:\n",
      "train loss: 4.2573717958202825e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6000:\n",
      "train loss: 3.7931696367829236e-14\n",
      "lr: 8.881175685514174e-15\n",
      "Epoch 6001:\n",
      "train loss: 3.317842360566022e-14\n",
      "Epoch 06003: reducing learning rate of group 0 to 8.4371e-15.\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6002:\n",
      "train loss: 4.708385729585491e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6003:\n",
      "train loss: 4.279996860812855e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6004:\n",
      "train loss: 3.301900803731116e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6005:\n",
      "train loss: 2.9110990903326503e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6006:\n",
      "train loss: 4.689012018216403e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6007:\n",
      "train loss: 4.257061261110294e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6008:\n",
      "train loss: 3.345903087394722e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6009:\n",
      "train loss: 2.8699868896679856e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6010:\n",
      "train loss: 4.669929057565549e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6011:\n",
      "train loss: 4.262814374049288e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6012:\n",
      "train loss: 3.3669813868315845e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6013:\n",
      "train loss: 2.916766217763538e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6014:\n",
      "train loss: 4.7411935388555e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6015:\n",
      "train loss: 4.250804370842901e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6016:\n",
      "train loss: 3.337296183194145e-14\n",
      "lr: 8.437116901238466e-15\n",
      "Epoch 6017:\n",
      "train loss: 2.9076870685166934e-14\n",
      "Epoch 06019: reducing learning rate of group 0 to 8.0153e-15.\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6018:\n",
      "train loss: 4.677144294335658e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6019:\n",
      "train loss: 4.257593057236429e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6020:\n",
      "train loss: 2.992596197901156e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6021:\n",
      "train loss: 2.532719491727931e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6022:\n",
      "train loss: 4.666150450029134e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6023:\n",
      "train loss: 4.2258316951240414e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6024:\n",
      "train loss: 2.930870416455219e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6025:\n",
      "train loss: 2.5792386016818615e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6026:\n",
      "train loss: 4.650324065861266e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6027:\n",
      "train loss: 4.2144842236829956e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6028:\n",
      "train loss: 2.973540977807399e-14\n",
      "lr: 8.015261056176542e-15\n",
      "Epoch 6029:\n",
      "train loss: 2.535015859655138e-14\n",
      "Epoch 06031: reducing learning rate of group 0 to 7.6145e-15.\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6030:\n",
      "train loss: 4.616953793519992e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6031:\n",
      "train loss: 4.231642439067521e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6032:\n",
      "train loss: 2.6213930193997692e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6033:\n",
      "train loss: 2.2564057006524207e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6034:\n",
      "train loss: 4.591238383060855e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6035:\n",
      "train loss: 4.156004090235258e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6036:\n",
      "train loss: 2.6302189624803777e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6037:\n",
      "train loss: 2.2831793993317548e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6038:\n",
      "train loss: 4.588747438453833e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6039:\n",
      "train loss: 4.179887251117227e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6040:\n",
      "train loss: 2.6392268946432706e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6041:\n",
      "train loss: 2.2018555986471245e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6042:\n",
      "train loss: 4.592008719640477e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6043:\n",
      "train loss: 4.227884192165922e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6044:\n",
      "train loss: 2.6026991264424705e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6045:\n",
      "train loss: 2.262677782932151e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6046:\n",
      "train loss: 4.583688531548465e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6047:\n",
      "train loss: 4.21973740397863e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6048:\n",
      "train loss: 2.691812356502853e-14\n",
      "lr: 7.614498003367715e-15\n",
      "Epoch 6049:\n",
      "train loss: 2.2943443978678055e-14\n",
      "Epoch 06051: reducing learning rate of group 0 to 7.2338e-15.\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6050:\n",
      "train loss: 4.549691854088095e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6051:\n",
      "train loss: 4.068749184972436e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6052:\n",
      "train loss: 2.4280433698954677e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6053:\n",
      "train loss: 2.076618702226587e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6054:\n",
      "train loss: 4.4330537454600266e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6055:\n",
      "train loss: 4.101778844855668e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6056:\n",
      "train loss: 2.5319149940954015e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6057:\n",
      "train loss: 2.131656237902205e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6058:\n",
      "train loss: 4.409424242754156e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6059:\n",
      "train loss: 4.027134103017781e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6060:\n",
      "train loss: 2.5754031365528663e-14\n",
      "lr: 7.233773103199328e-15\n",
      "Epoch 6061:\n",
      "train loss: 2.2138823860478205e-14\n",
      "Epoch 06063: reducing learning rate of group 0 to 6.8721e-15.\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6062:\n",
      "train loss: 4.277058129653217e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6063:\n",
      "train loss: 3.875786332960629e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6064:\n",
      "train loss: 2.2402478609007217e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6065:\n",
      "train loss: 1.9384126412536392e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6066:\n",
      "train loss: 4.239911394183328e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6067:\n",
      "train loss: 3.841420800564056e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6068:\n",
      "train loss: 2.3509404617658477e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6069:\n",
      "train loss: 2.0130281523022345e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6070:\n",
      "train loss: 4.189881554831109e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6071:\n",
      "train loss: 3.840906679634928e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6072:\n",
      "train loss: 2.4063933388186757e-14\n",
      "lr: 6.872084448039361e-15\n",
      "Epoch 6073:\n",
      "train loss: 2.07418509562038e-14\n",
      "Epoch 06075: reducing learning rate of group 0 to 6.5285e-15.\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6074:\n",
      "train loss: 4.0606935104691453e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6075:\n",
      "train loss: 3.6944237753413004e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6076:\n",
      "train loss: 2.1618000197752593e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6077:\n",
      "train loss: 1.9048961239628237e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6078:\n",
      "train loss: 3.808133238010598e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6079:\n",
      "train loss: 3.496022315141217e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6080:\n",
      "train loss: 2.3816930111733183e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6081:\n",
      "train loss: 2.037857209893983e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6082:\n",
      "train loss: 3.727224231093621e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6083:\n",
      "train loss: 3.4128580999541065e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6084:\n",
      "train loss: 2.5217648900556276e-14\n",
      "lr: 6.528480225637393e-15\n",
      "Epoch 6085:\n",
      "train loss: 2.226756420005129e-14\n",
      "Epoch 06087: reducing learning rate of group 0 to 6.2021e-15.\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6086:\n",
      "train loss: 3.575820329171109e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6087:\n",
      "train loss: 3.2122604631939184e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6088:\n",
      "train loss: 2.4268909694613252e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6089:\n",
      "train loss: 2.1440955000207738e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6090:\n",
      "train loss: 3.363136830684979e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6091:\n",
      "train loss: 2.984111803512257e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6092:\n",
      "train loss: 2.6229280688470838e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6093:\n",
      "train loss: 2.3873538623353174e-14\n",
      "lr: 6.202056214355523e-15\n",
      "Epoch 6094:\n",
      "train loss: 3.15593343260395e-14\n",
      "Epoch 06096: reducing learning rate of group 0 to 5.8920e-15.\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6095:\n",
      "train loss: 2.788246264224151e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6096:\n",
      "train loss: 2.818661491671549e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6097:\n",
      "train loss: 2.533554536523144e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6098:\n",
      "train loss: 2.7233046306766576e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6099:\n",
      "train loss: 2.3479488721986265e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6100:\n",
      "train loss: 2.920484482543226e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6101:\n",
      "train loss: 2.6915901302241555e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6102:\n",
      "train loss: 2.6838365746464526e-14\n",
      "lr: 5.891953403637747e-15\n",
      "Epoch 6103:\n",
      "train loss: 2.3629192765633495e-14\n",
      "Epoch 06105: reducing learning rate of group 0 to 5.5974e-15.\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6104:\n",
      "train loss: 2.969413650252185e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6105:\n",
      "train loss: 2.6617094722099254e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6106:\n",
      "train loss: 2.344980015021981e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6107:\n",
      "train loss: 2.0947040695913017e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6108:\n",
      "train loss: 2.992882607943504e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6109:\n",
      "train loss: 2.7205906806422962e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6110:\n",
      "train loss: 2.3049311405444353e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6111:\n",
      "train loss: 2.055374070638226e-14\n",
      "lr: 5.597355733455859e-15\n",
      "Epoch 6112:\n",
      "train loss: 2.999731456529208e-14\n",
      "Epoch 06114: reducing learning rate of group 0 to 5.3175e-15.\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6113:\n",
      "train loss: 2.7243164582881495e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6114:\n",
      "train loss: 2.3295730111742584e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6115:\n",
      "train loss: 2.0586195069049693e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6116:\n",
      "train loss: 2.7150089290520517e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6117:\n",
      "train loss: 2.4397384279010812e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6118:\n",
      "train loss: 2.2577756106680283e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6119:\n",
      "train loss: 2.0185126116939955e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6120:\n",
      "train loss: 2.7754519011668956e-14\n",
      "lr: 5.317487946783066e-15\n",
      "Epoch 6121:\n",
      "train loss: 2.5475551046985214e-14\n",
      "Epoch 06123: reducing learning rate of group 0 to 5.0516e-15.\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6122:\n",
      "train loss: 2.2548653705783794e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6123:\n",
      "train loss: 1.9779516543039674e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6124:\n",
      "train loss: 2.5672398002870804e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6125:\n",
      "train loss: 2.350898976218111e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6126:\n",
      "train loss: 2.258016935801832e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6127:\n",
      "train loss: 2.0594581682939945e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6128:\n",
      "train loss: 2.5937392813234625e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6129:\n",
      "train loss: 2.3253030451376156e-14\n",
      "lr: 5.0516135494439124e-15\n",
      "Epoch 6130:\n",
      "train loss: 2.2018942370432952e-14\n",
      "Epoch 06132: reducing learning rate of group 0 to 4.7990e-15.\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6131:\n",
      "train loss: 1.9736958033993002e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6132:\n",
      "train loss: 2.5653598958897244e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6133:\n",
      "train loss: 2.3273164444212804e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6134:\n",
      "train loss: 1.934952339411154e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6135:\n",
      "train loss: 1.7850136759061472e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6136:\n",
      "train loss: 2.588877774475047e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6137:\n",
      "train loss: 2.2927797384400426e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6138:\n",
      "train loss: 1.955890359347707e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6139:\n",
      "train loss: 1.7805698270445774e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6140:\n",
      "train loss: 2.4910284398109435e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6141:\n",
      "train loss: 2.2633678280230386e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6142:\n",
      "train loss: 2.0327300541188782e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6143:\n",
      "train loss: 1.7676570984551922e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6144:\n",
      "train loss: 2.531207514419354e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6145:\n",
      "train loss: 2.2375969295659787e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6146:\n",
      "train loss: 2.0019674630365217e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6147:\n",
      "train loss: 1.7969287289795785e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6148:\n",
      "train loss: 2.4413640731835925e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6149:\n",
      "train loss: 2.2197580961216407e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6150:\n",
      "train loss: 2.0769791442062347e-14\n",
      "lr: 4.7990328719717165e-15\n",
      "Epoch 6151:\n",
      "train loss: 1.8946469207984082e-14\n",
      "Epoch 06153: reducing learning rate of group 0 to 4.5591e-15.\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6152:\n",
      "train loss: 2.4361630870758292e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6153:\n",
      "train loss: 2.20958791066739e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6154:\n",
      "train loss: 1.95208542402766e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6155:\n",
      "train loss: 1.7410629549842014e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6156:\n",
      "train loss: 2.331345898112286e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6157:\n",
      "train loss: 2.138114148592692e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6158:\n",
      "train loss: 1.9736200743943385e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6159:\n",
      "train loss: 1.7906454684087975e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6160:\n",
      "train loss: 2.2825813819207307e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6161:\n",
      "train loss: 2.1218429750574698e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6162:\n",
      "train loss: 2.006890852452024e-14\n",
      "lr: 4.55908122837313e-15\n",
      "Epoch 6163:\n",
      "train loss: 1.8503250161097153e-14\n",
      "Epoch 06165: reducing learning rate of group 0 to 4.3311e-15.\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6164:\n",
      "train loss: 2.2794464037517165e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6165:\n",
      "train loss: 2.0538483487237135e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6166:\n",
      "train loss: 1.8236055684078067e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6167:\n",
      "train loss: 1.6006052929083598e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6168:\n",
      "train loss: 2.257671970641738e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6169:\n",
      "train loss: 1.971242879602923e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6170:\n",
      "train loss: 1.8675206711864254e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6171:\n",
      "train loss: 1.7201250558853133e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6172:\n",
      "train loss: 2.1240818304641816e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6173:\n",
      "train loss: 1.9669595331717222e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6174:\n",
      "train loss: 1.924106369326191e-14\n",
      "lr: 4.3311271669544734e-15\n",
      "Epoch 6175:\n",
      "train loss: 1.682315095706136e-14\n",
      "Epoch 06177: reducing learning rate of group 0 to 4.1146e-15.\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6176:\n",
      "train loss: 2.1486911886012366e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6177:\n",
      "train loss: 1.995339399167447e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6178:\n",
      "train loss: 1.7369561540717065e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6179:\n",
      "train loss: 1.5992369267153097e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6180:\n",
      "train loss: 2.1495288845027154e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6181:\n",
      "train loss: 1.983371270323997e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6182:\n",
      "train loss: 1.764682452404633e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6183:\n",
      "train loss: 1.6193612778846608e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6184:\n",
      "train loss: 2.1226964884192634e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6185:\n",
      "train loss: 1.9813223089793607e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6186:\n",
      "train loss: 1.7850171345552995e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6187:\n",
      "train loss: 1.545563947082988e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6188:\n",
      "train loss: 2.084995318590099e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6189:\n",
      "train loss: 1.9275699287609667e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6190:\n",
      "train loss: 1.786297244435539e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6191:\n",
      "train loss: 1.573093733970926e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6192:\n",
      "train loss: 2.1269884254533376e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6193:\n",
      "train loss: 1.9195368331104035e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6194:\n",
      "train loss: 1.7906089652765798e-14\n",
      "lr: 4.114570808606749e-15\n",
      "Epoch 6195:\n",
      "train loss: 1.5481312800429716e-14\n",
      "Epoch 06197: reducing learning rate of group 0 to 3.9088e-15.\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6196:\n",
      "train loss: 2.1156858244771633e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6197:\n",
      "train loss: 1.9535834993593796e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6198:\n",
      "train loss: 1.490208939291011e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6199:\n",
      "train loss: 1.359281310724338e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6200:\n",
      "train loss: 2.083690224094717e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6201:\n",
      "train loss: 1.9464606173821175e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6202:\n",
      "train loss: 1.502652173834714e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6203:\n",
      "train loss: 1.3896088065930257e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6204:\n",
      "train loss: 2.0433090143652227e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6205:\n",
      "train loss: 1.88739943117077e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6206:\n",
      "train loss: 1.5692189811489507e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6207:\n",
      "train loss: 1.3569543427220088e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6208:\n",
      "train loss: 2.0308331516830636e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6209:\n",
      "train loss: 1.9116234963532962e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6210:\n",
      "train loss: 1.614601324096893e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6211:\n",
      "train loss: 1.4162629751855967e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6212:\n",
      "train loss: 2.0398231363413215e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6213:\n",
      "train loss: 1.8630231354484958e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6214:\n",
      "train loss: 1.6223216054712304e-14\n",
      "lr: 3.908842268176412e-15\n",
      "Epoch 6215:\n",
      "train loss: 1.4944469614858947e-14\n",
      "Epoch 06217: reducing learning rate of group 0 to 3.7134e-15.\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6216:\n",
      "train loss: 1.9553907489172148e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6217:\n",
      "train loss: 1.8245135010161708e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6218:\n",
      "train loss: 1.4666557219589414e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6219:\n",
      "train loss: 1.376271982567848e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6220:\n",
      "train loss: 1.951630804394904e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6221:\n",
      "train loss: 1.7733620048394637e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6222:\n",
      "train loss: 1.5492836817508104e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6223:\n",
      "train loss: 1.4267754201309448e-14\n",
      "lr: 3.713400154767591e-15\n",
      "Epoch 6224:\n",
      "train loss: 1.905905652221996e-14\n",
      "Epoch 06226: reducing learning rate of group 0 to 3.5277e-15.\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6225:\n",
      "train loss: 1.8024972909287918e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6226:\n",
      "train loss: 1.5190080934250252e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6227:\n",
      "train loss: 1.4430812096599394e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6228:\n",
      "train loss: 1.753339382140786e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6229:\n",
      "train loss: 1.6623975547831344e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6230:\n",
      "train loss: 1.5574254213869302e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6231:\n",
      "train loss: 1.4169268931971701e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6232:\n",
      "train loss: 1.8003354421400327e-14\n",
      "lr: 3.5277301470292116e-15\n",
      "Epoch 6233:\n",
      "train loss: 1.6608420148467975e-14\n",
      "Epoch 06235: reducing learning rate of group 0 to 3.3513e-15.\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6234:\n",
      "train loss: 1.55822388373049e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6235:\n",
      "train loss: 1.465282740234375e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6236:\n",
      "train loss: 1.5495468679584503e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6237:\n",
      "train loss: 1.50417126601516e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6238:\n",
      "train loss: 1.54822915329808e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6239:\n",
      "train loss: 1.4327217871498007e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6240:\n",
      "train loss: 1.5444280122023388e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6241:\n",
      "train loss: 1.4084111224650119e-14\n",
      "lr: 3.351343639677751e-15\n",
      "Epoch 6242:\n",
      "train loss: 1.582274269874429e-14\n",
      "Epoch 06244: reducing learning rate of group 0 to 3.1838e-15.\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6243:\n",
      "train loss: 1.4325077353466815e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6244:\n",
      "train loss: 1.5565474414160698e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6245:\n",
      "train loss: 1.42788519830579e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6246:\n",
      "train loss: 1.465269718960795e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6247:\n",
      "train loss: 1.2950217577877718e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6248:\n",
      "train loss: 1.5820091962945267e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6249:\n",
      "train loss: 1.4072450659814505e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6250:\n",
      "train loss: 1.4172468517043976e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6251:\n",
      "train loss: 1.3251451323036932e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6252:\n",
      "train loss: 1.5412830963812693e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6253:\n",
      "train loss: 1.3716817249076947e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6254:\n",
      "train loss: 1.480653749917538e-14\n",
      "lr: 3.1837764576938634e-15\n",
      "Epoch 6255:\n",
      "train loss: 1.3362014924073004e-14\n",
      "Epoch 06257: reducing learning rate of group 0 to 3.0246e-15.\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6256:\n",
      "train loss: 1.577988487972415e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6257:\n",
      "train loss: 1.4038812104659548e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6258:\n",
      "train loss: 1.387310178605306e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6259:\n",
      "train loss: 1.2112854853214238e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6260:\n",
      "train loss: 1.538098181561159e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6261:\n",
      "train loss: 1.4210532009182954e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6262:\n",
      "train loss: 1.3879234686417493e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6263:\n",
      "train loss: 1.2630654225232792e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6264:\n",
      "train loss: 1.500318454045117e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6265:\n",
      "train loss: 1.3594669366442393e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6266:\n",
      "train loss: 1.3495975947465618e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6267:\n",
      "train loss: 1.1721854819901392e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6268:\n",
      "train loss: 1.5527214760721752e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6269:\n",
      "train loss: 1.4054486534695945e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6270:\n",
      "train loss: 1.3578532209284801e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6271:\n",
      "train loss: 1.231932435593859e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6272:\n",
      "train loss: 1.507815857983887e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6273:\n",
      "train loss: 1.3631271144809692e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6274:\n",
      "train loss: 1.4308935674765634e-14\n",
      "lr: 3.02458763480917e-15\n",
      "Epoch 6275:\n",
      "train loss: 1.2634409673216688e-14\n",
      "Epoch 06277: reducing learning rate of group 0 to 2.8734e-15.\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6276:\n",
      "train loss: 1.538996175093605e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6277:\n",
      "train loss: 1.3958931709590578e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6278:\n",
      "train loss: 1.1811498580643315e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6279:\n",
      "train loss: 1.1149981272612297e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6280:\n",
      "train loss: 1.460808933067124e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6281:\n",
      "train loss: 1.3451944718063842e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6282:\n",
      "train loss: 1.2435573070681931e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6283:\n",
      "train loss: 1.1507456131871237e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6284:\n",
      "train loss: 1.4474622622213005e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6285:\n",
      "train loss: 1.3168463644422936e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6286:\n",
      "train loss: 1.3122276356747215e-14\n",
      "lr: 2.8733582530687113e-15\n",
      "Epoch 6287:\n",
      "train loss: 1.1698060436403644e-14\n",
      "Epoch 06289: reducing learning rate of group 0 to 2.7297e-15.\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6288:\n",
      "train loss: 1.3674882558666787e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6289:\n",
      "train loss: 1.2830178877771782e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6290:\n",
      "train loss: 1.198682857802047e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6291:\n",
      "train loss: 1.0978351727976417e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6292:\n",
      "train loss: 1.3454190472521419e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6293:\n",
      "train loss: 1.2107931006851183e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6294:\n",
      "train loss: 1.2447062217142031e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6295:\n",
      "train loss: 1.1342638554885574e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6296:\n",
      "train loss: 1.3417085258172647e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6297:\n",
      "train loss: 1.216430962736975e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6298:\n",
      "train loss: 1.2273309028691406e-14\n",
      "lr: 2.7296903404152756e-15\n",
      "Epoch 6299:\n",
      "train loss: 1.1092348531097905e-14\n",
      "Epoch 06301: reducing learning rate of group 0 to 2.5932e-15.\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6300:\n",
      "train loss: 1.3122610921427854e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6301:\n",
      "train loss: 1.247723395378686e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6302:\n",
      "train loss: 1.2189996683837657e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6303:\n",
      "train loss: 1.1093165891331403e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6304:\n",
      "train loss: 1.2777313055203373e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6305:\n",
      "train loss: 1.1922328090628382e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6306:\n",
      "train loss: 1.2354706753966455e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6307:\n",
      "train loss: 1.1283139472471449e-14\n",
      "lr: 2.5932058233945116e-15\n",
      "Epoch 6308:\n",
      "train loss: 1.233505156717355e-14\n",
      "Epoch 06310: reducing learning rate of group 0 to 2.4635e-15.\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6309:\n",
      "train loss: 1.1773576895392083e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6310:\n",
      "train loss: 1.2329756123080825e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6311:\n",
      "train loss: 1.1888083466581722e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6312:\n",
      "train loss: 1.1055450829032618e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6313:\n",
      "train loss: 1.0128047611051461e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6314:\n",
      "train loss: 1.2411475070835428e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6315:\n",
      "train loss: 1.179830935456103e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6316:\n",
      "train loss: 1.1087942885764545e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6317:\n",
      "train loss: 1.0286191557922275e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6318:\n",
      "train loss: 1.2281274951884125e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6319:\n",
      "train loss: 1.2178666646833678e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6320:\n",
      "train loss: 1.1071297567515595e-14\n",
      "lr: 2.4635455322247857e-15\n",
      "Epoch 6321:\n",
      "train loss: 1.027090674704271e-14\n",
      "Epoch 06323: reducing learning rate of group 0 to 2.3404e-15.\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6322:\n",
      "train loss: 1.2757022448554932e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6323:\n",
      "train loss: 1.1359277621184272e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6324:\n",
      "train loss: 9.087441040640633e-15\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6325:\n",
      "train loss: 8.402029825056407e-15\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6326:\n",
      "train loss: 1.2436090590484568e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6327:\n",
      "train loss: 1.1602539936874608e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6328:\n",
      "train loss: 9.894450356912008e-15\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6329:\n",
      "train loss: 8.30043170042643e-15\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6330:\n",
      "train loss: 1.2260761211936366e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6331:\n",
      "train loss: 1.1249128546196583e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6332:\n",
      "train loss: 9.985935627200295e-15\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6333:\n",
      "train loss: 9.426515242463469e-15\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6334:\n",
      "train loss: 1.1427449563170697e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6335:\n",
      "train loss: 1.0103701306051542e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6336:\n",
      "train loss: 1.0770163893366587e-14\n",
      "lr: 2.340368255613546e-15\n",
      "Epoch 6337:\n",
      "train loss: 9.924869903066537e-15\n",
      "Epoch 06339: reducing learning rate of group 0 to 2.2233e-15.\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6338:\n",
      "train loss: 1.0985810199729059e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6339:\n",
      "train loss: 9.887311853332163e-15\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6340:\n",
      "train loss: 9.738398377545463e-15\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6341:\n",
      "train loss: 9.35270170790804e-15\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6342:\n",
      "train loss: 1.085236064352454e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6343:\n",
      "train loss: 9.217354310957894e-15\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6344:\n",
      "train loss: 1.0343942231836617e-14\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6345:\n",
      "train loss: 9.492009132512985e-15\n",
      "lr: 2.2233498428328687e-15\n",
      "Epoch 6346:\n",
      "train loss: 1.0884278753862588e-14\n",
      "Epoch 06348: reducing learning rate of group 0 to 2.1122e-15.\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6347:\n",
      "train loss: 9.488529536996692e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6348:\n",
      "train loss: 9.594643282491236e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6349:\n",
      "train loss: 9.409510391453714e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6350:\n",
      "train loss: 9.590291532732685e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6351:\n",
      "train loss: 9.088942725125474e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6352:\n",
      "train loss: 1.0396667473956548e-14\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6353:\n",
      "train loss: 9.267431139896088e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6354:\n",
      "train loss: 9.61054720619319e-15\n",
      "lr: 2.112182350691225e-15\n",
      "Epoch 6355:\n",
      "train loss: 9.322101193281728e-15\n",
      "Epoch 06357: reducing learning rate of group 0 to 2.0066e-15.\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6356:\n",
      "train loss: 9.958726215930582e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6357:\n",
      "train loss: 9.145143772220708e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6358:\n",
      "train loss: 8.844635185413475e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6359:\n",
      "train loss: 8.66752370871371e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6360:\n",
      "train loss: 1.0086450454362173e-14\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6361:\n",
      "train loss: 8.728123422711177e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6362:\n",
      "train loss: 9.529001823054513e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6363:\n",
      "train loss: 9.129620492681632e-15\n",
      "lr: 2.0065732331566636e-15\n",
      "Epoch 6364:\n",
      "train loss: 9.078903831873194e-15\n",
      "Epoch 06366: reducing learning rate of group 0 to 1.9062e-15.\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6365:\n",
      "train loss: 8.691573999748118e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6366:\n",
      "train loss: 8.96466223121407e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6367:\n",
      "train loss: 9.024276501871796e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6368:\n",
      "train loss: 8.506621095875574e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6369:\n",
      "train loss: 8.124753619345498e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6370:\n",
      "train loss: 9.452616585201767e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6371:\n",
      "train loss: 8.795779405094715e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6372:\n",
      "train loss: 8.358263064989762e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6373:\n",
      "train loss: 7.778851945298267e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6374:\n",
      "train loss: 1.0089655878624533e-14\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6375:\n",
      "train loss: 9.099335411940259e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6376:\n",
      "train loss: 8.374210387906159e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6377:\n",
      "train loss: 7.78481677140971e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6378:\n",
      "train loss: 9.80210312294814e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6379:\n",
      "train loss: 9.383537610906823e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6380:\n",
      "train loss: 8.420854654800515e-15\n",
      "lr: 1.90624457149883e-15\n",
      "Epoch 6381:\n",
      "train loss: 7.952981374390561e-15\n",
      "Epoch 06383: reducing learning rate of group 0 to 1.8109e-15.\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6382:\n",
      "train loss: 9.914784589430616e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6383:\n",
      "train loss: 8.939078450344438e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6384:\n",
      "train loss: 6.875331823051144e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6385:\n",
      "train loss: 5.901743608964094e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6386:\n",
      "train loss: 1.0145614253573446e-14\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6387:\n",
      "train loss: 9.58186530871168e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6388:\n",
      "train loss: 7.213849925220471e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6389:\n",
      "train loss: 6.1504569898859985e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6390:\n",
      "train loss: 9.639693446453315e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6391:\n",
      "train loss: 9.115443040780368e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6392:\n",
      "train loss: 7.201347088713449e-15\n",
      "lr: 1.8109323429238885e-15\n",
      "Epoch 6393:\n",
      "train loss: 6.811252902044171e-15\n",
      "Epoch 06395: reducing learning rate of group 0 to 1.7204e-15.\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6394:\n",
      "train loss: 9.244179225190213e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6395:\n",
      "train loss: 8.339784573208501e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6396:\n",
      "train loss: 6.354098374117418e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6397:\n",
      "train loss: 5.7964898651869416e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6398:\n",
      "train loss: 8.50200301192148e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6399:\n",
      "train loss: 7.732536671350624e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6400:\n",
      "train loss: 7.538730506780525e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6401:\n",
      "train loss: 7.363307820889615e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6402:\n",
      "train loss: 7.499683830488344e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6403:\n",
      "train loss: 6.991097812266902e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6404:\n",
      "train loss: 8.4300207084051e-15\n",
      "lr: 1.720385725777694e-15\n",
      "Epoch 6405:\n",
      "train loss: 8.24043754892454e-15\n",
      "Epoch 06407: reducing learning rate of group 0 to 1.6344e-15.\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6406:\n",
      "train loss: 6.9809552123143016e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6407:\n",
      "train loss: 6.476126777602257e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6408:\n",
      "train loss: 7.961088734392762e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6409:\n",
      "train loss: 7.392345032567144e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6410:\n",
      "train loss: 6.242260503675191e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6411:\n",
      "train loss: 6.7273204684367275e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6412:\n",
      "train loss: 8.147013421983668e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6413:\n",
      "train loss: 7.544546458851716e-15\n",
      "lr: 1.634366439488809e-15\n",
      "Epoch 6414:\n",
      "train loss: 6.671512050765686e-15\n",
      "Epoch 06416: reducing learning rate of group 0 to 1.5526e-15.\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6415:\n",
      "train loss: 6.004661097178648e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6416:\n",
      "train loss: 7.949350594599744e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6417:\n",
      "train loss: 7.260987080737556e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6418:\n",
      "train loss: 6.521312973586585e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6419:\n",
      "train loss: 5.925543893599899e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6420:\n",
      "train loss: 8.279726137279923e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6421:\n",
      "train loss: 7.0661489746392074e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6422:\n",
      "train loss: 6.476820077370385e-15\n",
      "lr: 1.5526481175143685e-15\n",
      "Epoch 6423:\n",
      "train loss: 6.083296469418246e-15\n",
      "Epoch 06425: reducing learning rate of group 0 to 1.4750e-15.\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6424:\n",
      "train loss: 7.575125702299788e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6425:\n",
      "train loss: 6.9983711917708814e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6426:\n",
      "train loss: 6.753557582553787e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6427:\n",
      "train loss: 6.311065342454117e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6428:\n",
      "train loss: 6.5158302114319766e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6429:\n",
      "train loss: 5.846101904420179e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6430:\n",
      "train loss: 7.033803601609935e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6431:\n",
      "train loss: 6.598275808013635e-15\n",
      "lr: 1.4750157116386501e-15\n",
      "Epoch 6432:\n",
      "train loss: 6.7653978892547365e-15\n",
      "Epoch 06434: reducing learning rate of group 0 to 1.4013e-15.\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6433:\n",
      "train loss: 6.282483969951924e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6434:\n",
      "train loss: 6.597093156187274e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6435:\n",
      "train loss: 6.473964989054048e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6436:\n",
      "train loss: 6.219700597303407e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6437:\n",
      "train loss: 5.3521934599468695e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6438:\n",
      "train loss: 6.566380119438308e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6439:\n",
      "train loss: 6.217569751081557e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6440:\n",
      "train loss: 6.1099459177081995e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6441:\n",
      "train loss: 5.485892987411802e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6442:\n",
      "train loss: 6.354477917390551e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6443:\n",
      "train loss: 5.846038066318929e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6444:\n",
      "train loss: 6.695988250141854e-15\n",
      "lr: 1.4012649260567176e-15\n",
      "Epoch 6445:\n",
      "train loss: 6.356767831089272e-15\n",
      "Epoch 06447: reducing learning rate of group 0 to 1.3312e-15.\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6446:\n",
      "train loss: 6.403239686654306e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6447:\n",
      "train loss: 5.951297890256184e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6448:\n",
      "train loss: 5.958422073619732e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6449:\n",
      "train loss: 5.829949576398147e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6450:\n",
      "train loss: 6.2212535928044234e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6451:\n",
      "train loss: 5.284264401857905e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6452:\n",
      "train loss: 6.536745758433409e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6453:\n",
      "train loss: 5.7320731807920545e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6454:\n",
      "train loss: 5.5394941896950615e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6455:\n",
      "train loss: 5.2567341138403135e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6456:\n",
      "train loss: 6.371464564023231e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6457:\n",
      "train loss: 5.619387431161518e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6458:\n",
      "train loss: 5.5695395182739505e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6459:\n",
      "train loss: 5.284470699417431e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6460:\n",
      "train loss: 5.656219948932517e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6461:\n",
      "train loss: 5.4512010602623354e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6462:\n",
      "train loss: 6.710702920597887e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6463:\n",
      "train loss: 5.875696267914584e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6464:\n",
      "train loss: 4.97557443063821e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6465:\n",
      "train loss: 5.2316499266766195e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6466:\n",
      "train loss: 7.115904501525033e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6467:\n",
      "train loss: 6.619380881997196e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6468:\n",
      "train loss: 5.540677205096269e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6469:\n",
      "train loss: 4.449559508764462e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6470:\n",
      "train loss: 6.88611825947025e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6471:\n",
      "train loss: 6.626454469933649e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6472:\n",
      "train loss: 5.4412257117463434e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6473:\n",
      "train loss: 5.3600808716455946e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6474:\n",
      "train loss: 6.747679153404386e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6475:\n",
      "train loss: 5.909976775001043e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6476:\n",
      "train loss: 5.831541498809895e-15\n",
      "lr: 1.3312016797538816e-15\n",
      "Epoch 6477:\n",
      "train loss: 6.003771282418036e-15\n",
      "Epoch 06479: reducing learning rate of group 0 to 1.2646e-15.\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6478:\n",
      "train loss: 6.419782476945723e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6479:\n",
      "train loss: 5.44370885731379e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6480:\n",
      "train loss: 5.154382312147582e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6481:\n",
      "train loss: 4.8456836002024505e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6482:\n",
      "train loss: 5.870725059881068e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6483:\n",
      "train loss: 5.757109406144343e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6484:\n",
      "train loss: 5.27164070188188e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6485:\n",
      "train loss: 4.731960677411248e-15\n",
      "lr: 1.2646415957661875e-15\n",
      "Epoch 6486:\n",
      "train loss: 6.069732784820221e-15\n",
      "Epoch 06488: reducing learning rate of group 0 to 1.2014e-15.\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6487:\n",
      "train loss: 5.5469778870523515e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6488:\n",
      "train loss: 5.355194819237811e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6489:\n",
      "train loss: 5.193933701833323e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6490:\n",
      "train loss: 4.892347157847434e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6491:\n",
      "train loss: 4.957528595981889e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6492:\n",
      "train loss: 5.433442065209727e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6493:\n",
      "train loss: 5.192018944884104e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6494:\n",
      "train loss: 5.1964922614946845e-15\n",
      "lr: 1.2014095159778781e-15\n",
      "Epoch 6495:\n",
      "train loss: 4.78455855128246e-15\n",
      "Epoch 06497: reducing learning rate of group 0 to 1.1413e-15.\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6496:\n",
      "train loss: 5.168871495051593e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6497:\n",
      "train loss: 5.1892323833394956e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6498:\n",
      "train loss: 4.5152496551461864e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6499:\n",
      "train loss: 3.975249979200674e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6500:\n",
      "train loss: 5.096760606313117e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6501:\n",
      "train loss: 4.89027458652203e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6502:\n",
      "train loss: 4.673398898457274e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6503:\n",
      "train loss: 4.64199855174037e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6504:\n",
      "train loss: 4.377539021568679e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6505:\n",
      "train loss: 4.7775550136804625e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6506:\n",
      "train loss: 5.312215184963758e-15\n",
      "lr: 1.1413390401789842e-15\n",
      "Epoch 6507:\n",
      "train loss: 4.863338598718952e-15\n",
      "Epoch 06509: reducing learning rate of group 0 to 1.0843e-15.\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6508:\n",
      "train loss: 4.211522726862028e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6509:\n",
      "train loss: 4.25484285890019e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6510:\n",
      "train loss: 4.653270462005189e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6511:\n",
      "train loss: 4.711025981936224e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6512:\n",
      "train loss: 4.445800544411742e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6513:\n",
      "train loss: 4.036265625518329e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6514:\n",
      "train loss: 4.694866896746522e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6515:\n",
      "train loss: 4.796384195532461e-15\n",
      "lr: 1.084272088170035e-15\n",
      "Epoch 6516:\n",
      "train loss: 5.015668655227237e-15\n",
      "Epoch 06518: reducing learning rate of group 0 to 1.0301e-15.\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6517:\n",
      "train loss: 5.154726135051711e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6518:\n",
      "train loss: 4.4590981012654265e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6519:\n",
      "train loss: 3.872232735865306e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6520:\n",
      "train loss: 4.894966628154353e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6521:\n",
      "train loss: 4.7122963330516346e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6522:\n",
      "train loss: 4.679823694842974e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6523:\n",
      "train loss: 4.137982965764679e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6524:\n",
      "train loss: 4.380718817552606e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6525:\n",
      "train loss: 4.103895702904171e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6526:\n",
      "train loss: 5.317140573731218e-15\n",
      "lr: 1.0300584837615332e-15\n",
      "Epoch 6527:\n",
      "train loss: 4.645754794578771e-15\n",
      "Epoch 06529: reducing learning rate of group 0 to 9.7856e-16.\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6528:\n",
      "train loss: 4.8180310501790585e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6529:\n",
      "train loss: 4.618896087210626e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6530:\n",
      "train loss: 3.9355492237938624e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6531:\n",
      "train loss: 3.777820268145119e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6532:\n",
      "train loss: 4.9466760690575195e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6533:\n",
      "train loss: 4.4743248982738205e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6534:\n",
      "train loss: 4.2710674002833745e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6535:\n",
      "train loss: 4.032548554821986e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6536:\n",
      "train loss: 4.741584667686918e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6537:\n",
      "train loss: 4.53208428538262e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6538:\n",
      "train loss: 4.090987896875364e-15\n",
      "lr: 9.785555595734565e-16\n",
      "Epoch 6539:\n",
      "train loss: 3.973424284226539e-15\n",
      "Epoch 06541: reducing learning rate of group 0 to 9.2963e-16.\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6540:\n",
      "train loss: 4.870439939533943e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6541:\n",
      "train loss: 4.292769905138624e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6542:\n",
      "train loss: 3.926253402396756e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6543:\n",
      "train loss: 3.6939129728209725e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6544:\n",
      "train loss: 4.312438007114631e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6545:\n",
      "train loss: 4.0517511051562736e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6546:\n",
      "train loss: 3.951085906678807e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6547:\n",
      "train loss: 3.801604784987547e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6548:\n",
      "train loss: 4.047972235359616e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6549:\n",
      "train loss: 4.118737988178409e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6550:\n",
      "train loss: 4.459159753610787e-15\n",
      "lr: 9.296277815947836e-16\n",
      "Epoch 6551:\n",
      "train loss: 4.273662361904774e-15\n",
      "Epoch 06553: reducing learning rate of group 0 to 8.8315e-16.\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6552:\n",
      "train loss: 3.944482227543e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6553:\n",
      "train loss: 3.766470833883062e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6554:\n",
      "train loss: 4.0316554764436625e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6555:\n",
      "train loss: 3.4083364785164543e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6556:\n",
      "train loss: 4.13046122564478e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6557:\n",
      "train loss: 4.166413164746033e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6558:\n",
      "train loss: 3.874929521824908e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6559:\n",
      "train loss: 3.850836171335961e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6560:\n",
      "train loss: 3.822754018176651e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6561:\n",
      "train loss: 3.827072915609079e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6562:\n",
      "train loss: 4.0733139517990115e-15\n",
      "lr: 8.831463925150444e-16\n",
      "Epoch 6563:\n",
      "train loss: 3.4992931779672584e-15\n",
      "Epoch 06565: reducing learning rate of group 0 to 8.3899e-16.\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6564:\n",
      "train loss: 4.245357046302396e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6565:\n",
      "train loss: 3.6610464758146595e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6566:\n",
      "train loss: 3.835035862894568e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6567:\n",
      "train loss: 3.4532486643289473e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6568:\n",
      "train loss: 3.830309356343269e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6569:\n",
      "train loss: 3.631100483091185e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6570:\n",
      "train loss: 4.174038415803148e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6571:\n",
      "train loss: 3.9885113206598415e-15\n",
      "lr: 8.389890728892921e-16\n",
      "Epoch 6572:\n",
      "train loss: 3.911607937844839e-15\n",
      "Epoch 06574: reducing learning rate of group 0 to 7.9704e-16.\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6573:\n",
      "train loss: 3.652045328863312e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6574:\n",
      "train loss: 4.174549871076647e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6575:\n",
      "train loss: 3.957814071968689e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6576:\n",
      "train loss: 2.7383612629692645e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6577:\n",
      "train loss: 2.8907251071544214e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6578:\n",
      "train loss: 3.81582366778875e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6579:\n",
      "train loss: 3.306277274760387e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6580:\n",
      "train loss: 3.968129911260356e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6581:\n",
      "train loss: 3.3287295927016984e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6582:\n",
      "train loss: 2.775004583776243e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6583:\n",
      "train loss: 2.2495017608151746e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6584:\n",
      "train loss: 3.8378472449242555e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6585:\n",
      "train loss: 3.284950617135113e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6586:\n",
      "train loss: 4.021429789879185e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6587:\n",
      "train loss: 4.241312401857154e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6588:\n",
      "train loss: 2.3778583028788767e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6589:\n",
      "train loss: 2.79987818478633e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6590:\n",
      "train loss: 4.7191012510765114e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6591:\n",
      "train loss: 4.528094236190826e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6592:\n",
      "train loss: 2.1482130341973495e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6593:\n",
      "train loss: 2.7048613764238254e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6594:\n",
      "train loss: 2.9842721616594907e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6595:\n",
      "train loss: 3.0007158974808724e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6596:\n",
      "train loss: 5.318026772700042e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6597:\n",
      "train loss: 4.961515661569028e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6598:\n",
      "train loss: 1.5444465178799165e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6599:\n",
      "train loss: 2.453883249864878e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6600:\n",
      "train loss: 2.4641025485120074e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6601:\n",
      "train loss: 1.4233060501361968e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6602:\n",
      "train loss: 3.3778513131051297e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6603:\n",
      "train loss: 1.987520653403721e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6604:\n",
      "train loss: 4.839303310361516e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6605:\n",
      "train loss: 5.04878522856257e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6606:\n",
      "train loss: 8.877475805197073e-16\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6607:\n",
      "train loss: 5.123624249967261e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6608:\n",
      "train loss: 5.486193157309011e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6609:\n",
      "train loss: 2.5787524150969028e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6610:\n",
      "train loss: 4.461917291986017e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6611:\n",
      "train loss: 4.3287810320500285e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6612:\n",
      "train loss: 3.126202697911594e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6613:\n",
      "train loss: 3.688414020935738e-15\n",
      "lr: 7.970396192448275e-16\n",
      "Epoch 6614:\n",
      "train loss: 2.7448341379188916e-15\n",
      "Epoch 06616: reducing learning rate of group 0 to 7.5719e-16.\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6615:\n",
      "train loss: 3.883679581593509e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6616:\n",
      "train loss: 3.46771947544286e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6617:\n",
      "train loss: 3.067535900648648e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6618:\n",
      "train loss: 3.51373003530572e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6619:\n",
      "train loss: 2.5546032673388503e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6620:\n",
      "train loss: 3.1217505277840063e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6621:\n",
      "train loss: 2.4299808519532728e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6622:\n",
      "train loss: 3.738577033119828e-15\n",
      "lr: 7.571876382825861e-16\n",
      "Epoch 6623:\n",
      "train loss: 3.55765137499899e-15\n",
      "Epoch 06625: reducing learning rate of group 0 to 7.1933e-16.\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6624:\n",
      "train loss: 2.757102038587325e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6625:\n",
      "train loss: 2.992112264256782e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6626:\n",
      "train loss: 3.2491260931659843e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6627:\n",
      "train loss: 2.2008956505284456e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6628:\n",
      "train loss: 3.854535599356954e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6629:\n",
      "train loss: 2.684417630120099e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6630:\n",
      "train loss: 3.701052669543453e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6631:\n",
      "train loss: 3.437172066483636e-15\n",
      "lr: 7.193282563684567e-16\n",
      "Epoch 6632:\n",
      "train loss: 3.456056885864301e-15\n",
      "Epoch 06634: reducing learning rate of group 0 to 6.8336e-16.\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6633:\n",
      "train loss: 3.4025869792020353e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6634:\n",
      "train loss: 2.9208264095023842e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6635:\n",
      "train loss: 2.9372820753730885e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6636:\n",
      "train loss: 3.3664823374310435e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6637:\n",
      "train loss: 3.2448085105756774e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6638:\n",
      "train loss: 2.2653582834148957e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6639:\n",
      "train loss: 2.0633449877967147e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6640:\n",
      "train loss: 3.6845129265927755e-15\n",
      "lr: 6.833618435500339e-16\n",
      "Epoch 6641:\n",
      "train loss: 3.1512257461729834e-15\n",
      "Epoch 06643: reducing learning rate of group 0 to 6.4919e-16.\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6642:\n",
      "train loss: 2.909108053521684e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6643:\n",
      "train loss: 2.956191730489673e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6644:\n",
      "train loss: 1.9085854996507655e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6645:\n",
      "train loss: 2.0094705627025627e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6646:\n",
      "train loss: 3.2242253581954976e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6647:\n",
      "train loss: 3.281673381584159e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6648:\n",
      "train loss: 2.431905010545359e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6649:\n",
      "train loss: 2.287080240347496e-15\n",
      "lr: 6.491937513725322e-16\n",
      "Epoch 6650:\n",
      "train loss: 3.1033574670411362e-15\n",
      "Epoch 06652: reducing learning rate of group 0 to 6.1673e-16.\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6651:\n",
      "train loss: 2.842710741851045e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6652:\n",
      "train loss: 2.860131454226794e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6653:\n",
      "train loss: 2.536500677019722e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6654:\n",
      "train loss: 2.684451835620212e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6655:\n",
      "train loss: 2.0632936800065497e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6656:\n",
      "train loss: 2.523223649152792e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6657:\n",
      "train loss: 2.58160687200919e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6658:\n",
      "train loss: 2.6909729859092954e-15\n",
      "lr: 6.167340638039056e-16\n",
      "Epoch 6659:\n",
      "train loss: 2.208410751563774e-15\n",
      "Epoch 06661: reducing learning rate of group 0 to 5.8590e-16.\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6660:\n",
      "train loss: 2.9525459630933947e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6661:\n",
      "train loss: 2.9127332735683702e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6662:\n",
      "train loss: 2.1208122268928104e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6663:\n",
      "train loss: 2.576871219939873e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6664:\n",
      "train loss: 3.0331448764598665e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6665:\n",
      "train loss: 2.7862430630972233e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6666:\n",
      "train loss: 2.4956195036950633e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6667:\n",
      "train loss: 2.1766551202047006e-15\n",
      "lr: 5.858973606137102e-16\n",
      "Epoch 6668:\n",
      "train loss: 2.6859048656432652e-15\n",
      "Epoch 06670: reducing learning rate of group 0 to 5.5660e-16.\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6669:\n",
      "train loss: 2.4469870826842054e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6670:\n",
      "train loss: 2.5448330429160108e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6671:\n",
      "train loss: 2.4005160664160173e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6672:\n",
      "train loss: 2.557857533689183e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6673:\n",
      "train loss: 2.359836034269936e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6674:\n",
      "train loss: 2.7809357315217345e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6675:\n",
      "train loss: 2.6175888115314297e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6676:\n",
      "train loss: 2.006906855187616e-15\n",
      "lr: 5.566024925830247e-16\n",
      "Epoch 6677:\n",
      "train loss: 2.2093735810783626e-15\n",
      "Epoch 06679: reducing learning rate of group 0 to 5.2877e-16.\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6678:\n",
      "train loss: 2.7220985788005394e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6679:\n",
      "train loss: 2.9530718133482954e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6680:\n",
      "train loss: 1.9702026204516405e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6681:\n",
      "train loss: 2.4264370331196985e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6682:\n",
      "train loss: 2.5625106182263166e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6683:\n",
      "train loss: 2.481845336360819e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6684:\n",
      "train loss: 2.21820836654712e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6685:\n",
      "train loss: 2.4527249427825065e-15\n",
      "lr: 5.287723679538735e-16\n",
      "Epoch 6686:\n",
      "train loss: 2.51586304317314e-15\n",
      "Epoch 06688: reducing learning rate of group 0 to 5.0233e-16.\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6687:\n",
      "train loss: 2.6119255344015554e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6688:\n",
      "train loss: 2.501108358774995e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6689:\n",
      "train loss: 2.241713523932856e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6690:\n",
      "train loss: 2.118904246320212e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6691:\n",
      "train loss: 2.1111523103216485e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6692:\n",
      "train loss: 2.4489412007538013e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6693:\n",
      "train loss: 2.539087899260612e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6694:\n",
      "train loss: 2.3673706046098743e-15\n",
      "lr: 5.023337495561798e-16\n",
      "Epoch 6695:\n",
      "train loss: 2.1862071380125645e-15\n",
      "Epoch 06697: reducing learning rate of group 0 to 4.7722e-16.\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6696:\n",
      "train loss: 2.2485331210483347e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6697:\n",
      "train loss: 2.302916612768419e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6698:\n",
      "train loss: 2.5798562437122713e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6699:\n",
      "train loss: 2.3105388744994102e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6700:\n",
      "train loss: 2.1172155881271382e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6701:\n",
      "train loss: 2.0062979247196336e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6702:\n",
      "train loss: 2.673503185453868e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6703:\n",
      "train loss: 2.8031955524871596e-15\n",
      "lr: 4.772170620783707e-16\n",
      "Epoch 6704:\n",
      "train loss: 1.869713616262007e-15\n",
      "Epoch 06706: reducing learning rate of group 0 to 4.5336e-16.\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6705:\n",
      "train loss: 2.132158753386717e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6706:\n",
      "train loss: 2.4715762008691793e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6707:\n",
      "train loss: 2.564749214472051e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6708:\n",
      "train loss: 2.170569442997355e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6709:\n",
      "train loss: 2.2906545106580735e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6710:\n",
      "train loss: 2.3017827963941373e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6711:\n",
      "train loss: 2.5370152044884143e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6712:\n",
      "train loss: 2.0500637661491405e-15\n",
      "lr: 4.533562089744522e-16\n",
      "Epoch 6713:\n",
      "train loss: 1.737439519223962e-15\n",
      "Epoch 06715: reducing learning rate of group 0 to 4.3069e-16.\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6714:\n",
      "train loss: 2.326072492734769e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6715:\n",
      "train loss: 2.5998982314774424e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6716:\n",
      "train loss: 2.445238127012152e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6717:\n",
      "train loss: 2.6806326278587702e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6718:\n",
      "train loss: 2.205715910870497e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6719:\n",
      "train loss: 1.9687450178180273e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6720:\n",
      "train loss: 2.9508902887195364e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6721:\n",
      "train loss: 2.817612433380861e-15\n",
      "lr: 4.306883985257295e-16\n",
      "Epoch 6722:\n",
      "train loss: 1.9077331129407454e-15\n",
      "Epoch 06724: reducing learning rate of group 0 to 4.0915e-16.\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6723:\n",
      "train loss: 1.7657858365714746e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6724:\n",
      "train loss: 2.6470547694535926e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6725:\n",
      "train loss: 2.571457152228381e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6726:\n",
      "train loss: 1.856647010231146e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6727:\n",
      "train loss: 2.1228076006558443e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6728:\n",
      "train loss: 1.882452951499667e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6729:\n",
      "train loss: 2.2253800511729224e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6730:\n",
      "train loss: 2.1471139878328523e-15\n",
      "lr: 4.09153978599443e-16\n",
      "Epoch 6731:\n",
      "train loss: 2.0861598976463317e-15\n",
      "Epoch 06733: reducing learning rate of group 0 to 3.8870e-16.\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6732:\n",
      "train loss: 2.3880652836792e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6733:\n",
      "train loss: 2.157869351993136e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6734:\n",
      "train loss: 2.0399398441799998e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6735:\n",
      "train loss: 1.9231400551605998e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6736:\n",
      "train loss: 2.4962714354881854e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6737:\n",
      "train loss: 1.9834017646901014e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6738:\n",
      "train loss: 1.797185698600131e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6739:\n",
      "train loss: 2.116028600892194e-15\n",
      "lr: 3.8869627966947084e-16\n",
      "Epoch 6740:\n",
      "train loss: 1.6555705330764354e-15\n",
      "Epoch 06742: reducing learning rate of group 0 to 3.6926e-16.\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6741:\n",
      "train loss: 1.982338089037512e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6742:\n",
      "train loss: 2.1675456590116305e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6743:\n",
      "train loss: 2.2161702313354487e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6744:\n",
      "train loss: 1.9251035275217842e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6745:\n",
      "train loss: 1.839953179514199e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6746:\n",
      "train loss: 1.9846229401179522e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6747:\n",
      "train loss: 2.1949642358537318e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6748:\n",
      "train loss: 1.6329640041973394e-15\n",
      "lr: 3.692614656859973e-16\n",
      "Epoch 6749:\n",
      "train loss: 1.5729362004423251e-15\n",
      "Epoch 06751: reducing learning rate of group 0 to 3.5080e-16.\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6750:\n",
      "train loss: 2.314024769056979e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6751:\n",
      "train loss: 2.3136938542450455e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6752:\n",
      "train loss: 1.4715581127260281e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6753:\n",
      "train loss: 1.8001598638223723e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6754:\n",
      "train loss: 1.6327638487997052e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6755:\n",
      "train loss: 1.6430744616609727e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6756:\n",
      "train loss: 2.1283860627012118e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6757:\n",
      "train loss: 1.7965723356820045e-15\n",
      "lr: 3.507983924016974e-16\n",
      "Epoch 6758:\n",
      "train loss: 1.2402015367599077e-15\n",
      "Epoch 06760: reducing learning rate of group 0 to 3.3326e-16.\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6759:\n",
      "train loss: 1.2090126592127472e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6760:\n",
      "train loss: 2.178892963174787e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6761:\n",
      "train loss: 2.35667296399855e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6762:\n",
      "train loss: 1.0444183754751915e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6763:\n",
      "train loss: 1.1632477881241717e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6764:\n",
      "train loss: 1.5896905766107184e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6765:\n",
      "train loss: 1.202550867688399e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6766:\n",
      "train loss: 2.479079044520624e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6767:\n",
      "train loss: 2.4677211839238213e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6768:\n",
      "train loss: 7.040044781699436e-16\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6769:\n",
      "train loss: 7.011107182773934e-16\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6770:\n",
      "train loss: 1.1990985637576066e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6771:\n",
      "train loss: 1.1113557309775617e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6772:\n",
      "train loss: 2.7703532307251083e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6773:\n",
      "train loss: 2.4880543380997353e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6774:\n",
      "train loss: 8.130117816291652e-16\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6775:\n",
      "train loss: 6.408455640929336e-16\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6776:\n",
      "train loss: 1.0084083932144773e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6777:\n",
      "train loss: 2.952042303506097e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6778:\n",
      "train loss: 2.31863626806886e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6779:\n",
      "train loss: 9.37986405902342e-16\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6780:\n",
      "train loss: 2.5169015047487285e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6781:\n",
      "train loss: 2.7123938599382574e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6782:\n",
      "train loss: 1.279271057586303e-15\n",
      "lr: 3.3325847278161254e-16\n",
      "Epoch 6783:\n",
      "train loss: 1.3092592539309996e-15\n",
      "Epoch 06785: reducing learning rate of group 0 to 3.1660e-16.\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6784:\n",
      "train loss: 1.43188960807361e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6785:\n",
      "train loss: 1.4972608703982409e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6786:\n",
      "train loss: 1.0217706270870926e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6787:\n",
      "train loss: 2.33668474593947e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6788:\n",
      "train loss: 2.424882061851919e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6789:\n",
      "train loss: 1.1727690440007746e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6790:\n",
      "train loss: 1.067157994575736e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6791:\n",
      "train loss: 1.0931841039094187e-15\n",
      "lr: 3.165955491425319e-16\n",
      "Epoch 6792:\n",
      "train loss: 1.0775704705370547e-15\n",
      "Epoch 06794: reducing learning rate of group 0 to 3.0077e-16.\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6793:\n",
      "train loss: 1.055159717055695e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6794:\n",
      "train loss: 1.4350313695353532e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6795:\n",
      "train loss: 1.5405265044240456e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6796:\n",
      "train loss: 1.7162579605722298e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6797:\n",
      "train loss: 1.364116198837653e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6798:\n",
      "train loss: 1.5484168521418852e-15\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6799:\n",
      "train loss: 9.81905163704541e-16\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6800:\n",
      "train loss: 6.610333434537547e-16\n",
      "lr: 3.007657716854053e-16\n",
      "Epoch 6801:\n",
      "train loss: 7.852919614507164e-16\n",
      "Epoch 06803: reducing learning rate of group 0 to 2.8573e-16.\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6802:\n",
      "train loss: 8.133271950330343e-16\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6803:\n",
      "train loss: 7.626366783484868e-16\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6804:\n",
      "train loss: 1.0616939945305662e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6805:\n",
      "train loss: 8.517433656229325e-16\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6806:\n",
      "train loss: 7.070888783571689e-16\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6807:\n",
      "train loss: 1.0544024306045123e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6808:\n",
      "train loss: 1.2401597461066303e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6809:\n",
      "train loss: 2.214424588383251e-15\n",
      "lr: 2.85727483101135e-16\n",
      "Epoch 6810:\n",
      "train loss: 1.3135708024246948e-15\n",
      "Epoch 06812: reducing learning rate of group 0 to 2.7144e-16.\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6811:\n",
      "train loss: 1.7895859978208876e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6812:\n",
      "train loss: 1.548699707219432e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6813:\n",
      "train loss: 7.172179812397327e-16\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6814:\n",
      "train loss: 8.327356984469628e-16\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6815:\n",
      "train loss: 1.008265313221596e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6816:\n",
      "train loss: 7.788445252103407e-16\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6817:\n",
      "train loss: 1.5747945622384915e-15\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6818:\n",
      "train loss: 9.583752643699866e-16\n",
      "lr: 2.7144110894607826e-16\n",
      "Epoch 6819:\n",
      "train loss: 1.1389926248496602e-15\n",
      "Epoch 06821: reducing learning rate of group 0 to 2.5787e-16.\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6820:\n",
      "train loss: 1.081842942565388e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6821:\n",
      "train loss: 6.454781405298416e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6822:\n",
      "train loss: 9.200690555263291e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6823:\n",
      "train loss: 1.277937291058739e-15\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6824:\n",
      "train loss: 8.111216936010343e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6825:\n",
      "train loss: 9.065713851031388e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6826:\n",
      "train loss: 5.418878312810967e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6827:\n",
      "train loss: 6.74563448751003e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6828:\n",
      "train loss: 7.351792771378436e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6829:\n",
      "train loss: 9.177359582500205e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6830:\n",
      "train loss: 9.101546961315641e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6831:\n",
      "train loss: 7.768105471834113e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6832:\n",
      "train loss: 5.688085219472341e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6833:\n",
      "train loss: 8.10501450377031e-16\n",
      "lr: 2.5786905349877434e-16\n",
      "Epoch 6834:\n",
      "train loss: 7.700705498281635e-16\n",
      "Epoch 06836: reducing learning rate of group 0 to 2.4498e-16.\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6835:\n",
      "train loss: 9.055479732291793e-16\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6836:\n",
      "train loss: 8.347141483472428e-16\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6837:\n",
      "train loss: 6.290419552100697e-16\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6838:\n",
      "train loss: 7.42351367972317e-16\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6839:\n",
      "train loss: 6.915049762422341e-16\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6840:\n",
      "train loss: 1.1391927251292361e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6841:\n",
      "train loss: 1.109758853419856e-15\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6842:\n",
      "train loss: 9.032929995117903e-16\n",
      "lr: 2.449756008238356e-16\n",
      "Epoch 6843:\n",
      "train loss: 6.524095907752925e-16\n",
      "Epoch 06845: reducing learning rate of group 0 to 2.3273e-16.\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6844:\n",
      "train loss: 9.0975150659988e-16\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6845:\n",
      "train loss: 7.578622424302366e-16\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6846:\n",
      "train loss: 1.3174269300451924e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6847:\n",
      "train loss: 1.040032207027759e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6848:\n",
      "train loss: 1.3505194008031943e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6849:\n",
      "train loss: 1.0886184012420797e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6850:\n",
      "train loss: 1.213208438223813e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6851:\n",
      "train loss: 1.011433198887706e-15\n",
      "lr: 2.327268207826438e-16\n",
      "Epoch 6852:\n",
      "train loss: 7.303010360693152e-16\n",
      "Epoch 06854: reducing learning rate of group 0 to 2.2109e-16.\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6853:\n",
      "train loss: 8.256273428834975e-16\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6854:\n",
      "train loss: 8.261194715336005e-16\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6855:\n",
      "train loss: 5.910455569778553e-16\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6856:\n",
      "train loss: 6.881019470659967e-16\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6857:\n",
      "train loss: 5.687503032717809e-16\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6858:\n",
      "train loss: 1.1587661301758316e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6859:\n",
      "train loss: 1.2799351221085437e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6860:\n",
      "train loss: 1.0499415932096955e-15\n",
      "lr: 2.210904797435116e-16\n",
      "Epoch 6861:\n",
      "train loss: 1.3391998933750638e-15\n",
      "Epoch 06863: reducing learning rate of group 0 to 2.1004e-16.\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6862:\n",
      "train loss: 6.786963902590036e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6863:\n",
      "train loss: 1.0033943774733658e-15\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6864:\n",
      "train loss: 6.408954658840197e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6865:\n",
      "train loss: 8.242031079191373e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6866:\n",
      "train loss: 9.049179791351841e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6867:\n",
      "train loss: 7.787781684571633e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6868:\n",
      "train loss: 9.362827435145544e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6869:\n",
      "train loss: 7.050626826638535e-16\n",
      "lr: 2.1003595575633602e-16\n",
      "Epoch 6870:\n",
      "train loss: 5.616738751293641e-16\n",
      "Epoch 06872: reducing learning rate of group 0 to 1.9953e-16.\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6871:\n",
      "train loss: 7.414178705401673e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6872:\n",
      "train loss: 6.361921308145258e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6873:\n",
      "train loss: 5.166142541385175e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6874:\n",
      "train loss: 6.471374642558974e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6875:\n",
      "train loss: 5.548106632439771e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6876:\n",
      "train loss: 7.346240562827832e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6877:\n",
      "train loss: 6.648471295461532e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6878:\n",
      "train loss: 7.503050387460972e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6879:\n",
      "train loss: 6.412557790932389e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6880:\n",
      "train loss: 8.145933041986439e-16\n",
      "lr: 1.995341579685192e-16\n",
      "Epoch 6881:\n",
      "train loss: 7.580886729638812e-16\n",
      "Epoch 06883: reducing learning rate of group 0 to 1.8956e-16.\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6882:\n",
      "train loss: 7.259892119072771e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6883:\n",
      "train loss: 1.0564195305432783e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6884:\n",
      "train loss: 5.387008225465695e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6885:\n",
      "train loss: 1.4205790975237004e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6886:\n",
      "train loss: 1.7618468880295883e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6887:\n",
      "train loss: 1.0432152486966455e-15\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6888:\n",
      "train loss: 9.992811368285395e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6889:\n",
      "train loss: 7.663405535705947e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6890:\n",
      "train loss: 7.270981191387441e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6891:\n",
      "train loss: 3.8042668191359305e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6892:\n",
      "train loss: 5.329461664529536e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6893:\n",
      "train loss: 6.027478823932381e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6894:\n",
      "train loss: 6.404693049640494e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6895:\n",
      "train loss: 3.0279293834755694e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6896:\n",
      "train loss: 8.661466934536098e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6897:\n",
      "train loss: 7.133959329730019e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6898:\n",
      "train loss: 4.927568641142991e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6899:\n",
      "train loss: 8.364270895822322e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6900:\n",
      "train loss: 4.564025585769851e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6901:\n",
      "train loss: 6.931707072377709e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6902:\n",
      "train loss: 8.593433314117405e-16\n",
      "lr: 1.8955745007009323e-16\n",
      "Epoch 6903:\n",
      "train loss: 8.322517495326927e-16\n",
      "Epoch 06905: reducing learning rate of group 0 to 1.8008e-16.\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6904:\n",
      "train loss: 9.31732173329708e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6905:\n",
      "train loss: 7.900029099155937e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6906:\n",
      "train loss: 5.03871434825164e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6907:\n",
      "train loss: 7.352108365895593e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6908:\n",
      "train loss: 6.234452239930243e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6909:\n",
      "train loss: 1.0005923735261776e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6910:\n",
      "train loss: 1.4210111127255913e-15\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6911:\n",
      "train loss: 7.785838937639008e-16\n",
      "lr: 1.8007957756658857e-16\n",
      "Epoch 6912:\n",
      "train loss: 9.386332131391815e-16\n",
      "Epoch 06914: reducing learning rate of group 0 to 1.7108e-16.\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6913:\n",
      "train loss: 8.905136443820392e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6914:\n",
      "train loss: 8.704991874811028e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6915:\n",
      "train loss: 7.325052446933001e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6916:\n",
      "train loss: 8.081576079645256e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6917:\n",
      "train loss: 8.750187028827638e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6918:\n",
      "train loss: 7.843364641184177e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6919:\n",
      "train loss: 6.77482218360454e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6920:\n",
      "train loss: 7.892125171674678e-16\n",
      "lr: 1.7107559868825913e-16\n",
      "Epoch 6921:\n",
      "train loss: 7.839609341681507e-16\n",
      "Epoch 06923: reducing learning rate of group 0 to 1.6252e-16.\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6922:\n",
      "train loss: 6.077911352410676e-16\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6923:\n",
      "train loss: 1.0150562911210998e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6924:\n",
      "train loss: 1.0067759771070787e-15\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6925:\n",
      "train loss: 5.495398601403184e-16\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6926:\n",
      "train loss: 9.124760038438562e-16\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6927:\n",
      "train loss: 8.014748978577999e-16\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6928:\n",
      "train loss: 3.086609727397875e-16\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6929:\n",
      "train loss: 8.410907655214104e-16\n",
      "lr: 1.6252181875384616e-16\n",
      "Epoch 6930:\n",
      "train loss: 6.665543093957636e-16\n",
      "Epoch 06932: reducing learning rate of group 0 to 1.5440e-16.\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6931:\n",
      "train loss: 4.955450160670793e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6932:\n",
      "train loss: 9.737515281351636e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6933:\n",
      "train loss: 1.2227898784018052e-15\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6934:\n",
      "train loss: 8.623663812182529e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6935:\n",
      "train loss: 7.372782742092285e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6936:\n",
      "train loss: 9.646555781054221e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6937:\n",
      "train loss: 7.414102926453832e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6938:\n",
      "train loss: 8.101187098260637e-16\n",
      "lr: 1.5439572781615384e-16\n",
      "Epoch 6939:\n",
      "train loss: 7.205146647641029e-16\n",
      "Epoch 06941: reducing learning rate of group 0 to 1.4668e-16.\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6940:\n",
      "train loss: 5.348017106209823e-16\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6941:\n",
      "train loss: 1.2603302940481256e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6942:\n",
      "train loss: 1.6357728133408995e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6943:\n",
      "train loss: 1.1007208089304349e-15\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6944:\n",
      "train loss: 9.733546220908905e-16\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6945:\n",
      "train loss: 9.806678883847836e-16\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6946:\n",
      "train loss: 9.865161283450164e-16\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6947:\n",
      "train loss: 9.437944936647822e-16\n",
      "lr: 1.4667594142534614e-16\n",
      "Epoch 6948:\n",
      "train loss: 1.2507020543936066e-15\n",
      "Epoch 06950: reducing learning rate of group 0 to 1.3934e-16.\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6949:\n",
      "train loss: 5.312495368675805e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6950:\n",
      "train loss: 8.847893226308695e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6951:\n",
      "train loss: 9.330327812976097e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6952:\n",
      "train loss: 4.693091454582256e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6953:\n",
      "train loss: 8.779652870114517e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6954:\n",
      "train loss: 1.0357107762469572e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6955:\n",
      "train loss: 5.418844670146434e-16\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6956:\n",
      "train loss: 1.0076987610439906e-15\n",
      "lr: 1.3934214435407884e-16\n",
      "Epoch 6957:\n",
      "train loss: 8.822490999401643e-16\n",
      "Epoch 06959: reducing learning rate of group 0 to 1.3238e-16.\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6958:\n",
      "train loss: 4.3273200799024206e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6959:\n",
      "train loss: 3.1404567153546124e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6960:\n",
      "train loss: 5.375238278628078e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6961:\n",
      "train loss: 3.967331758494168e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6962:\n",
      "train loss: 4.904203037839777e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6963:\n",
      "train loss: 3.760562947934452e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6964:\n",
      "train loss: 5.416411464192034e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6965:\n",
      "train loss: 4.3361780068782275e-16\n",
      "lr: 1.3237503713637489e-16\n",
      "Epoch 6966:\n",
      "train loss: 3.739460359542902e-16\n",
      "Epoch 06968: reducing learning rate of group 0 to 1.2576e-16.\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6967:\n",
      "train loss: 6.562736577877382e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6968:\n",
      "train loss: 7.167958379612487e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6969:\n",
      "train loss: 7.038555123459041e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6970:\n",
      "train loss: 5.252474782821793e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6971:\n",
      "train loss: 9.700984358335193e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6972:\n",
      "train loss: 9.042248125661811e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6973:\n",
      "train loss: 6.7646826629658425e-16\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6974:\n",
      "train loss: 1.0431609776312183e-15\n",
      "lr: 1.2575628527955613e-16\n",
      "Epoch 6975:\n",
      "train loss: 1.1408067930618197e-15\n",
      "Epoch 06977: reducing learning rate of group 0 to 1.1947e-16.\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6976:\n",
      "train loss: 8.1337336909944e-16\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6977:\n",
      "train loss: 1.3020445719131405e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6978:\n",
      "train loss: 1.570944893456385e-15\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6979:\n",
      "train loss: 7.477275466244781e-16\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6980:\n",
      "train loss: 7.036735086500276e-16\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6981:\n",
      "train loss: 6.927540152652275e-16\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6982:\n",
      "train loss: 5.045279233454865e-16\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6983:\n",
      "train loss: 8.745519661076958e-16\n",
      "lr: 1.1946847101557832e-16\n",
      "Epoch 6984:\n",
      "train loss: 7.763752620821296e-16\n",
      "Epoch 06986: reducing learning rate of group 0 to 1.1350e-16.\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6985:\n",
      "train loss: 6.665970414631279e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6986:\n",
      "train loss: 8.057871783222357e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6987:\n",
      "train loss: 7.249052795515047e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6988:\n",
      "train loss: 7.897481275189168e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6989:\n",
      "train loss: 6.958654898448274e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6990:\n",
      "train loss: 8.364053271653818e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6991:\n",
      "train loss: 4.793403783798676e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6992:\n",
      "train loss: 8.736722820360948e-16\n",
      "lr: 1.134950474647994e-16\n",
      "Epoch 6993:\n",
      "train loss: 8.770900172142766e-16\n",
      "Epoch 06995: reducing learning rate of group 0 to 1.0782e-16.\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 6994:\n",
      "train loss: 8.645915037582377e-16\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 6995:\n",
      "train loss: 4.0110303915487575e-16\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 6996:\n",
      "train loss: 1.204614648274888e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 6997:\n",
      "train loss: 1.082688261356257e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 6998:\n",
      "train loss: 7.486250642530534e-16\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 6999:\n",
      "train loss: 1.2899214596834532e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7000:\n",
      "train loss: 1.482167942243565e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7001:\n",
      "train loss: 1.5878220721073842e-15\n",
      "lr: 1.0782029509155943e-16\n",
      "Epoch 7002:\n",
      "train loss: 5.150929762479317e-16\n",
      "Epoch 07004: reducing learning rate of group 0 to 1.0243e-16.\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7003:\n",
      "train loss: 5.000722386102535e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7004:\n",
      "train loss: 9.245168860296803e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7005:\n",
      "train loss: 7.775395587906587e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7006:\n",
      "train loss: 1.0226489095669585e-15\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7007:\n",
      "train loss: 8.63454368238584e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7008:\n",
      "train loss: 8.872375084630574e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7009:\n",
      "train loss: 7.197802063175966e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7010:\n",
      "train loss: 6.493418309109545e-16\n",
      "lr: 1.0242928033698145e-16\n",
      "Epoch 7011:\n",
      "train loss: 7.00633163620212e-16\n",
      "Epoch 07013: reducing learning rate of group 0 to 9.7308e-17.\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7012:\n",
      "train loss: 4.0393612403562535e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7013:\n",
      "train loss: 6.207636837565434e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7014:\n",
      "train loss: 6.769074549754564e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7015:\n",
      "train loss: 3.802567891336787e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7016:\n",
      "train loss: 5.904022925603394e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7017:\n",
      "train loss: 4.1427032541700693e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7018:\n",
      "train loss: 4.228750801575301e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7019:\n",
      "train loss: 5.245947698078913e-16\n",
      "lr: 9.730781632013237e-17\n",
      "Epoch 7020:\n",
      "train loss: 6.018147420179196e-16\n",
      "Epoch 07022: reducing learning rate of group 0 to 9.2442e-17.\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7021:\n",
      "train loss: 6.193326127075181e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7022:\n",
      "train loss: 5.147826250633055e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7023:\n",
      "train loss: 5.895047933589894e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7024:\n",
      "train loss: 4.193615776170543e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7025:\n",
      "train loss: 3.686247111059691e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7026:\n",
      "train loss: 4.855752211692588e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7027:\n",
      "train loss: 4.861714251272622e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7028:\n",
      "train loss: 6.846988342424955e-16\n",
      "lr: 9.244242550412575e-17\n",
      "Epoch 7029:\n",
      "train loss: 8.244914289964743e-16\n",
      "Epoch 07031: reducing learning rate of group 0 to 8.7820e-17.\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7030:\n",
      "train loss: 9.116012329167945e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7031:\n",
      "train loss: 7.425074247393168e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7032:\n",
      "train loss: 8.657994266054853e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7033:\n",
      "train loss: 5.256584564975639e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7034:\n",
      "train loss: 5.195907592475636e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7035:\n",
      "train loss: 4.0777153629749597e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7036:\n",
      "train loss: 3.8516653564833578e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7037:\n",
      "train loss: 2.660325568114381e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7038:\n",
      "train loss: 3.3805045409991933e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7039:\n",
      "train loss: 8.324023654701399e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7040:\n",
      "train loss: 1.1825561152718655e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7041:\n",
      "train loss: 6.482508506283847e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7042:\n",
      "train loss: 5.785831670517918e-16\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7043:\n",
      "train loss: 1.060500521125802e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7044:\n",
      "train loss: 1.2026412414135237e-15\n",
      "lr: 8.782030422891946e-17\n",
      "Epoch 7045:\n",
      "train loss: 7.712055151781913e-16\n",
      "Epoch 07047: reducing learning rate of group 0 to 8.3429e-17.\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7046:\n",
      "train loss: 1.0249413484163736e-15\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7047:\n",
      "train loss: 9.360176067663485e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7048:\n",
      "train loss: 9.152427194019758e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7049:\n",
      "train loss: 7.78580867729979e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7050:\n",
      "train loss: 7.439742144423234e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7051:\n",
      "train loss: 8.60482444972893e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7052:\n",
      "train loss: 9.288813953943101e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7053:\n",
      "train loss: 6.028672596880891e-16\n",
      "lr: 8.342928901747348e-17\n",
      "Epoch 7054:\n",
      "train loss: 9.100468226867796e-16\n",
      "Epoch 07056: reducing learning rate of group 0 to 7.9258e-17.\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7055:\n",
      "train loss: 1.263527722256068e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7056:\n",
      "train loss: 1.0032586231064267e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7057:\n",
      "train loss: 7.358111746930545e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7058:\n",
      "train loss: 9.489184021872943e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7059:\n",
      "train loss: 1.1309517257916448e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7060:\n",
      "train loss: 1.1676189490532046e-15\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7061:\n",
      "train loss: 7.953300503516102e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7062:\n",
      "train loss: 7.829838436647336e-16\n",
      "lr: 7.925782456659981e-17\n",
      "Epoch 7063:\n",
      "train loss: 1.1494516513993157e-15\n",
      "Epoch 07065: reducing learning rate of group 0 to 7.5295e-17.\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7064:\n",
      "train loss: 1.2654420164800667e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7065:\n",
      "train loss: 5.559003785102197e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7066:\n",
      "train loss: 8.412049613648866e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7067:\n",
      "train loss: 9.808656121450385e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7068:\n",
      "train loss: 1.102415207577548e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7069:\n",
      "train loss: 4.092014276694416e-16\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7070:\n",
      "train loss: 1.368673001818071e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7071:\n",
      "train loss: 1.2566815000196305e-15\n",
      "lr: 7.529493333826981e-17\n",
      "Epoch 7072:\n",
      "train loss: 1.1798192972792689e-15\n",
      "Epoch 07074: reducing learning rate of group 0 to 7.1530e-17.\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7073:\n",
      "train loss: 9.18032913472148e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7074:\n",
      "train loss: 5.163864743979952e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7075:\n",
      "train loss: 8.964817982756067e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7076:\n",
      "train loss: 6.198320926806373e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7077:\n",
      "train loss: 7.441372082370772e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7078:\n",
      "train loss: 5.336799690190375e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7079:\n",
      "train loss: 8.701455605843541e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7080:\n",
      "train loss: 5.26937052423072e-16\n",
      "lr: 7.153018667135633e-17\n",
      "Epoch 7081:\n",
      "train loss: 2.860994564424749e-16\n",
      "Epoch 07083: reducing learning rate of group 0 to 6.7954e-17.\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7082:\n",
      "train loss: 4.253833605138658e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7083:\n",
      "train loss: 3.9561750394846973e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7084:\n",
      "train loss: 4.923910660133559e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7085:\n",
      "train loss: 5.102249427346359e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7086:\n",
      "train loss: 3.557975092176509e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7087:\n",
      "train loss: 3.8085995197242304e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7088:\n",
      "train loss: 9.570578442606453e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7089:\n",
      "train loss: 9.146175840674736e-16\n",
      "lr: 6.795367733778851e-17\n",
      "Epoch 7090:\n",
      "train loss: 1.0689626633453203e-15\n",
      "Epoch 07092: reducing learning rate of group 0 to 6.4556e-17.\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7091:\n",
      "train loss: 7.430464397440905e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7092:\n",
      "train loss: 7.970049761372936e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7093:\n",
      "train loss: 9.06514755305068e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7094:\n",
      "train loss: 7.738923412677156e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7095:\n",
      "train loss: 1.0390071611699797e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7096:\n",
      "train loss: 4.612095661581983e-16\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7097:\n",
      "train loss: 1.6306171041313592e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7098:\n",
      "train loss: 1.7208492692035795e-15\n",
      "lr: 6.455599347089908e-17\n",
      "Epoch 7099:\n",
      "train loss: 1.594706108471162e-15\n",
      "Epoch 07101: reducing learning rate of group 0 to 6.1328e-17.\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7100:\n",
      "train loss: 1.447817298726577e-15\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7101:\n",
      "train loss: 8.495606785780216e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7102:\n",
      "train loss: 6.474323219580497e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7103:\n",
      "train loss: 1.1365616493152072e-15\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7104:\n",
      "train loss: 1.1788539724746542e-15\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7105:\n",
      "train loss: 9.47254857924823e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7106:\n",
      "train loss: 6.547611324543175e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7107:\n",
      "train loss: 3.3783561779204236e-16\n",
      "lr: 6.132819379735412e-17\n",
      "Epoch 7108:\n",
      "train loss: 1.1588532446443948e-15\n",
      "Epoch 07110: reducing learning rate of group 0 to 5.8262e-17.\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7109:\n",
      "train loss: 1.0855635216851246e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7110:\n",
      "train loss: 1.1853649475449299e-15\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7111:\n",
      "train loss: 6.210470734182864e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7112:\n",
      "train loss: 4.316497937647695e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7113:\n",
      "train loss: 6.281527133773529e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7114:\n",
      "train loss: 7.970865462921061e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7115:\n",
      "train loss: 4.501915659115503e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7116:\n",
      "train loss: 5.027362480067562e-16\n",
      "lr: 5.826178410748642e-17\n",
      "Epoch 7117:\n",
      "train loss: 4.726216921380026e-16\n",
      "Epoch 07119: reducing learning rate of group 0 to 5.5349e-17.\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7118:\n",
      "train loss: 4.6281074381423545e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7119:\n",
      "train loss: 2.85220150814018e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7120:\n",
      "train loss: 3.9687201803716715e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7121:\n",
      "train loss: 3.668648898714954e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7122:\n",
      "train loss: 4.585500084702163e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7123:\n",
      "train loss: 7.045190802465348e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7124:\n",
      "train loss: 7.424481155383854e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7125:\n",
      "train loss: 6.512569181572032e-16\n",
      "lr: 5.534869490211209e-17\n",
      "Epoch 7126:\n",
      "train loss: 4.545410675165422e-16\n",
      "Epoch 07128: reducing learning rate of group 0 to 5.2581e-17.\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7127:\n",
      "train loss: 4.477421109470939e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7128:\n",
      "train loss: 4.89082774474295e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7129:\n",
      "train loss: 4.1358723719446716e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7130:\n",
      "train loss: 3.633462254610597e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7131:\n",
      "train loss: 4.213131973262479e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7132:\n",
      "train loss: 3.624770559292262e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7133:\n",
      "train loss: 4.661355368252048e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7134:\n",
      "train loss: 5.914165370558023e-16\n",
      "lr: 5.2581260157006484e-17\n",
      "Epoch 7135:\n",
      "train loss: 4.894659135601263e-16\n",
      "Epoch 07137: reducing learning rate of group 0 to 4.9952e-17.\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7136:\n",
      "train loss: 3.9658837871787097e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7137:\n",
      "train loss: 5.604174683596356e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7138:\n",
      "train loss: 6.711929890559628e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7139:\n",
      "train loss: 4.838234489560193e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7140:\n",
      "train loss: 5.656472639693271e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7141:\n",
      "train loss: 4.3574350617742013e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7142:\n",
      "train loss: 3.760494611541338e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7143:\n",
      "train loss: 4.0616959633378193e-16\n",
      "lr: 4.995219714915616e-17\n",
      "Epoch 7144:\n",
      "train loss: 3.187001449196605e-16\n",
      "Epoch 07146: reducing learning rate of group 0 to 4.7455e-17.\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7145:\n",
      "train loss: 5.474944623352695e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7146:\n",
      "train loss: 5.540608480244237e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7147:\n",
      "train loss: 4.833060175662809e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7148:\n",
      "train loss: 4.407819704886051e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7149:\n",
      "train loss: 9.348696773265423e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7150:\n",
      "train loss: 8.0469652077633525e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7151:\n",
      "train loss: 6.371510448313907e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7152:\n",
      "train loss: 7.346664839551417e-16\n",
      "lr: 4.745458729169835e-17\n",
      "Epoch 7153:\n",
      "train loss: 4.530941498830504e-16\n",
      "Epoch 07155: reducing learning rate of group 0 to 4.5082e-17.\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7154:\n",
      "train loss: 3.876969955965609e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7155:\n",
      "train loss: 9.794756586106853e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7156:\n",
      "train loss: 1.169467458079334e-15\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7157:\n",
      "train loss: 9.741436464024022e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7158:\n",
      "train loss: 7.29674661201011e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7159:\n",
      "train loss: 3.601294995666283e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7160:\n",
      "train loss: 7.554238923294024e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7161:\n",
      "train loss: 5.038437998852743e-16\n",
      "lr: 4.508185792711343e-17\n",
      "Epoch 7162:\n",
      "train loss: 5.681843228133497e-16\n",
      "Epoch 07164: reducing learning rate of group 0 to 4.2828e-17.\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7163:\n",
      "train loss: 6.273686019051736e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7164:\n",
      "train loss: 4.959631119131534e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7165:\n",
      "train loss: 5.226371940714824e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7166:\n",
      "train loss: 9.331513276764882e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7167:\n",
      "train loss: 7.311082642521109e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7168:\n",
      "train loss: 6.742681709521622e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7169:\n",
      "train loss: 5.294374293522917e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7170:\n",
      "train loss: 3.855296150596966e-16\n",
      "lr: 4.2827765030757754e-17\n",
      "Epoch 7171:\n",
      "train loss: 3.006437572002199e-16\n",
      "Epoch 07173: reducing learning rate of group 0 to 4.0686e-17.\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7172:\n",
      "train loss: 3.809938297053305e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7173:\n",
      "train loss: 3.3834255543288626e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7174:\n",
      "train loss: 3.9688970323892634e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7175:\n",
      "train loss: 5.691312887769715e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7176:\n",
      "train loss: 4.591706009434377e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7177:\n",
      "train loss: 4.1972238074410252e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7178:\n",
      "train loss: 3.431079408693335e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7179:\n",
      "train loss: 5.994622967630015e-16\n",
      "lr: 4.068637677921986e-17\n",
      "Epoch 7180:\n",
      "train loss: 6.050146158394886e-16\n",
      "Epoch 07182: reducing learning rate of group 0 to 3.8652e-17.\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7181:\n",
      "train loss: 6.050146158394886e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7182:\n",
      "train loss: 4.076575987485333e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7183:\n",
      "train loss: 4.1228313086319487e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7184:\n",
      "train loss: 4.59649589892522e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7185:\n",
      "train loss: 3.2193994741607903e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7186:\n",
      "train loss: 5.774201157413928e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7187:\n",
      "train loss: 5.774201157413928e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7188:\n",
      "train loss: 7.815463781595856e-16\n",
      "lr: 3.865205794025887e-17\n",
      "Epoch 7189:\n",
      "train loss: 6.181723328813161e-16\n",
      "Epoch 07191: reducing learning rate of group 0 to 3.6719e-17.\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7190:\n",
      "train loss: 6.246052186275499e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7191:\n",
      "train loss: 3.7510579791674793e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7192:\n",
      "train loss: 3.086433328008008e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7193:\n",
      "train loss: 3.398085681309945e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7194:\n",
      "train loss: 3.1870965113056564e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7195:\n",
      "train loss: 5.051267044537574e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7196:\n",
      "train loss: 3.925237876035929e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7197:\n",
      "train loss: 4.581850721110858e-16\n",
      "lr: 3.6719455043245923e-17\n",
      "Epoch 7198:\n",
      "train loss: 5.753637445683967e-16\n",
      "Epoch 07200: reducing learning rate of group 0 to 3.4883e-17.\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7199:\n",
      "train loss: 5.753637445683967e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7200:\n",
      "train loss: 4.325809516337743e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7201:\n",
      "train loss: 3.726573462929154e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7202:\n",
      "train loss: 2.8313083533863883e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7203:\n",
      "train loss: 3.5034973348513444e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7204:\n",
      "train loss: 3.8118347041158495e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7205:\n",
      "train loss: 3.876970095721507e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7206:\n",
      "train loss: 3.4710425916375743e-16\n",
      "lr: 3.4883482291083625e-17\n",
      "Epoch 7207:\n",
      "train loss: 7.228224811445165e-16\n",
      "Epoch 07209: reducing learning rate of group 0 to 3.3139e-17.\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7208:\n",
      "train loss: 6.585501435458292e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7209:\n",
      "train loss: 6.082017548762165e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7210:\n",
      "train loss: 6.17550891112574e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7211:\n",
      "train loss: 6.529663328450349e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7212:\n",
      "train loss: 6.534617548364102e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7213:\n",
      "train loss: 4.69189651080815e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7214:\n",
      "train loss: 5.404795403656887e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7215:\n",
      "train loss: 4.461716375221322e-16\n",
      "lr: 3.3139308176529444e-17\n",
      "Epoch 7216:\n",
      "train loss: 7.042414357946555e-16\n",
      "Epoch 07218: reducing learning rate of group 0 to 3.1482e-17.\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7217:\n",
      "train loss: 6.288267344738324e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7218:\n",
      "train loss: 4.2810354612952e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7219:\n",
      "train loss: 4.30847330554025e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7220:\n",
      "train loss: 4.972239351738511e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7221:\n",
      "train loss: 5.314668543922249e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7222:\n",
      "train loss: 6.431891875124018e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7223:\n",
      "train loss: 6.862538916650258e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7224:\n",
      "train loss: 5.831502752397286e-16\n",
      "lr: 3.1482342767702974e-17\n",
      "Epoch 7225:\n",
      "train loss: 6.116524549412576e-16\n",
      "Epoch 07227: reducing learning rate of group 0 to 2.9908e-17.\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7226:\n",
      "train loss: 5.374589573476365e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7227:\n",
      "train loss: 3.848250896471422e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7228:\n",
      "train loss: 7.962809975726708e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7229:\n",
      "train loss: 5.537179648378614e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7230:\n",
      "train loss: 7.008085421800902e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7231:\n",
      "train loss: 5.757543888320873e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7232:\n",
      "train loss: 5.43657238825102e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7233:\n",
      "train loss: 3.390657588839353e-16\n",
      "lr: 2.9908225629317825e-17\n",
      "Epoch 7234:\n",
      "train loss: 3.5531379134768633e-16\n",
      "Epoch 07236: reducing learning rate of group 0 to 2.8413e-17.\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7235:\n",
      "train loss: 4.704891928241922e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7236:\n",
      "train loss: 4.1208642742884554e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7237:\n",
      "train loss: 3.875124236557245e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7238:\n",
      "train loss: 3.875124236557245e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7239:\n",
      "train loss: 3.2365981338166477e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7240:\n",
      "train loss: 5.669728775928904e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7241:\n",
      "train loss: 2.6851260375270276e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7242:\n",
      "train loss: 4.610803233323097e-16\n",
      "lr: 2.841281434785193e-17\n",
      "Epoch 7243:\n",
      "train loss: 4.964558050480411e-16\n",
      "Epoch 07245: reducing learning rate of group 0 to 2.6992e-17.\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7244:\n",
      "train loss: 3.306816750358571e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7245:\n",
      "train loss: 3.4731141322877264e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7246:\n",
      "train loss: 2.7118771470505027e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7247:\n",
      "train loss: 2.275587654939326e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7248:\n",
      "train loss: 3.0274275225914623e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7249:\n",
      "train loss: 4.2961173440580766e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7250:\n",
      "train loss: 4.569960246742399e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7251:\n",
      "train loss: 4.56530735038983e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7252:\n",
      "train loss: 4.945118199766032e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7253:\n",
      "train loss: 4.097717328480986e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7254:\n",
      "train loss: 3.260955761123243e-16\n",
      "lr: 2.6992173630459333e-17\n",
      "Epoch 7255:\n",
      "train loss: 3.150456130725126e-16\n",
      "Epoch 07257: reducing learning rate of group 0 to 2.5643e-17.\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7256:\n",
      "train loss: 3.760702358115887e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7257:\n",
      "train loss: 3.7715552301361946e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7258:\n",
      "train loss: 3.7715552301361946e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7259:\n",
      "train loss: 4.610994669516308e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7260:\n",
      "train loss: 4.0852144982467854e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7261:\n",
      "train loss: 4.097433644950554e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7262:\n",
      "train loss: 4.0514939067102313e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7263:\n",
      "train loss: 8.570278498518614e-16\n",
      "lr: 2.5642564948936364e-17\n",
      "Epoch 7264:\n",
      "train loss: 8.879731311582021e-16\n",
      "Epoch 07266: reducing learning rate of group 0 to 2.4360e-17.\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7265:\n",
      "train loss: 9.23862488911587e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7266:\n",
      "train loss: 5.827212143743961e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7267:\n",
      "train loss: 3.4613465130730647e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7268:\n",
      "train loss: 3.4712010565454633e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7269:\n",
      "train loss: 3.786549357570779e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7270:\n",
      "train loss: 3.253284081005791e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7271:\n",
      "train loss: 6.938187341870749e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7272:\n",
      "train loss: 7.190489711787047e-16\n",
      "lr: 2.4360436701489546e-17\n",
      "Epoch 7273:\n",
      "train loss: 7.190489711787047e-16\n",
      "Epoch 07275: reducing learning rate of group 0 to 2.3142e-17.\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7274:\n",
      "train loss: 7.171698365009403e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7275:\n",
      "train loss: 6.2223532544570955e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7276:\n",
      "train loss: 5.785965038285717e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7277:\n",
      "train loss: 2.054651028729956e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7278:\n",
      "train loss: 3.349049065980352e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7279:\n",
      "train loss: 2.4454255286896347e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7280:\n",
      "train loss: 3.6588252940972327e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7281:\n",
      "train loss: 7.217243507764884e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7282:\n",
      "train loss: 7.849556070284386e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7283:\n",
      "train loss: 5.745939533592791e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7284:\n",
      "train loss: 4.3370917416459285e-16\n",
      "lr: 2.3142414866415066e-17\n",
      "Epoch 7285:\n",
      "train loss: 2.2996612543535205e-16\n",
      "Epoch 07287: reducing learning rate of group 0 to 2.1985e-17.\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7286:\n",
      "train loss: 2.8157099391693073e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7287:\n",
      "train loss: 2.223377935062594e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7288:\n",
      "train loss: 4.828835164804409e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7289:\n",
      "train loss: 5.698699097283583e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7290:\n",
      "train loss: 5.069619621211119e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7291:\n",
      "train loss: 3.601876395635028e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7292:\n",
      "train loss: 3.5575422899833917e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7293:\n",
      "train loss: 3.5415876827187697e-16\n",
      "lr: 2.198529412309431e-17\n",
      "Epoch 7294:\n",
      "train loss: 3.1941531742045465e-16\n",
      "Epoch 07296: reducing learning rate of group 0 to 2.0886e-17.\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7295:\n",
      "train loss: 2.949669864202596e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7296:\n",
      "train loss: 2.919542305770208e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7297:\n",
      "train loss: 2.9131170062912356e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7298:\n",
      "train loss: 3.5148762985595227e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7299:\n",
      "train loss: 3.4854053673807997e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7300:\n",
      "train loss: 5.111192475210159e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7301:\n",
      "train loss: 5.298730659539745e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7302:\n",
      "train loss: 5.177755397393085e-16\n",
      "lr: 2.0886029416939595e-17\n",
      "Epoch 7303:\n",
      "train loss: 3.8886168290471864e-16\n",
      "Epoch 07305: reducing learning rate of group 0 to 1.9842e-17.\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7304:\n",
      "train loss: 3.73159654381247e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7305:\n",
      "train loss: 5.083391215636717e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7306:\n",
      "train loss: 7.401539631928217e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7307:\n",
      "train loss: 7.401539631928217e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7308:\n",
      "train loss: 6.554619567840255e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7309:\n",
      "train loss: 5.401608721059909e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7310:\n",
      "train loss: 4.526427684274959e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7311:\n",
      "train loss: 2.6689298857307426e-16\n",
      "lr: 1.9841727946092614e-17\n",
      "Epoch 7312:\n",
      "train loss: 3.948842996749987e-16\n",
      "Epoch 07314: reducing learning rate of group 0 to 1.8850e-17.\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7313:\n",
      "train loss: 4.065920774458704e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7314:\n",
      "train loss: 4.0732359283420657e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7315:\n",
      "train loss: 3.793054543813771e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7316:\n",
      "train loss: 2.3581849961984485e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7317:\n",
      "train loss: 1.9456469812154259e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7318:\n",
      "train loss: 1.7046746928542765e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7319:\n",
      "train loss: 1.816573815346578e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7320:\n",
      "train loss: 1.7838223819569647e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7321:\n",
      "train loss: 2.616691647696143e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7322:\n",
      "train loss: 2.417898177309953e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7323:\n",
      "train loss: 2.0778767628619372e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7324:\n",
      "train loss: 2.0778767628619372e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7325:\n",
      "train loss: 3.8188055454723457e-16\n",
      "lr: 1.8849641548787983e-17\n",
      "Epoch 7326:\n",
      "train loss: 3.676588043180021e-16\n",
      "Epoch 07328: reducing learning rate of group 0 to 1.7907e-17.\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7327:\n",
      "train loss: 2.622689869914752e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7328:\n",
      "train loss: 2.472681275082483e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7329:\n",
      "train loss: 2.984651744105226e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7330:\n",
      "train loss: 3.457745529109296e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7331:\n",
      "train loss: 3.4360407616324054e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7332:\n",
      "train loss: 3.4360407616324054e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7333:\n",
      "train loss: 2.327163916472721e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7334:\n",
      "train loss: 2.308851735721179e-16\n",
      "lr: 1.7907159471348585e-17\n",
      "Epoch 7335:\n",
      "train loss: 2.697114612696882e-16\n",
      "Epoch 07337: reducing learning rate of group 0 to 1.7012e-17.\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7336:\n",
      "train loss: 2.593680204018368e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7337:\n",
      "train loss: 2.775624707964092e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7338:\n",
      "train loss: 2.639957896154806e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7339:\n",
      "train loss: 2.971405742638736e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7340:\n",
      "train loss: 3.345503736910443e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7341:\n",
      "train loss: 3.1237125168519347e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7342:\n",
      "train loss: 2.598688081303587e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7343:\n",
      "train loss: 2.662323325508854e-16\n",
      "lr: 1.7011801497781155e-17\n",
      "Epoch 7344:\n",
      "train loss: 2.580700854189775e-16\n",
      "Epoch 07346: reducing learning rate of group 0 to 1.6161e-17.\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7345:\n",
      "train loss: 4.606139702901833e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7346:\n",
      "train loss: 3.558898077942585e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7347:\n",
      "train loss: 3.326578634147427e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7348:\n",
      "train loss: 2.279952569323456e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7349:\n",
      "train loss: 2.32555183463942e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7350:\n",
      "train loss: 3.0871642710688317e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7351:\n",
      "train loss: 2.9623636085486763e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7352:\n",
      "train loss: 2.911345559215929e-16\n",
      "lr: 1.6161211422892097e-17\n",
      "Epoch 7353:\n",
      "train loss: 3.110180935836612e-16\n",
      "Epoch 07355: reducing learning rate of group 0 to 1.5353e-17.\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7354:\n",
      "train loss: 3.5462685524396103e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7355:\n",
      "train loss: 2.912992236253535e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7356:\n",
      "train loss: 2.912992236253535e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7357:\n",
      "train loss: 2.912992236253535e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7358:\n",
      "train loss: 4.168057142363863e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7359:\n",
      "train loss: 2.565426846013346e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7360:\n",
      "train loss: 2.471999841472254e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7361:\n",
      "train loss: 2.918109613350374e-16\n",
      "lr: 1.535315085174749e-17\n",
      "Epoch 7362:\n",
      "train loss: 2.8626387882463007e-16\n",
      "Epoch 07364: reducing learning rate of group 0 to 1.4585e-17.\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7363:\n",
      "train loss: 3.7739964608705816e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7364:\n",
      "train loss: 3.7684453680718113e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7365:\n",
      "train loss: 3.861412104619441e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7366:\n",
      "train loss: 1.9045100143377384e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7367:\n",
      "train loss: 1.9045100143377384e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7368:\n",
      "train loss: 2.2970192040040313e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7369:\n",
      "train loss: 2.583792491393814e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7370:\n",
      "train loss: 2.5537155725683083e-16\n",
      "lr: 1.4585493309160116e-17\n",
      "Epoch 7371:\n",
      "train loss: 2.393989645529257e-16\n",
      "Epoch 07373: reducing learning rate of group 0 to 1.3856e-17.\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7372:\n",
      "train loss: 2.0936113356599157e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7373:\n",
      "train loss: 2.0936113356599157e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7374:\n",
      "train loss: 2.1678662460534996e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7375:\n",
      "train loss: 2.2546664185975093e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7376:\n",
      "train loss: 2.305475938218273e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7377:\n",
      "train loss: 3.058750432200634e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7378:\n",
      "train loss: 2.5362663332527956e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7379:\n",
      "train loss: 2.5362663332527956e-16\n",
      "lr: 1.3856218643702108e-17\n",
      "Epoch 7380:\n",
      "train loss: 2.3307644792524448e-16\n",
      "Epoch 07382: reducing learning rate of group 0 to 1.3163e-17.\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7381:\n",
      "train loss: 2.5142177832114235e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7382:\n",
      "train loss: 2.8322829119876785e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7383:\n",
      "train loss: 2.9526127786191465e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7384:\n",
      "train loss: 2.9309161620883625e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7385:\n",
      "train loss: 2.6527425458528443e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7386:\n",
      "train loss: 3.219994050910202e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7387:\n",
      "train loss: 3.317521432550845e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7388:\n",
      "train loss: 2.737933099688955e-16\n",
      "lr: 1.3163407711517002e-17\n",
      "Epoch 7389:\n",
      "train loss: 4.991057502361506e-16\n",
      "Epoch 07391: reducing learning rate of group 0 to 1.2505e-17.\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7390:\n",
      "train loss: 4.894722380858911e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7391:\n",
      "train loss: 4.826543657563561e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7392:\n",
      "train loss: 3.831993896340706e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7393:\n",
      "train loss: 3.7417181772679213e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7394:\n",
      "train loss: 3.681127362884752e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7395:\n",
      "train loss: 2.5993251226286576e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7396:\n",
      "train loss: 1.8633705735608744e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7397:\n",
      "train loss: 2.498807275016921e-16\n",
      "lr: 1.2505237325941152e-17\n",
      "Epoch 7398:\n",
      "train loss: 2.8326865680480717e-16\n",
      "Epoch 07400: reducing learning rate of group 0 to 1.1880e-17.\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7399:\n",
      "train loss: 2.7314767589367004e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7400:\n",
      "train loss: 4.2042889851127463e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7401:\n",
      "train loss: 4.2042889851127463e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7402:\n",
      "train loss: 4.192213948287592e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7403:\n",
      "train loss: 3.796468041320951e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7404:\n",
      "train loss: 3.787208021296464e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7405:\n",
      "train loss: 3.4600615858673353e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7406:\n",
      "train loss: 3.871587973305503e-16\n",
      "lr: 1.1879975459644094e-17\n",
      "Epoch 7407:\n",
      "train loss: 4.1847503513172696e-16\n",
      "Epoch 07409: reducing learning rate of group 0 to 1.1286e-17.\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7408:\n",
      "train loss: 3.672060829269101e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7409:\n",
      "train loss: 4.1989654241067645e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7410:\n",
      "train loss: 4.1989654241067645e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7411:\n",
      "train loss: 2.4989414042146215e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7412:\n",
      "train loss: 2.362407520449509e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7413:\n",
      "train loss: 2.080469053602355e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7414:\n",
      "train loss: 1.857534444165141e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7415:\n",
      "train loss: 1.8799571586277811e-16\n",
      "lr: 1.1285976686661888e-17\n",
      "Epoch 7416:\n",
      "train loss: 1.8799571586277811e-16\n",
      "Epoch 07418: reducing learning rate of group 0 to 1.0722e-17.\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7417:\n",
      "train loss: 2.1245571658787685e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7418:\n",
      "train loss: 2.1304724206399824e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7419:\n",
      "train loss: 2.1304724206399824e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7420:\n",
      "train loss: 2.756179667665687e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7421:\n",
      "train loss: 2.756179667665687e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7422:\n",
      "train loss: 2.4462546451074504e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7423:\n",
      "train loss: 2.522398851877529e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7424:\n",
      "train loss: 2.989050860307669e-16\n",
      "lr: 1.0721677852328793e-17\n",
      "Epoch 7425:\n",
      "train loss: 2.493107757103678e-16\n",
      "Epoch 07427: reducing learning rate of group 0 to 1.0186e-17.\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7426:\n",
      "train loss: 2.493107757103678e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7427:\n",
      "train loss: 2.2059416635485605e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7428:\n",
      "train loss: 2.024930715455113e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7429:\n",
      "train loss: 2.538895472539679e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7430:\n",
      "train loss: 2.0643089259930112e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7431:\n",
      "train loss: 2.5972184085462787e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7432:\n",
      "train loss: 4.462733088263054e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7433:\n",
      "train loss: 4.480625218803598e-16\n",
      "lr: 1.0185593959712353e-17\n",
      "Epoch 7434:\n",
      "train loss: 4.480625218803598e-16\n",
      "Epoch 07436: reducing learning rate of group 0 to 9.6763e-18.\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7435:\n",
      "train loss: 4.808978159339884e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7436:\n",
      "train loss: 4.808978159339884e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7437:\n",
      "train loss: 3.6357006469732556e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7438:\n",
      "train loss: 3.8910802274199397e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7439:\n",
      "train loss: 3.9554648246477645e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7440:\n",
      "train loss: 3.006430040806425e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7441:\n",
      "train loss: 2.019854449372527e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7442:\n",
      "train loss: 1.7852514197695354e-16\n",
      "lr: 9.676314261726736e-18\n",
      "Epoch 7443:\n",
      "train loss: 2.0479679197312834e-16\n",
      "Epoch 07445: reducing learning rate of group 0 to 9.1925e-18.\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7444:\n",
      "train loss: 2.654533302052947e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7445:\n",
      "train loss: 2.5666106267992125e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7446:\n",
      "train loss: 2.4657221822828287e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7447:\n",
      "train loss: 2.4657221822828287e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7448:\n",
      "train loss: 2.4657221822828287e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7449:\n",
      "train loss: 2.4657221822828287e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7450:\n",
      "train loss: 2.5186695533441455e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7451:\n",
      "train loss: 2.521913390957181e-16\n",
      "lr: 9.192498548640399e-18\n",
      "Epoch 7452:\n",
      "train loss: 2.521913390957181e-16\n",
      "Epoch 07454: reducing learning rate of group 0 to 8.7329e-18.\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7453:\n",
      "train loss: 4.3486036824659863e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7454:\n",
      "train loss: 2.9538125347951064e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7455:\n",
      "train loss: 1.953002968864958e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7456:\n",
      "train loss: 2.1312152221273317e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7457:\n",
      "train loss: 2.045186886872916e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7458:\n",
      "train loss: 2.045186886872916e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7459:\n",
      "train loss: 1.713119546300097e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7460:\n",
      "train loss: 4.226113488725466e-16\n",
      "lr: 8.732873621208379e-18\n",
      "Epoch 7461:\n",
      "train loss: 3.8011108041868805e-16\n",
      "Epoch 07463: reducing learning rate of group 0 to 8.2962e-18.\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7462:\n",
      "train loss: 3.8011108041868805e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7463:\n",
      "train loss: 3.8011108041868805e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7464:\n",
      "train loss: 3.8011108041868805e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7465:\n",
      "train loss: 3.8011108041868805e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7466:\n",
      "train loss: 1.804379552942001e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7467:\n",
      "train loss: 1.6154870471417735e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7468:\n",
      "train loss: 2.1549923919317744e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7469:\n",
      "train loss: 2.1549923919317744e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7470:\n",
      "train loss: 2.1549923919317744e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7471:\n",
      "train loss: 2.1549923919317744e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7472:\n",
      "train loss: 2.1549923919317744e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7473:\n",
      "train loss: 1.831214234652271e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7474:\n",
      "train loss: 1.831214234652271e-16\n",
      "lr: 8.29622994014796e-18\n",
      "Epoch 7475:\n",
      "train loss: 1.7935044476852988e-16\n",
      "Epoch 07477: reducing learning rate of group 0 to 7.8814e-18.\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7476:\n",
      "train loss: 2.0159813789350852e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7477:\n",
      "train loss: 3.3344427908047195e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7478:\n",
      "train loss: 3.0711637261502367e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7479:\n",
      "train loss: 3.0711637261502367e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7480:\n",
      "train loss: 3.0711637261502367e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7481:\n",
      "train loss: 3.060081105742029e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7482:\n",
      "train loss: 1.658380597332422e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7483:\n",
      "train loss: 1.6659890046074832e-16\n",
      "lr: 7.881418443140562e-18\n",
      "Epoch 7484:\n",
      "train loss: 1.6659890046074832e-16\n",
      "Epoch 07486: reducing learning rate of group 0 to 7.4873e-18.\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7485:\n",
      "train loss: 1.6659890046074832e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7486:\n",
      "train loss: 1.6659890046074832e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7487:\n",
      "train loss: 1.6659890046074832e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7488:\n",
      "train loss: 1.6820360744455628e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7489:\n",
      "train loss: 1.7874725414681567e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7490:\n",
      "train loss: 1.8791617989842233e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7491:\n",
      "train loss: 1.696583320919047e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7492:\n",
      "train loss: 2.5680088046852766e-16\n",
      "lr: 7.487347520983533e-18\n",
      "Epoch 7493:\n",
      "train loss: 2.559348062801393e-16\n",
      "Epoch 07495: reducing learning rate of group 0 to 7.1130e-18.\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7494:\n",
      "train loss: 2.7753909499791217e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7495:\n",
      "train loss: 2.3177964458991417e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7496:\n",
      "train loss: 2.3177964458991417e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7497:\n",
      "train loss: 2.0757925302893193e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7498:\n",
      "train loss: 2.0757925302893193e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7499:\n",
      "train loss: 2.387185284721938e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7500:\n",
      "train loss: 2.387185284721938e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7501:\n",
      "train loss: 2.387185284721938e-16\n",
      "lr: 7.112980144934355e-18\n",
      "Epoch 7502:\n",
      "train loss: 2.387185284721938e-16\n",
      "Epoch 07504: reducing learning rate of group 0 to 6.7573e-18.\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7503:\n",
      "train loss: 2.209158183660525e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7504:\n",
      "train loss: 1.975182815341262e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7505:\n",
      "train loss: 1.9771177329549763e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7506:\n",
      "train loss: 1.9771177329549763e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7507:\n",
      "train loss: 2.6944712471543267e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7508:\n",
      "train loss: 2.6944712471543267e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7509:\n",
      "train loss: 2.735884017570877e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7510:\n",
      "train loss: 2.735884017570877e-16\n",
      "lr: 6.757331137687637e-18\n",
      "Epoch 7511:\n",
      "train loss: 1.7951192150171724e-16\n",
      "Epoch 07513: reducing learning rate of group 0 to 6.4195e-18.\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7512:\n",
      "train loss: 1.7951192150171724e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7513:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7514:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7515:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7516:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7517:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7518:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7519:\n",
      "train loss: 1.63568679733985e-16\n",
      "lr: 6.419464580803255e-18\n",
      "Epoch 7520:\n",
      "train loss: 1.6406742718674742e-16\n",
      "Epoch 07522: reducing learning rate of group 0 to 6.0985e-18.\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7521:\n",
      "train loss: 1.7571838783711544e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7522:\n",
      "train loss: 2.5056753424751076e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7523:\n",
      "train loss: 2.8244942123491273e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7524:\n",
      "train loss: 2.405512383581565e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7525:\n",
      "train loss: 2.3271161723864584e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7526:\n",
      "train loss: 2.4356837608065565e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7527:\n",
      "train loss: 2.4356837608065565e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7528:\n",
      "train loss: 2.516836983914295e-16\n",
      "lr: 6.098491351763092e-18\n",
      "Epoch 7529:\n",
      "train loss: 2.2982083098763507e-16\n",
      "Epoch 07531: reducing learning rate of group 0 to 5.7936e-18.\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7530:\n",
      "train loss: 2.3717512370169285e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7531:\n",
      "train loss: 2.3717512370169285e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7532:\n",
      "train loss: 2.3569952022837104e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7533:\n",
      "train loss: 2.3569952022837104e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7534:\n",
      "train loss: 2.3569952022837104e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7535:\n",
      "train loss: 2.5256206798546054e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7536:\n",
      "train loss: 2.5256206798546054e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7537:\n",
      "train loss: 2.3711134306882726e-16\n",
      "lr: 5.793566784174937e-18\n",
      "Epoch 7538:\n",
      "train loss: 2.3711134306882726e-16\n",
      "Epoch 07540: reducing learning rate of group 0 to 5.5039e-18.\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7539:\n",
      "train loss: 2.3711134306882726e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7540:\n",
      "train loss: 2.73436537198194e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7541:\n",
      "train loss: 3.538724201033152e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7542:\n",
      "train loss: 3.0412796854943305e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7543:\n",
      "train loss: 3.0412796854943305e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7544:\n",
      "train loss: 1.9232217367385734e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7545:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7546:\n",
      "train loss: 1.4041582559480757e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7547:\n",
      "train loss: 1.4041582559480757e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7548:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7549:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7550:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7551:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7552:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7553:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7554:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7555:\n",
      "train loss: 1.3168879669405007e-16\n",
      "lr: 5.50388844496619e-18\n",
      "Epoch 7556:\n",
      "train loss: 1.3168879669405007e-16\n",
      "Epoch 07558: reducing learning rate of group 0 to 5.2287e-18.\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7557:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7558:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7559:\n",
      "train loss: 1.5725285771890604e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7560:\n",
      "train loss: 1.855643917151013e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7561:\n",
      "train loss: 1.855643917151013e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7562:\n",
      "train loss: 1.855643917151013e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7563:\n",
      "train loss: 1.855643917151013e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7564:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 5.22869402271788e-18\n",
      "Epoch 7565:\n",
      "train loss: 1.5189661629441746e-16\n",
      "Epoch 07567: reducing learning rate of group 0 to 4.9673e-18.\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7566:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7567:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7568:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7569:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7570:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7571:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7572:\n",
      "train loss: 1.5189661629441746e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7573:\n",
      "train loss: 1.5086873649643852e-16\n",
      "lr: 4.967259321581986e-18\n",
      "Epoch 7574:\n",
      "train loss: 1.7036156743261963e-16\n",
      "Epoch 07576: reducing learning rate of group 0 to 4.7189e-18.\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7575:\n",
      "train loss: 1.8975759389670517e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7576:\n",
      "train loss: 1.8896425858911146e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7577:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7578:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7579:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7580:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7581:\n",
      "train loss: 2.961077608148273e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7582:\n",
      "train loss: 2.6598683700253316e-16\n",
      "lr: 4.718896355502886e-18\n",
      "Epoch 7583:\n",
      "train loss: 2.6598683700253316e-16\n",
      "Epoch 07585: reducing learning rate of group 0 to 4.4830e-18.\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7584:\n",
      "train loss: 2.6598683700253316e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7585:\n",
      "train loss: 2.6598683700253316e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7586:\n",
      "train loss: 2.612385478233062e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7587:\n",
      "train loss: 2.937952059779552e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7588:\n",
      "train loss: 2.937952059779552e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7589:\n",
      "train loss: 2.9316110531938864e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7590:\n",
      "train loss: 1.6024193878808964e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7591:\n",
      "train loss: 1.6024193878808964e-16\n",
      "lr: 4.482951537727742e-18\n",
      "Epoch 7592:\n",
      "train loss: 1.6024193878808964e-16\n",
      "Epoch 07594: reducing learning rate of group 0 to 4.2588e-18.\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7593:\n",
      "train loss: 1.6024193878808964e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7594:\n",
      "train loss: 1.6024193878808964e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7595:\n",
      "train loss: 1.6024193878808964e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7596:\n",
      "train loss: 1.6024193878808964e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7597:\n",
      "train loss: 1.397189569444595e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7598:\n",
      "train loss: 1.397189569444595e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7599:\n",
      "train loss: 1.397189569444595e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7600:\n",
      "train loss: 1.397189569444595e-16\n",
      "lr: 4.258803960841354e-18\n",
      "Epoch 7601:\n",
      "train loss: 1.8883211411165125e-16\n",
      "Epoch 07603: reducing learning rate of group 0 to 4.0459e-18.\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7602:\n",
      "train loss: 1.8883211411165125e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7603:\n",
      "train loss: 1.8639633546756013e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7604:\n",
      "train loss: 1.8639633546756013e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7605:\n",
      "train loss: 1.8239442666073296e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7606:\n",
      "train loss: 1.6829480720829747e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7607:\n",
      "train loss: 1.8308475182772965e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7608:\n",
      "train loss: 1.8308475182772965e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7609:\n",
      "train loss: 1.8308475182772965e-16\n",
      "lr: 4.0458637627992864e-18\n",
      "Epoch 7610:\n",
      "train loss: 1.4253504461921102e-16\n",
      "Epoch 07612: reducing learning rate of group 0 to 3.8436e-18.\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7611:\n",
      "train loss: 1.4253504461921102e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7612:\n",
      "train loss: 1.4253504461921102e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7613:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7614:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7615:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7616:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7617:\n",
      "train loss: 3.162601272658515e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7618:\n",
      "train loss: 3.162601272658515e-16\n",
      "lr: 3.843570574659322e-18\n",
      "Epoch 7619:\n",
      "train loss: 2.8778636536308743e-16\n",
      "Epoch 07621: reducing learning rate of group 0 to 3.6514e-18.\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7620:\n",
      "train loss: 3.1755551119233703e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7621:\n",
      "train loss: 3.1755551119233703e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7622:\n",
      "train loss: 2.5080540990164607e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7623:\n",
      "train loss: 2.5080540990164607e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7624:\n",
      "train loss: 2.5080540990164607e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7625:\n",
      "train loss: 2.5080540990164607e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7626:\n",
      "train loss: 2.5080540990164607e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7627:\n",
      "train loss: 2.3029366500438126e-16\n",
      "lr: 3.651392045926356e-18\n",
      "Epoch 7628:\n",
      "train loss: 2.363225623307994e-16\n",
      "Epoch 07630: reducing learning rate of group 0 to 3.4688e-18.\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7629:\n",
      "train loss: 2.363225623307994e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7630:\n",
      "train loss: 2.3602482888022255e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7631:\n",
      "train loss: 2.3602482888022255e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7632:\n",
      "train loss: 2.3602482888022255e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7633:\n",
      "train loss: 3.0865773642883253e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7634:\n",
      "train loss: 2.993332723557099e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7635:\n",
      "train loss: 2.993332723557099e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7636:\n",
      "train loss: 2.993332723557099e-16\n",
      "lr: 3.4688224436300383e-18\n",
      "Epoch 7637:\n",
      "train loss: 2.993332723557099e-16\n",
      "Epoch 07639: reducing learning rate of group 0 to 3.2954e-18.\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7638:\n",
      "train loss: 2.8784187421365213e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7639:\n",
      "train loss: 2.8784187421365213e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7640:\n",
      "train loss: 2.8784187421365213e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7641:\n",
      "train loss: 2.8784187421365213e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7642:\n",
      "train loss: 2.6330196147141713e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7643:\n",
      "train loss: 2.7000155727839025e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7644:\n",
      "train loss: 2.7000155727839025e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7645:\n",
      "train loss: 1.6003373781401003e-16\n",
      "lr: 3.2953813214485363e-18\n",
      "Epoch 7646:\n",
      "train loss: 1.6003373781401003e-16\n",
      "Epoch 07648: reducing learning rate of group 0 to 3.1306e-18.\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7647:\n",
      "train loss: 1.6003373781401003e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7648:\n",
      "train loss: 1.6003373781401003e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7649:\n",
      "train loss: 1.6315444762387486e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7650:\n",
      "train loss: 1.6315444762387486e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7651:\n",
      "train loss: 1.5462309050640716e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7652:\n",
      "train loss: 1.5462309050640716e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7653:\n",
      "train loss: 1.5462309050640716e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7654:\n",
      "train loss: 1.5462309050640716e-16\n",
      "lr: 3.1306122553761093e-18\n",
      "Epoch 7655:\n",
      "train loss: 1.5382577429532593e-16\n",
      "Epoch 07657: reducing learning rate of group 0 to 2.9741e-18.\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7656:\n",
      "train loss: 1.5382577429532593e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7657:\n",
      "train loss: 1.5526441527844912e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7658:\n",
      "train loss: 1.5526441527844912e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7659:\n",
      "train loss: 1.5526441527844912e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7660:\n",
      "train loss: 1.5526441527844912e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7661:\n",
      "train loss: 1.5526441527844912e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7662:\n",
      "train loss: 1.7351840035707244e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7663:\n",
      "train loss: 1.902464311191335e-16\n",
      "lr: 2.974081642607304e-18\n",
      "Epoch 7664:\n",
      "train loss: 1.902464311191335e-16\n",
      "Epoch 07666: reducing learning rate of group 0 to 2.8254e-18.\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7665:\n",
      "train loss: 1.902464311191335e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7666:\n",
      "train loss: 1.902464311191335e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7667:\n",
      "train loss: 1.7254204600815247e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7668:\n",
      "train loss: 1.7254204600815247e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7669:\n",
      "train loss: 1.7254204600815247e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7670:\n",
      "train loss: 1.7254204600815247e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7671:\n",
      "train loss: 1.7470719967493472e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7672:\n",
      "train loss: 1.4273172086828326e-16\n",
      "lr: 2.8253775604769387e-18\n",
      "Epoch 7673:\n",
      "train loss: 1.4273172086828326e-16\n",
      "Epoch 07675: reducing learning rate of group 0 to 2.6841e-18.\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7674:\n",
      "train loss: 1.4475212030972365e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7675:\n",
      "train loss: 1.6956638173226572e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7676:\n",
      "train loss: 1.6956638173226572e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7677:\n",
      "train loss: 1.6956638173226572e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7678:\n",
      "train loss: 1.5382577429532593e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7679:\n",
      "train loss: 1.5382577429532593e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7680:\n",
      "train loss: 1.5382577429532593e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7681:\n",
      "train loss: 1.5382577429532593e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7682:\n",
      "train loss: 1.3041751588330739e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7683:\n",
      "train loss: 1.3253985295590003e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7684:\n",
      "train loss: 1.9593171519724333e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7685:\n",
      "train loss: 1.9593171519724333e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7686:\n",
      "train loss: 2.1329453472001355e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7687:\n",
      "train loss: 2.44203178522801e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7688:\n",
      "train loss: 2.44203178522801e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7689:\n",
      "train loss: 2.44203178522801e-16\n",
      "lr: 2.684108682453092e-18\n",
      "Epoch 7690:\n",
      "train loss: 2.44203178522801e-16\n",
      "Epoch 07692: reducing learning rate of group 0 to 2.5499e-18.\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7691:\n",
      "train loss: 2.961077608148273e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7692:\n",
      "train loss: 2.1640971100900605e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7693:\n",
      "train loss: 2.1640971100900605e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7694:\n",
      "train loss: 2.1640971100900605e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7695:\n",
      "train loss: 2.1640971100900605e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7696:\n",
      "train loss: 3.558773592398004e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7697:\n",
      "train loss: 3.558773592398004e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7698:\n",
      "train loss: 3.558773592398004e-16\n",
      "lr: 2.5499032483304373e-18\n",
      "Epoch 7699:\n",
      "train loss: 3.558773592398004e-16\n",
      "Epoch 07701: reducing learning rate of group 0 to 2.4224e-18.\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7700:\n",
      "train loss: 3.558773592398004e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7701:\n",
      "train loss: 3.558773592398004e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7702:\n",
      "train loss: 3.556713003516452e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7703:\n",
      "train loss: 1.7411260887276074e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7704:\n",
      "train loss: 1.7411260887276074e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7705:\n",
      "train loss: 1.5725285771890604e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7706:\n",
      "train loss: 1.5725285771890604e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7707:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.4224080859139154e-18\n",
      "Epoch 7708:\n",
      "train loss: 1.3492058880701386e-16\n",
      "Epoch 07710: reducing learning rate of group 0 to 2.3013e-18.\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7709:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7710:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7711:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7712:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7713:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7714:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7715:\n",
      "train loss: 1.3492058880701386e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7716:\n",
      "train loss: 1.5725285771890604e-16\n",
      "lr: 2.3012876816182196e-18\n",
      "Epoch 7717:\n",
      "train loss: 1.5725285771890604e-16\n",
      "Epoch 07719: reducing learning rate of group 0 to 2.1862e-18.\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7718:\n",
      "train loss: 1.5725285771890604e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7719:\n",
      "train loss: 1.4864811015513078e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7720:\n",
      "train loss: 1.4864811015513078e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7721:\n",
      "train loss: 1.4942994333061206e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7722:\n",
      "train loss: 1.4942994333061206e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7723:\n",
      "train loss: 1.387309024815929e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7724:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7725:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.1862232975373084e-18\n",
      "Epoch 7726:\n",
      "train loss: 1.7983080234481825e-16\n",
      "Epoch 07728: reducing learning rate of group 0 to 2.0769e-18.\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7727:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7728:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7729:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7730:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7731:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7732:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7733:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7734:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 2.0769121326604427e-18\n",
      "Epoch 7735:\n",
      "train loss: 1.7983080234481825e-16\n",
      "Epoch 07737: reducing learning rate of group 0 to 1.9731e-18.\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7736:\n",
      "train loss: 1.7983080234481825e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7737:\n",
      "train loss: 2.0461106441498693e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7738:\n",
      "train loss: 2.0461106441498693e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7739:\n",
      "train loss: 2.0461106441498693e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7740:\n",
      "train loss: 2.873949791457876e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7741:\n",
      "train loss: 2.873949791457876e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7742:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7743:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 1.9730665260274207e-18\n",
      "Epoch 7744:\n",
      "train loss: 3.143227826152183e-16\n",
      "Epoch 07746: reducing learning rate of group 0 to 1.8744e-18.\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7745:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7746:\n",
      "train loss: 3.143227826152183e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7747:\n",
      "train loss: 2.8475447949346075e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7748:\n",
      "train loss: 2.8475447949346075e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7749:\n",
      "train loss: 2.8475447949346075e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7750:\n",
      "train loss: 2.8475447949346075e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7751:\n",
      "train loss: 2.8475447949346075e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7752:\n",
      "train loss: 2.8608875294089476e-16\n",
      "lr: 1.8744131997260496e-18\n",
      "Epoch 7753:\n",
      "train loss: 2.8778636536308743e-16\n",
      "Epoch 07755: reducing learning rate of group 0 to 1.7807e-18.\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7754:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7755:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7756:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7757:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7758:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7759:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7760:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7761:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.780692539739747e-18\n",
      "Epoch 7762:\n",
      "train loss: 2.8778636536308743e-16\n",
      "Epoch 07764: reducing learning rate of group 0 to 1.6917e-18.\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7763:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7764:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7765:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7766:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7767:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7768:\n",
      "train loss: 2.747848699397179e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7769:\n",
      "train loss: 2.747848699397179e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7770:\n",
      "train loss: 2.747848699397179e-16\n",
      "lr: 1.6916579127527596e-18\n",
      "Epoch 7771:\n",
      "train loss: 2.747848699397179e-16\n",
      "Epoch 07773: reducing learning rate of group 0 to 1.6071e-18.\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7772:\n",
      "train loss: 2.5064642720634716e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7773:\n",
      "train loss: 2.5064642720634716e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7774:\n",
      "train loss: 2.5064642720634716e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7775:\n",
      "train loss: 2.5064642720634716e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7776:\n",
      "train loss: 2.5064642720634716e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7777:\n",
      "train loss: 2.5064642720634716e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7778:\n",
      "train loss: 2.225815385552137e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7779:\n",
      "train loss: 2.3947058027557526e-16\n",
      "lr: 1.6070750171151216e-18\n",
      "Epoch 7780:\n",
      "train loss: 2.3947058027557526e-16\n",
      "Epoch 07782: reducing learning rate of group 0 to 1.5267e-18.\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7781:\n",
      "train loss: 2.3947058027557526e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7782:\n",
      "train loss: 2.3947058027557526e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7783:\n",
      "train loss: 2.3947058027557526e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7784:\n",
      "train loss: 2.1779495020723564e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7785:\n",
      "train loss: 2.1779495020723564e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7786:\n",
      "train loss: 2.1197793756930268e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7787:\n",
      "train loss: 2.1197793756930268e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7788:\n",
      "train loss: 2.1197793756930268e-16\n",
      "lr: 1.5267212662593654e-18\n",
      "Epoch 7789:\n",
      "train loss: 2.1197793756930268e-16\n",
      "Epoch 07791: reducing learning rate of group 0 to 1.4504e-18.\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7790:\n",
      "train loss: 2.1197793756930268e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7791:\n",
      "train loss: 2.1197793756930268e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7792:\n",
      "train loss: 2.41218869989084e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7793:\n",
      "train loss: 2.3980105337937326e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7794:\n",
      "train loss: 2.3980105337937326e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7795:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7796:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7797:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.450385202946397e-18\n",
      "Epoch 7798:\n",
      "train loss: 1.5507793180291769e-16\n",
      "Epoch 07800: reducing learning rate of group 0 to 1.3779e-18.\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7799:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7800:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7801:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7802:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7803:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7804:\n",
      "train loss: 1.5507793180291769e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7805:\n",
      "train loss: 2.6444051147114855e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7806:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3778659427990772e-18\n",
      "Epoch 7807:\n",
      "train loss: 2.3841552375924042e-16\n",
      "Epoch 07809: reducing learning rate of group 0 to 1.3090e-18.\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7808:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7809:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7810:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7811:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7812:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7813:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7814:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7815:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.3089726456591233e-18\n",
      "Epoch 7816:\n",
      "train loss: 2.3841552375924042e-16\n",
      "Epoch 07818: reducing learning rate of group 0 to 1.2435e-18.\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7817:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7818:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7819:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7820:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7821:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7822:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7823:\n",
      "train loss: 2.3841552375924042e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7824:\n",
      "train loss: 1.875975267605992e-16\n",
      "lr: 1.243524013376167e-18\n",
      "Epoch 7825:\n",
      "train loss: 1.875975267605992e-16\n",
      "Epoch 07827: reducing learning rate of group 0 to 1.1813e-18.\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7826:\n",
      "train loss: 1.875975267605992e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7827:\n",
      "train loss: 1.875975267605992e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7828:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7829:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7830:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7831:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7832:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7833:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1813478127073586e-18\n",
      "Epoch 7834:\n",
      "train loss: 1.5632180659971157e-16\n",
      "Epoch 07836: reducing learning rate of group 0 to 1.1223e-18.\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7835:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7836:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7837:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7838:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7839:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7840:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7841:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7842:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.1222804220719906e-18\n",
      "Epoch 7843:\n",
      "train loss: 1.5632180659971157e-16\n",
      "Epoch 07845: reducing learning rate of group 0 to 1.0662e-18.\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7844:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7845:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7846:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7847:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7848:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7849:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7850:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7851:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.066166400968391e-18\n",
      "Epoch 7852:\n",
      "train loss: 1.5632180659971157e-16\n",
      "Epoch 07854: reducing learning rate of group 0 to 1.0129e-18.\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7853:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7854:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7855:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7856:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7857:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7858:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7859:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7860:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 1.0128580809199715e-18\n",
      "Epoch 7861:\n",
      "train loss: 2.8778636536308743e-16\n",
      "Epoch 07863: reducing learning rate of group 0 to 9.6222e-19.\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7862:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7863:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7864:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7865:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7866:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7867:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7868:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7869:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.622151768739729e-19\n",
      "Epoch 7870:\n",
      "train loss: 2.8778636536308743e-16\n",
      "Epoch 07872: reducing learning rate of group 0 to 9.1410e-19.\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7871:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7872:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7873:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7874:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7875:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7876:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7877:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7878:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 9.141044180302743e-19\n",
      "Epoch 7879:\n",
      "train loss: 1.5632180659971157e-16\n",
      "Epoch 07881: reducing learning rate of group 0 to 8.6840e-19.\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7880:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7881:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7882:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7883:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7884:\n",
      "train loss: 1.5632180659971157e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7885:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7886:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7887:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 8.683991971287606e-19\n",
      "Epoch 7888:\n",
      "train loss: 2.8778636536308743e-16\n",
      "Epoch 07890: reducing learning rate of group 0 to 8.2498e-19.\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7889:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7890:\n",
      "train loss: 2.8778636536308743e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7891:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7892:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7893:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7894:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7895:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7896:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.249792372723225e-19\n",
      "Epoch 7897:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 07899: reducing learning rate of group 0 to 7.8373e-19.\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7898:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7899:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7900:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7901:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7902:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7903:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7904:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7905:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.837302754087064e-19\n",
      "Epoch 7906:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 07908: reducing learning rate of group 0 to 7.4454e-19.\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7907:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7908:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7909:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7910:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7911:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7912:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7913:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7914:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.44543761638271e-19\n",
      "Epoch 7915:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 07917: reducing learning rate of group 0 to 7.0732e-19.\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7916:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7917:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7918:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7919:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7920:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7921:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7922:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7923:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.073165735563574e-19\n",
      "Epoch 7924:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 07926: reducing learning rate of group 0 to 6.7195e-19.\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7925:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7926:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7927:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7928:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7929:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7930:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7931:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7932:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.719507448785395e-19\n",
      "Epoch 7933:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 07935: reducing learning rate of group 0 to 6.3835e-19.\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7934:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7935:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7936:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7937:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7938:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7939:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7940:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7941:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.383532076346125e-19\n",
      "Epoch 7942:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 07944: reducing learning rate of group 0 to 6.0644e-19.\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7943:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7944:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7945:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7946:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7947:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7948:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7949:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7950:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 6.064355472528819e-19\n",
      "Epoch 7951:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 07953: reducing learning rate of group 0 to 5.7611e-19.\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7952:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7953:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7954:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7955:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7956:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7957:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7958:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7959:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.761137698902377e-19\n",
      "Epoch 7960:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 07962: reducing learning rate of group 0 to 5.4731e-19.\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7961:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7962:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7963:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7964:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7965:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7966:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7967:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7968:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.473080813957258e-19\n",
      "Epoch 7969:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 07971: reducing learning rate of group 0 to 5.1994e-19.\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7970:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7971:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7972:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7973:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7974:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7975:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7976:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7977:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 5.199426773259396e-19\n",
      "Epoch 7978:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 07980: reducing learning rate of group 0 to 4.9395e-19.\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7979:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7980:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7981:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7982:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7983:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7984:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7985:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7986:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.939455434596425e-19\n",
      "Epoch 7987:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 07989: reducing learning rate of group 0 to 4.6925e-19.\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7988:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7989:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7990:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7991:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7992:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7993:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7994:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7995:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.692482662866604e-19\n",
      "Epoch 7996:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 07998: reducing learning rate of group 0 to 4.4579e-19.\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 7997:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 7998:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 7999:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8000:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8001:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8002:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8003:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8004:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.457858529723274e-19\n",
      "Epoch 8005:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08007: reducing learning rate of group 0 to 4.2350e-19.\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8006:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8007:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8008:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8009:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8010:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8011:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8012:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8013:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.23496560323711e-19\n",
      "Epoch 8014:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08016: reducing learning rate of group 0 to 4.0232e-19.\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8015:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8016:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8017:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8018:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8019:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8020:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8021:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8022:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 4.0232173230752545e-19\n",
      "Epoch 8023:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08025: reducing learning rate of group 0 to 3.8221e-19.\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8024:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8025:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8026:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8027:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8028:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8029:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8030:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8031:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.8220564569214917e-19\n",
      "Epoch 8032:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 08034: reducing learning rate of group 0 to 3.6310e-19.\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8033:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8034:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8035:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8036:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8037:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8038:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8039:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8040:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.630953634075417e-19\n",
      "Epoch 8041:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08043: reducing learning rate of group 0 to 3.4494e-19.\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8042:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8043:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8044:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8045:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8046:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8047:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8048:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8049:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.449405952371646e-19\n",
      "Epoch 8050:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08052: reducing learning rate of group 0 to 3.2769e-19.\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8051:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8052:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8053:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8054:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8055:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8056:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8057:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8058:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.2769356547530635e-19\n",
      "Epoch 8059:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 08061: reducing learning rate of group 0 to 3.1131e-19.\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8060:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8061:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8062:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8063:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8064:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8065:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8066:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8067:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.1130888720154103e-19\n",
      "Epoch 8068:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08070: reducing learning rate of group 0 to 2.9574e-19.\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8069:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8070:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8071:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8072:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8073:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8074:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8075:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8076:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.9574344284146397e-19\n",
      "Epoch 8077:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08079: reducing learning rate of group 0 to 2.8096e-19.\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8078:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8079:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8080:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8081:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8082:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8083:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8084:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8085:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.8095627069939075e-19\n",
      "Epoch 8086:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08088: reducing learning rate of group 0 to 2.6691e-19.\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8087:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8088:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8089:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8090:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8091:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8092:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8093:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8094:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.669084571644212e-19\n",
      "Epoch 8095:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08097: reducing learning rate of group 0 to 2.5356e-19.\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8096:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8097:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8098:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8099:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8100:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8101:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8102:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8103:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.535630343062001e-19\n",
      "Epoch 8104:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08106: reducing learning rate of group 0 to 2.4088e-19.\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8105:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8106:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8107:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8108:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8109:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8110:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8111:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8112:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4088488259089006e-19\n",
      "Epoch 8113:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08115: reducing learning rate of group 0 to 2.2884e-19.\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8114:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8115:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8116:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8117:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8118:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8119:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8120:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8121:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2884063846134555e-19\n",
      "Epoch 8122:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08124: reducing learning rate of group 0 to 2.1740e-19.\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8123:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8124:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8125:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8126:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8127:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8128:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8129:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8130:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 2.1739860653827826e-19\n",
      "Epoch 8131:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 08133: reducing learning rate of group 0 to 2.0653e-19.\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8132:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8133:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8134:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8135:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8136:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8137:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8138:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8139:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.0652867621136434e-19\n",
      "Epoch 8140:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08142: reducing learning rate of group 0 to 1.9620e-19.\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8141:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8142:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8143:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8144:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8145:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8146:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8147:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8148:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.962022424007961e-19\n",
      "Epoch 8149:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08151: reducing learning rate of group 0 to 1.8639e-19.\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8150:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8151:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8152:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8153:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8154:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8155:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8156:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8157:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.863921302807563e-19\n",
      "Epoch 8158:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 08160: reducing learning rate of group 0 to 1.7707e-19.\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8159:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8160:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8161:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8162:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8163:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8164:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8165:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8166:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 1.7707252376671849e-19\n",
      "Epoch 8167:\n",
      "train loss: 3.3652543722031035e-16\n",
      "Epoch 08169: reducing learning rate of group 0 to 1.6822e-19.\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8168:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8169:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8170:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8171:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8172:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8173:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8174:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8175:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6821889757838255e-19\n",
      "Epoch 8176:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08178: reducing learning rate of group 0 to 1.5981e-19.\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8177:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8178:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8179:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8180:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8181:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8182:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8183:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8184:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5980795269946342e-19\n",
      "Epoch 8185:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 08187: reducing learning rate of group 0 to 1.5182e-19.\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8186:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8187:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8188:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8189:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8190:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8191:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8192:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8193:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.5181755506449024e-19\n",
      "Epoch 8194:\n",
      "train loss: 2.6297317058456347e-16\n",
      "Epoch 08196: reducing learning rate of group 0 to 1.4423e-19.\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8195:\n",
      "train loss: 2.6297317058456347e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8196:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8197:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8198:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8199:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8200:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8201:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8202:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.442266773112657e-19\n",
      "Epoch 8203:\n",
      "train loss: 3.3652543722031035e-16\n",
      "Epoch 08205: reducing learning rate of group 0 to 1.3702e-19.\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8204:\n",
      "train loss: 3.3652543722031035e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8205:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8206:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8207:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8208:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8209:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8210:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8211:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3701534344570241e-19\n",
      "Epoch 8212:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08214: reducing learning rate of group 0 to 1.3016e-19.\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8213:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8214:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8215:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8216:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8217:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8218:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8219:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8220:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3016457627341728e-19\n",
      "Epoch 8221:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08223: reducing learning rate of group 0 to 1.2366e-19.\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8222:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8223:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8224:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8225:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8226:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8227:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8228:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8229:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.236563474597464e-19\n",
      "Epoch 8230:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08232: reducing learning rate of group 0 to 1.1747e-19.\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8231:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8232:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8233:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8234:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8235:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8236:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8237:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8238:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1747353008675908e-19\n",
      "Epoch 8239:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08241: reducing learning rate of group 0 to 1.1160e-19.\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8240:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8241:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8242:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8243:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8244:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8245:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8246:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8247:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1159985358242111e-19\n",
      "Epoch 8248:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08250: reducing learning rate of group 0 to 1.0602e-19.\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8249:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8250:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8251:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8252:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8253:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8254:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8255:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8256:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0601986090330005e-19\n",
      "Epoch 8257:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08259: reducing learning rate of group 0 to 1.0072e-19.\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8258:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8259:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8260:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8261:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8262:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8263:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8264:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8265:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0071886785813504e-19\n",
      "Epoch 8266:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08268: reducing learning rate of group 0 to 9.5683e-20.\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8267:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8268:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8269:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8270:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8271:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8272:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8273:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8274:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.568292446522828e-20\n",
      "Epoch 8275:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08277: reducing learning rate of group 0 to 9.0899e-20.\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8276:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8277:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8278:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8279:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8280:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8281:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8282:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8283:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.089877824196687e-20\n",
      "Epoch 8284:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08286: reducing learning rate of group 0 to 8.6354e-20.\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8285:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8286:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8287:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8288:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8289:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8290:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8291:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8292:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.635383932986852e-20\n",
      "Epoch 8293:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08295: reducing learning rate of group 0 to 8.2036e-20.\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8294:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8295:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8296:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8297:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8298:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8299:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8300:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8301:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.20361473633751e-20\n",
      "Epoch 8302:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08304: reducing learning rate of group 0 to 7.7934e-20.\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8303:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8304:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8305:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8306:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8307:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8308:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8309:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8310:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.793433999520634e-20\n",
      "Epoch 8311:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08313: reducing learning rate of group 0 to 7.4038e-20.\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8312:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8313:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8314:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8315:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8316:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8317:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8318:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8319:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.403762299544603e-20\n",
      "Epoch 8320:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08322: reducing learning rate of group 0 to 7.0336e-20.\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8321:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8322:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8323:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8324:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8325:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8326:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8327:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8328:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.033574184567372e-20\n",
      "Epoch 8329:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08331: reducing learning rate of group 0 to 6.6819e-20.\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8330:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8331:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8332:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8333:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8334:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8335:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8336:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8337:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.681895475339002e-20\n",
      "Epoch 8338:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08340: reducing learning rate of group 0 to 6.3478e-20.\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8339:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8340:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8341:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8342:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8343:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8344:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8345:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8346:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.347800701572053e-20\n",
      "Epoch 8347:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08349: reducing learning rate of group 0 to 6.0304e-20.\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8348:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8349:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8350:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8351:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8352:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8353:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8354:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8355:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.03041066649345e-20\n",
      "Epoch 8356:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08358: reducing learning rate of group 0 to 5.7289e-20.\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8357:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8358:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8359:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8360:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8361:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8362:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8363:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8364:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.728890133168777e-20\n",
      "Epoch 8365:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08367: reducing learning rate of group 0 to 5.4424e-20.\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8366:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8367:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8368:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8369:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8370:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8371:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8372:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8373:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.442445626510338e-20\n",
      "Epoch 8374:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08376: reducing learning rate of group 0 to 5.1703e-20.\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8375:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8376:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8377:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8378:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8379:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8380:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8381:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8382:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.1703233451848206e-20\n",
      "Epoch 8383:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08385: reducing learning rate of group 0 to 4.9118e-20.\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8384:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8385:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8386:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8387:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8388:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8389:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8390:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8391:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.9118071779255794e-20\n",
      "Epoch 8392:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08394: reducing learning rate of group 0 to 4.6662e-20.\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8393:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8394:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8395:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8396:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8397:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8398:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8399:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8400:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.6662168190293004e-20\n",
      "Epoch 8401:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08403: reducing learning rate of group 0 to 4.4329e-20.\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8402:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8403:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8404:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8405:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8406:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8407:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8408:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8409:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.432905978077835e-20\n",
      "Epoch 8410:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08412: reducing learning rate of group 0 to 4.2113e-20.\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8411:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8412:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8413:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8414:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8415:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8416:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8417:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8418:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.211260679173943e-20\n",
      "Epoch 8419:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08421: reducing learning rate of group 0 to 4.0007e-20.\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8420:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8421:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8422:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8423:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8424:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8425:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8426:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8427:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.000697645215246e-20\n",
      "Epoch 8428:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08430: reducing learning rate of group 0 to 3.8007e-20.\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8429:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8430:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8431:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8432:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8433:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8434:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8435:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8436:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.8006627629544834e-20\n",
      "Epoch 8437:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08439: reducing learning rate of group 0 to 3.6106e-20.\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8438:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8439:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8440:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8441:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8442:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8443:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8444:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8445:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.610629624806759e-20\n",
      "Epoch 8446:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08448: reducing learning rate of group 0 to 3.4301e-20.\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8447:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8448:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8449:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8450:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8451:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8452:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8453:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8454:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.430098143566421e-20\n",
      "Epoch 8455:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08457: reducing learning rate of group 0 to 3.2586e-20.\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8456:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8457:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8458:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8459:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8460:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8461:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8462:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8463:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2585932363881e-20\n",
      "Epoch 8464:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08466: reducing learning rate of group 0 to 3.0957e-20.\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8465:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8466:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8467:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8468:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8469:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8470:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8471:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8472:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0956635745686946e-20\n",
      "Epoch 8473:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08475: reducing learning rate of group 0 to 2.9409e-20.\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8474:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8475:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8476:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8477:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8478:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8479:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8480:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8481:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.9408803958402597e-20\n",
      "Epoch 8482:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08484: reducing learning rate of group 0 to 2.7938e-20.\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8483:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8484:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8485:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8486:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8487:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8488:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8489:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8490:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7938363760482465e-20\n",
      "Epoch 8491:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08493: reducing learning rate of group 0 to 2.6541e-20.\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8492:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8493:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8494:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8495:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8496:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8497:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8498:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8499:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.654144557245834e-20\n",
      "Epoch 8500:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08502: reducing learning rate of group 0 to 2.5214e-20.\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8501:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8502:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8503:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8504:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8505:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8506:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8507:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8508:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.521437329383542e-20\n",
      "Epoch 8509:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08511: reducing learning rate of group 0 to 2.3954e-20.\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8510:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8511:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8512:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8513:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8514:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8515:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8516:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8517:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3953654629143648e-20\n",
      "Epoch 8518:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08520: reducing learning rate of group 0 to 2.2756e-20.\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8519:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8520:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8521:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8522:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8523:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8524:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8525:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8526:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2755971897686465e-20\n",
      "Epoch 8527:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08529: reducing learning rate of group 0 to 2.1618e-20.\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8528:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8529:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8530:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8531:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8532:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8533:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8534:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8535:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.161817330280214e-20\n",
      "Epoch 8536:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08538: reducing learning rate of group 0 to 2.0537e-20.\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8537:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8538:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8539:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8540:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8541:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8542:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8543:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8544:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.053726463766203e-20\n",
      "Epoch 8545:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08547: reducing learning rate of group 0 to 1.9510e-20.\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8546:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8547:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8548:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8549:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8550:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8551:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8552:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8553:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.951040140577893e-20\n",
      "Epoch 8554:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08556: reducing learning rate of group 0 to 1.8535e-20.\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8555:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8556:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8557:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8558:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8559:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8560:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8561:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8562:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8534881335489984e-20\n",
      "Epoch 8563:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08565: reducing learning rate of group 0 to 1.7608e-20.\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8564:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8565:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8566:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8567:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8568:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8569:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8570:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8571:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7608137268715483e-20\n",
      "Epoch 8572:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08574: reducing learning rate of group 0 to 1.6728e-20.\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8573:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8574:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8575:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8576:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8577:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8578:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8579:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8580:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6727730405279707e-20\n",
      "Epoch 8581:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08583: reducing learning rate of group 0 to 1.5891e-20.\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8582:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8583:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8584:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8585:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8586:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8587:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8588:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8589:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.589134388501572e-20\n",
      "Epoch 8590:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08592: reducing learning rate of group 0 to 1.5097e-20.\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8591:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8592:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8593:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8594:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8595:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8596:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8597:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8598:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5096776690764934e-20\n",
      "Epoch 8599:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08601: reducing learning rate of group 0 to 1.4342e-20.\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8600:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8601:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8602:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8603:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8604:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8605:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8606:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8607:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4341937856226687e-20\n",
      "Epoch 8608:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08610: reducing learning rate of group 0 to 1.3625e-20.\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8609:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8610:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8611:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8612:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8613:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8614:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8615:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8616:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.3624840963415353e-20\n",
      "Epoch 8617:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08619: reducing learning rate of group 0 to 1.2944e-20.\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8618:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8619:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8620:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8621:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8622:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8623:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8624:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8625:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2943598915244584e-20\n",
      "Epoch 8626:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08628: reducing learning rate of group 0 to 1.2296e-20.\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8627:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8628:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8629:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8630:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8631:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8632:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8633:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8634:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2296418969482354e-20\n",
      "Epoch 8635:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08637: reducing learning rate of group 0 to 1.1682e-20.\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8636:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8637:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8638:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8639:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8640:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8641:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8642:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8643:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1681598021008235e-20\n",
      "Epoch 8644:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08646: reducing learning rate of group 0 to 1.1098e-20.\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8645:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8646:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8647:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8648:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8649:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8650:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8651:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8652:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1097518119957822e-20\n",
      "Epoch 8653:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08655: reducing learning rate of group 0 to 1.0543e-20.\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8654:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8655:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8656:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8657:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8658:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8659:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8660:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8661:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0542642213959932e-20\n",
      "Epoch 8662:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08664: reducing learning rate of group 0 to 1.0016e-20.\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8663:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8664:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8665:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8666:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8667:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8668:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8669:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8670:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0015510103261934e-20\n",
      "Epoch 8671:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08673: reducing learning rate of group 0 to 9.5147e-21.\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8672:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8673:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8674:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8675:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8676:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8677:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8678:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8679:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.514734598098837e-21\n",
      "Epoch 8680:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08682: reducing learning rate of group 0 to 9.0390e-21.\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8681:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8682:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8683:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8684:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8685:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8686:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8687:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8688:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.038997868193895e-21\n",
      "Epoch 8689:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08691: reducing learning rate of group 0 to 8.5870e-21.\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8690:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8691:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8692:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8693:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8694:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8695:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8696:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8697:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.5870479747842e-21\n",
      "Epoch 8698:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08700: reducing learning rate of group 0 to 8.1577e-21.\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8699:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8700:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8701:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8702:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8703:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8704:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8705:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8706:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.15769557604499e-21\n",
      "Epoch 8707:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08709: reducing learning rate of group 0 to 7.7498e-21.\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8708:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8709:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8710:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8711:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8712:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8713:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8714:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8715:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.74981079724274e-21\n",
      "Epoch 8716:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08718: reducing learning rate of group 0 to 7.3623e-21.\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8717:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8718:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8719:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8720:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8721:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8722:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8723:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8724:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.362320257380602e-21\n",
      "Epoch 8725:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08727: reducing learning rate of group 0 to 6.9942e-21.\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8726:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8727:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8728:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8729:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8730:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8731:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8732:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8733:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.994204244511572e-21\n",
      "Epoch 8734:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08736: reducing learning rate of group 0 to 6.6445e-21.\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8735:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8736:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8737:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8738:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8739:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8740:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8741:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8742:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.6444940322859926e-21\n",
      "Epoch 8743:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08745: reducing learning rate of group 0 to 6.3123e-21.\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8744:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8745:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8746:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8747:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8748:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8749:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8750:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8751:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.312269330671692e-21\n",
      "Epoch 8752:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08754: reducing learning rate of group 0 to 5.9967e-21.\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8753:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8754:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8755:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8756:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8757:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8758:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8759:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8760:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.996655864138107e-21\n",
      "Epoch 8761:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08763: reducing learning rate of group 0 to 5.6968e-21.\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8762:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8763:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8764:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8765:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8766:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8767:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8768:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8769:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.696823070931202e-21\n",
      "Epoch 8770:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08772: reducing learning rate of group 0 to 5.4120e-21.\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8771:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8772:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8773:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8774:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8775:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8776:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8777:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8778:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.411981917384641e-21\n",
      "Epoch 8779:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08781: reducing learning rate of group 0 to 5.1414e-21.\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8780:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8781:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8782:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8783:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8784:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8785:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8786:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8787:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.141382821515409e-21\n",
      "Epoch 8788:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08790: reducing learning rate of group 0 to 4.8843e-21.\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8789:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8790:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8791:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8792:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8793:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8794:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8795:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8796:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.884313680439639e-21\n",
      "Epoch 8797:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08799: reducing learning rate of group 0 to 4.6401e-21.\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8798:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8799:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8800:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8801:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8802:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8803:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8804:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8805:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.640097996417656e-21\n",
      "Epoch 8806:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08808: reducing learning rate of group 0 to 4.4081e-21.\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8807:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8808:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8809:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8810:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8811:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8812:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8813:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8814:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.408093096596773e-21\n",
      "Epoch 8815:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08817: reducing learning rate of group 0 to 4.1877e-21.\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8816:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8817:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8818:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8819:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8820:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8821:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8822:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8823:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.187688441766934e-21\n",
      "Epoch 8824:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08826: reducing learning rate of group 0 to 3.9783e-21.\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8825:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8826:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8827:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8828:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8829:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8830:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8831:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8832:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.978304019678587e-21\n",
      "Epoch 8833:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08835: reducing learning rate of group 0 to 3.7794e-21.\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8834:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8835:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8836:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8837:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8838:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8839:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8840:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8841:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.779388818694658e-21\n",
      "Epoch 8842:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08844: reducing learning rate of group 0 to 3.5904e-21.\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8843:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8844:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8845:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8846:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8847:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8848:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8849:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8850:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.590419377759925e-21\n",
      "Epoch 8851:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08853: reducing learning rate of group 0 to 3.4109e-21.\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8852:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8853:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8854:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8855:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8856:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8857:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8858:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8859:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.4108984088719285e-21\n",
      "Epoch 8860:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08862: reducing learning rate of group 0 to 3.2404e-21.\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8861:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8862:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8863:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8864:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8865:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8866:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8867:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8868:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.2403534884283317e-21\n",
      "Epoch 8869:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08871: reducing learning rate of group 0 to 3.0783e-21.\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8870:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8871:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8872:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8873:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8874:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8875:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8876:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8877:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.078335814006915e-21\n",
      "Epoch 8878:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08880: reducing learning rate of group 0 to 2.9244e-21.\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8879:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8880:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8881:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8882:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8883:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8884:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8885:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8886:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.924419023306569e-21\n",
      "Epoch 8887:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08889: reducing learning rate of group 0 to 2.7782e-21.\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8888:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8889:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8890:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8891:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8892:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8893:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8894:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8895:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7781980721412403e-21\n",
      "Epoch 8896:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08898: reducing learning rate of group 0 to 2.6393e-21.\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8897:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8898:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8899:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8900:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8901:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8902:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8903:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8904:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.6392881685341782e-21\n",
      "Epoch 8905:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08907: reducing learning rate of group 0 to 2.5073e-21.\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8906:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8907:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8908:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8909:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8910:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8911:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8912:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8913:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.5073237601074692e-21\n",
      "Epoch 8914:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08916: reducing learning rate of group 0 to 2.3820e-21.\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8915:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8916:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8917:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8918:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8919:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8920:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8921:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8922:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3819575721020958e-21\n",
      "Epoch 8923:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08925: reducing learning rate of group 0 to 2.2629e-21.\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8924:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8925:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8926:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8927:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8928:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8929:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8930:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8931:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2628596934969907e-21\n",
      "Epoch 8932:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08934: reducing learning rate of group 0 to 2.1497e-21.\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8933:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8934:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8935:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8936:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8937:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8938:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8939:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8940:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.149716708822141e-21\n",
      "Epoch 8941:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08943: reducing learning rate of group 0 to 2.0422e-21.\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8942:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8943:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8944:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8945:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8946:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8947:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8948:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8949:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.042230873381034e-21\n",
      "Epoch 8950:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08952: reducing learning rate of group 0 to 1.9401e-21.\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8951:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8952:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8953:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8954:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8955:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8956:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8957:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8958:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9401193297119822e-21\n",
      "Epoch 8959:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08961: reducing learning rate of group 0 to 1.8431e-21.\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8960:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8961:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8962:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8963:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8964:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8965:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8966:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8967:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8431133632263832e-21\n",
      "Epoch 8968:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08970: reducing learning rate of group 0 to 1.7510e-21.\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8969:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8970:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8971:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8972:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8973:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8974:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8975:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8976:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.750957695065064e-21\n",
      "Epoch 8977:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08979: reducing learning rate of group 0 to 1.6634e-21.\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8978:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8979:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8980:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8981:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8982:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8983:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8984:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8985:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6634098103118107e-21\n",
      "Epoch 8986:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08988: reducing learning rate of group 0 to 1.5802e-21.\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8987:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8988:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8989:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8990:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8991:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8992:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8993:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8994:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5802393197962202e-21\n",
      "Epoch 8995:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 08997: reducing learning rate of group 0 to 1.5012e-21.\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 8996:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 8997:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 8998:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 8999:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9000:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9001:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9002:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9003:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5012273538064091e-21\n",
      "Epoch 9004:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09006: reducing learning rate of group 0 to 1.4262e-21.\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9005:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9006:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9007:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9008:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9009:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9010:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9011:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9012:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4261659861160885e-21\n",
      "Epoch 9013:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09015: reducing learning rate of group 0 to 1.3549e-21.\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9014:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9015:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9016:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9017:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9018:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9019:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9020:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9021:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.354857686810284e-21\n",
      "Epoch 9022:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09024: reducing learning rate of group 0 to 1.2871e-21.\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9023:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9024:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9025:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9026:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9027:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9028:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9029:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9030:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2871148024697696e-21\n",
      "Epoch 9031:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09033: reducing learning rate of group 0 to 1.2228e-21.\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9032:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9033:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9034:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9035:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9036:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9037:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9038:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9039:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2227590623462811e-21\n",
      "Epoch 9040:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09042: reducing learning rate of group 0 to 1.1616e-21.\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9041:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9042:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9043:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9044:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9045:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9046:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9047:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9048:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.161621109228967e-21\n",
      "Epoch 9049:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09051: reducing learning rate of group 0 to 1.1035e-21.\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9050:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9051:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9052:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9053:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9054:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9055:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9056:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9057:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1035400537675188e-21\n",
      "Epoch 9058:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09060: reducing learning rate of group 0 to 1.0484e-21.\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9059:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9060:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9061:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9062:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9063:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9064:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9065:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9066:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0483630510791427e-21\n",
      "Epoch 9067:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09069: reducing learning rate of group 0 to 9.9594e-22.\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9068:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9069:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9070:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9071:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9072:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9073:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9074:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9075:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.959448985251855e-22\n",
      "Epoch 9076:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09078: reducing learning rate of group 0 to 9.4615e-22.\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9077:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9078:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9079:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9080:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9081:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9082:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9083:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9084:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.461476535989262e-22\n",
      "Epoch 9085:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09087: reducing learning rate of group 0 to 8.9884e-22.\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9086:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9087:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9088:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9089:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9090:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9091:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9092:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9093:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.988402709189797e-22\n",
      "Epoch 9094:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09096: reducing learning rate of group 0 to 8.5390e-22.\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9095:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9096:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9097:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9098:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9099:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9100:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9101:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9102:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.538982573730308e-22\n",
      "Epoch 9103:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09105: reducing learning rate of group 0 to 8.1120e-22.\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9104:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9105:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9106:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9107:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9108:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9109:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9110:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9111:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.112033445043792e-22\n",
      "Epoch 9112:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09114: reducing learning rate of group 0 to 7.7064e-22.\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9113:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9114:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9115:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9116:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9117:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9118:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9119:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9120:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.706431772791602e-22\n",
      "Epoch 9121:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09123: reducing learning rate of group 0 to 7.3211e-22.\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9122:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9123:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9124:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9125:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9126:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9127:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9128:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9129:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.321110184152022e-22\n",
      "Epoch 9130:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09132: reducing learning rate of group 0 to 6.9551e-22.\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9131:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9132:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9133:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9134:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9135:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9136:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9137:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9138:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.955054674944421e-22\n",
      "Epoch 9139:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09141: reducing learning rate of group 0 to 6.6073e-22.\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9140:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9141:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9142:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9143:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9144:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9145:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9146:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9147:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.607301941197199e-22\n",
      "Epoch 9148:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09150: reducing learning rate of group 0 to 6.2769e-22.\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9149:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9150:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9151:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9152:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9153:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9154:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9155:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9156:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.276936844137339e-22\n",
      "Epoch 9157:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09159: reducing learning rate of group 0 to 5.9631e-22.\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9158:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9159:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9160:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9161:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9162:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9163:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9164:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9165:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.963090001930472e-22\n",
      "Epoch 9166:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09168: reducing learning rate of group 0 to 5.6649e-22.\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9167:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9168:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9169:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9170:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9171:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9172:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9173:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9174:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.664935501833948e-22\n",
      "Epoch 9175:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09177: reducing learning rate of group 0 to 5.3817e-22.\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9176:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9177:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9178:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9179:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9180:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9181:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9182:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9183:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.381688726742251e-22\n",
      "Epoch 9184:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09186: reducing learning rate of group 0 to 5.1126e-22.\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9185:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9186:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9187:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9188:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9189:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9190:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9191:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9192:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.112604290405138e-22\n",
      "Epoch 9193:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09195: reducing learning rate of group 0 to 4.8570e-22.\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9194:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9195:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9196:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9197:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9198:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9199:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9200:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9201:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.856974075884881e-22\n",
      "Epoch 9202:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09204: reducing learning rate of group 0 to 4.6141e-22.\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9203:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9204:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9205:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9206:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9207:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9208:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9209:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9210:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.614125372090637e-22\n",
      "Epoch 9211:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09213: reducing learning rate of group 0 to 4.3834e-22.\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9212:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9213:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9214:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9215:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9216:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9217:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9218:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9219:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.383419103486105e-22\n",
      "Epoch 9220:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09222: reducing learning rate of group 0 to 4.1642e-22.\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9221:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9222:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9223:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9224:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9225:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9226:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9227:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9228:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.164248148311799e-22\n",
      "Epoch 9229:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09231: reducing learning rate of group 0 to 3.9560e-22.\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9230:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9231:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9232:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9233:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9234:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9235:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9236:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9237:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.956035740896209e-22\n",
      "Epoch 9238:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09240: reducing learning rate of group 0 to 3.7582e-22.\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9239:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9240:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9241:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9242:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9243:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9244:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9245:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9246:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7582339538513985e-22\n",
      "Epoch 9247:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09249: reducing learning rate of group 0 to 3.5703e-22.\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9248:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9249:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9250:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9251:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9252:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9253:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9254:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9255:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.570322256158828e-22\n",
      "Epoch 9256:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09258: reducing learning rate of group 0 to 3.3918e-22.\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9257:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9258:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9259:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9260:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9261:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9262:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9263:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9264:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3918061433508864e-22\n",
      "Epoch 9265:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09267: reducing learning rate of group 0 to 3.2222e-22.\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9266:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9267:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9268:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9269:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9270:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9271:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9272:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9273:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.222215836183342e-22\n",
      "Epoch 9274:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09276: reducing learning rate of group 0 to 3.0611e-22.\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9275:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9276:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9277:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9278:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9279:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9280:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9281:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9282:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.061105044374175e-22\n",
      "Epoch 9283:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09285: reducing learning rate of group 0 to 2.9080e-22.\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9284:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9285:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9286:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9287:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9288:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9289:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9290:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9291:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.908049792155466e-22\n",
      "Epoch 9292:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09294: reducing learning rate of group 0 to 2.7626e-22.\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9293:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9294:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9295:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9296:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9297:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9298:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9299:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9300:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.7626473025476928e-22\n",
      "Epoch 9301:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09303: reducing learning rate of group 0 to 2.6245e-22.\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9302:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9303:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9304:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9305:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9306:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9307:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9308:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9309:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.624514937420308e-22\n",
      "Epoch 9310:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09312: reducing learning rate of group 0 to 2.4933e-22.\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9311:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9312:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9313:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9314:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9315:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9316:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9317:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9318:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.4932891905492925e-22\n",
      "Epoch 9319:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09321: reducing learning rate of group 0 to 2.3686e-22.\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9320:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9321:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9322:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9323:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9324:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9325:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9326:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9327:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3686247310218276e-22\n",
      "Epoch 9328:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09330: reducing learning rate of group 0 to 2.2502e-22.\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9329:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9330:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9331:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9332:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9333:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9334:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9335:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9336:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.250193494470736e-22\n",
      "Epoch 9337:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09339: reducing learning rate of group 0 to 2.1377e-22.\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9338:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9339:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9340:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9341:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9342:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9343:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9344:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9345:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.137683819747199e-22\n",
      "Epoch 9346:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09348: reducing learning rate of group 0 to 2.0308e-22.\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9347:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9348:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9349:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9350:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9351:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9352:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9353:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9354:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.030799628759839e-22\n",
      "Epoch 9355:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09357: reducing learning rate of group 0 to 1.9293e-22.\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9356:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9357:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9358:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9359:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9360:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9361:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9362:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9363:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.929259647321847e-22\n",
      "Epoch 9364:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09366: reducing learning rate of group 0 to 1.8328e-22.\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9365:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9366:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9367:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9368:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9369:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9370:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9371:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9372:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8327966649557544e-22\n",
      "Epoch 9373:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09375: reducing learning rate of group 0 to 1.7412e-22.\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9374:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9375:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9376:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9377:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9378:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9379:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9380:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9381:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7411568317079665e-22\n",
      "Epoch 9382:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09384: reducing learning rate of group 0 to 1.6541e-22.\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9383:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9384:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9385:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9386:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9387:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9388:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9389:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9390:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6540989901225682e-22\n",
      "Epoch 9391:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09393: reducing learning rate of group 0 to 1.5714e-22.\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9392:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9393:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9394:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9395:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9396:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9397:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9398:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9399:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5713940406164398e-22\n",
      "Epoch 9400:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09402: reducing learning rate of group 0 to 1.4928e-22.\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9401:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9402:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9403:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9404:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9405:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9406:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9407:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9408:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4928243385856178e-22\n",
      "Epoch 9409:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09411: reducing learning rate of group 0 to 1.4182e-22.\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9410:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9411:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9412:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9413:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9414:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9415:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9416:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9417:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4181831216563368e-22\n",
      "Epoch 9418:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09420: reducing learning rate of group 0 to 1.3473e-22.\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9419:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9420:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9421:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9422:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9423:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9424:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9425:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9426:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.34727396557352e-22\n",
      "Epoch 9427:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09429: reducing learning rate of group 0 to 1.2799e-22.\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9428:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9429:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9430:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9431:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9432:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9433:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9434:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9435:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2799102672948439e-22\n",
      "Epoch 9436:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09438: reducing learning rate of group 0 to 1.2159e-22.\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9437:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9438:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9439:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9440:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9441:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9442:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9443:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9444:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2159147539301016e-22\n",
      "Epoch 9445:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09447: reducing learning rate of group 0 to 1.1551e-22.\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9446:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9447:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9448:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9449:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9450:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9451:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9452:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9453:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1551190162335965e-22\n",
      "Epoch 9454:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09456: reducing learning rate of group 0 to 1.0974e-22.\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9455:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9456:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9457:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9458:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9459:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9460:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9461:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9462:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0973630654219165e-22\n",
      "Epoch 9463:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09465: reducing learning rate of group 0 to 1.0425e-22.\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9464:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9465:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9466:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9467:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9468:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9469:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9470:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9471:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0424949121508207e-22\n",
      "Epoch 9472:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09474: reducing learning rate of group 0 to 9.9037e-23.\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9473:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9474:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9475:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9476:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9477:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9478:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9479:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9480:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.903701665432796e-23\n",
      "Epoch 9481:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09483: reducing learning rate of group 0 to 9.4085e-23.\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9482:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9483:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9484:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9485:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9486:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9487:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9488:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9489:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.408516582161156e-23\n",
      "Epoch 9490:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09492: reducing learning rate of group 0 to 8.9381e-23.\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9491:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9492:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9493:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9494:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9495:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9496:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9497:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9498:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.938090753053098e-23\n",
      "Epoch 9499:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09501: reducing learning rate of group 0 to 8.4912e-23.\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9500:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9501:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9502:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9503:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9504:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9505:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9506:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9507:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.491186215400443e-23\n",
      "Epoch 9508:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09510: reducing learning rate of group 0 to 8.0666e-23.\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9509:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9510:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9511:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9512:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9513:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9514:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9515:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9516:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.06662690463042e-23\n",
      "Epoch 9517:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09519: reducing learning rate of group 0 to 7.6633e-23.\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9518:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9519:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9520:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9521:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9522:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9523:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9524:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9525:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.663295559398899e-23\n",
      "Epoch 9526:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09528: reducing learning rate of group 0 to 7.2801e-23.\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9527:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9528:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9529:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9530:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9531:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9532:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9533:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9534:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.280130781428953e-23\n",
      "Epoch 9535:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09537: reducing learning rate of group 0 to 6.9161e-23.\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9536:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9537:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9538:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9539:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9540:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9541:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9542:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9543:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.916124242357506e-23\n",
      "Epoch 9544:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09546: reducing learning rate of group 0 to 6.5703e-23.\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9545:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9546:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9547:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9548:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9549:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9550:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9551:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9552:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.57031803023963e-23\n",
      "Epoch 9553:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09555: reducing learning rate of group 0 to 6.2418e-23.\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9554:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9555:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9556:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9557:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9558:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9559:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9560:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9561:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.241802128727648e-23\n",
      "Epoch 9562:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09564: reducing learning rate of group 0 to 5.9297e-23.\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9563:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9564:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9565:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9566:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9567:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9568:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9569:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9570:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.929712022291265e-23\n",
      "Epoch 9571:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09573: reducing learning rate of group 0 to 5.6332e-23.\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9572:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9573:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9574:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9575:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9576:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9577:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9578:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9579:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.633226421176702e-23\n",
      "Epoch 9580:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09582: reducing learning rate of group 0 to 5.3516e-23.\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9581:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9582:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9583:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9584:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9585:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9586:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9587:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9588:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.351565100117867e-23\n",
      "Epoch 9589:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09591: reducing learning rate of group 0 to 5.0840e-23.\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9590:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9591:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9592:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9593:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9594:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9595:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9596:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9597:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.083986845111973e-23\n",
      "Epoch 9598:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09600: reducing learning rate of group 0 to 4.8298e-23.\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9599:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9600:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9601:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9602:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9603:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9604:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9605:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9606:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.829787502856374e-23\n",
      "Epoch 9607:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09609: reducing learning rate of group 0 to 4.5883e-23.\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9608:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9609:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9610:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9611:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9612:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9613:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9614:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9615:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.5882981277135553e-23\n",
      "Epoch 9616:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09618: reducing learning rate of group 0 to 4.3589e-23.\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9617:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9618:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9619:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9620:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9621:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9622:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9623:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9624:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.3588832213278774e-23\n",
      "Epoch 9625:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09627: reducing learning rate of group 0 to 4.1409e-23.\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9626:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9627:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9628:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9629:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9630:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9631:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9632:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9633:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 4.1409390602614834e-23\n",
      "Epoch 9634:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09636: reducing learning rate of group 0 to 3.9339e-23.\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9635:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9636:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9637:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9638:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9639:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9640:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9641:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9642:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.933892107248409e-23\n",
      "Epoch 9643:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09645: reducing learning rate of group 0 to 3.7372e-23.\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9644:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9645:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9646:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9647:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9648:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9649:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9650:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9651:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.7371975018859886e-23\n",
      "Epoch 9652:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09654: reducing learning rate of group 0 to 3.5503e-23.\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9653:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9654:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9655:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9656:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9657:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9658:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9659:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9660:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.550337626791689e-23\n",
      "Epoch 9661:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09663: reducing learning rate of group 0 to 3.3728e-23.\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9662:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9663:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9664:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9665:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9666:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9667:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9668:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9669:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.3728207454521045e-23\n",
      "Epoch 9670:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09672: reducing learning rate of group 0 to 3.2042e-23.\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9671:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9672:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9673:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9674:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9675:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9676:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9677:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9678:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.204179708179499e-23\n",
      "Epoch 9679:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09681: reducing learning rate of group 0 to 3.0440e-23.\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9680:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9681:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9682:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9683:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9684:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9685:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9686:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9687:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 3.0439707227705237e-23\n",
      "Epoch 9688:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09690: reducing learning rate of group 0 to 2.8918e-23.\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9689:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9690:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9691:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9692:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9693:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9694:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9695:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9696:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.891772186631997e-23\n",
      "Epoch 9697:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09699: reducing learning rate of group 0 to 2.7472e-23.\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9698:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9699:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9700:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9701:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9702:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9703:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9704:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9705:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.747183577300397e-23\n",
      "Epoch 9706:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09708: reducing learning rate of group 0 to 2.6098e-23.\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9707:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9708:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9709:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9710:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9711:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9712:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9713:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9714:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.609824398435377e-23\n",
      "Epoch 9715:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09717: reducing learning rate of group 0 to 2.4793e-23.\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9716:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9717:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9718:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9719:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9720:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9721:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9722:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9723:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.479333178513608e-23\n",
      "Epoch 9724:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09726: reducing learning rate of group 0 to 2.3554e-23.\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9725:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9726:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9727:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9728:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9729:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9730:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9731:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9732:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.3553665195879277e-23\n",
      "Epoch 9733:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09735: reducing learning rate of group 0 to 2.2376e-23.\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9734:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9735:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9736:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9737:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9738:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9739:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9740:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9741:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.2375981936085312e-23\n",
      "Epoch 9742:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09744: reducing learning rate of group 0 to 2.1257e-23.\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9743:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9744:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9745:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9746:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9747:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9748:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9749:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9750:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.1257182839281045e-23\n",
      "Epoch 9751:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09753: reducing learning rate of group 0 to 2.0194e-23.\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9752:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9753:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9754:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9755:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9756:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9757:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9758:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9759:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 2.019432369731699e-23\n",
      "Epoch 9760:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09762: reducing learning rate of group 0 to 1.9185e-23.\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9761:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9762:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9763:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9764:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9765:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9766:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9767:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9768:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.9184607512451142e-23\n",
      "Epoch 9769:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09771: reducing learning rate of group 0 to 1.8225e-23.\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9770:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9771:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9772:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9773:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9774:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9775:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9776:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9777:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.8225377136828584e-23\n",
      "Epoch 9778:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09780: reducing learning rate of group 0 to 1.7314e-23.\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9779:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9780:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9781:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9782:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9783:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9784:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9785:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9786:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.7314108279987153e-23\n",
      "Epoch 9787:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09789: reducing learning rate of group 0 to 1.6448e-23.\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9788:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9789:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9790:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9791:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9792:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9793:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9794:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9795:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.6448402865987793e-23\n",
      "Epoch 9796:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09798: reducing learning rate of group 0 to 1.5626e-23.\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9797:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9798:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9799:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9800:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9801:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9802:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9803:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9804:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.5625982722688402e-23\n",
      "Epoch 9805:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09807: reducing learning rate of group 0 to 1.4845e-23.\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9806:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9807:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9808:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9809:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9810:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9811:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9812:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9813:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4844683586553982e-23\n",
      "Epoch 9814:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09816: reducing learning rate of group 0 to 1.4102e-23.\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9815:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9816:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9817:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9818:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9819:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9820:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9821:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9822:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.4102449407226283e-23\n",
      "Epoch 9823:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09825: reducing learning rate of group 0 to 1.3397e-23.\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9824:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9825:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9826:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9827:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9828:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9829:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9830:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9831:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.339732693686497e-23\n",
      "Epoch 9832:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09834: reducing learning rate of group 0 to 1.2727e-23.\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9833:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9834:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9835:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9836:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9837:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9838:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9839:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9840:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.272746059002172e-23\n",
      "Epoch 9841:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09843: reducing learning rate of group 0 to 1.2091e-23.\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9842:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9843:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9844:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9845:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9846:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9847:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9848:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9849:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.2091087560520634e-23\n",
      "Epoch 9850:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09852: reducing learning rate of group 0 to 1.1487e-23.\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9851:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9852:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9853:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9854:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9855:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9856:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9857:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9858:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.1486533182494601e-23\n",
      "Epoch 9859:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09861: reducing learning rate of group 0 to 1.0912e-23.\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9860:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9861:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9862:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9863:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9864:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9865:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9866:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9867:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.091220652336987e-23\n",
      "Epoch 9868:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09870: reducing learning rate of group 0 to 1.0367e-23.\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9869:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9870:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9871:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9872:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9873:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9874:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9875:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9876:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 1.0366596197201376e-23\n",
      "Epoch 9877:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09879: reducing learning rate of group 0 to 9.8483e-24.\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9878:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9879:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9880:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9881:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9882:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9883:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9884:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9885:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.848266387341306e-24\n",
      "Epoch 9886:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09888: reducing learning rate of group 0 to 9.3559e-24.\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9887:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9888:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9889:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9890:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9891:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9892:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9893:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9894:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 9.355853067974241e-24\n",
      "Epoch 9895:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09897: reducing learning rate of group 0 to 8.8881e-24.\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9896:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9897:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9898:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9899:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9900:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9901:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9902:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9903:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.888060414575529e-24\n",
      "Epoch 9904:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09906: reducing learning rate of group 0 to 8.4437e-24.\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9905:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9906:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9907:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9908:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9909:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9910:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9911:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9912:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.443657393846752e-24\n",
      "Epoch 9913:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09915: reducing learning rate of group 0 to 8.0215e-24.\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9914:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9915:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9916:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9917:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9918:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9919:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9920:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9921:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 8.021474524154414e-24\n",
      "Epoch 9922:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09924: reducing learning rate of group 0 to 7.6204e-24.\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9923:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9924:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9925:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9926:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9927:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9928:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9929:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9930:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.620400797946693e-24\n",
      "Epoch 9931:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09933: reducing learning rate of group 0 to 7.2394e-24.\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9932:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9933:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9934:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9935:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9936:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9937:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9938:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9939:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 7.239380758049359e-24\n",
      "Epoch 9940:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09942: reducing learning rate of group 0 to 6.8774e-24.\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9941:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9942:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9943:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9944:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9945:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9946:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9947:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9948:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.87741172014689e-24\n",
      "Epoch 9949:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09951: reducing learning rate of group 0 to 6.5335e-24.\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9950:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9951:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9952:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9953:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9954:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9955:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9956:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9957:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.533541134139545e-24\n",
      "Epoch 9958:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09960: reducing learning rate of group 0 to 6.2069e-24.\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9959:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9960:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9961:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9962:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9963:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9964:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9965:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9966:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 6.206864077432567e-24\n",
      "Epoch 9967:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09969: reducing learning rate of group 0 to 5.8965e-24.\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9968:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9969:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9970:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9971:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9972:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9973:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9974:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9975:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.896520873560938e-24\n",
      "Epoch 9976:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09978: reducing learning rate of group 0 to 5.6017e-24.\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9977:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9978:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9979:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9980:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9981:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9982:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9983:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9984:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.6016948298828906e-24\n",
      "Epoch 9985:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09987: reducing learning rate of group 0 to 5.3216e-24.\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9986:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9987:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9988:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9989:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9990:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9991:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9992:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9993:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.321610088388746e-24\n",
      "Epoch 9994:\n",
      "train loss: 1.8398842248297887e-16\n",
      "Epoch 09996: reducing learning rate of group 0 to 5.0555e-24.\n",
      "lr: 5.055529583969308e-24\n",
      "Epoch 9995:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.055529583969308e-24\n",
      "Epoch 9996:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.055529583969308e-24\n",
      "Epoch 9997:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.055529583969308e-24\n",
      "Epoch 9998:\n",
      "train loss: 1.8398842248297887e-16\n",
      "lr: 5.055529583969308e-24\n",
      "Epoch 9999:\n",
      "train loss: 1.8398842248297887e-16\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epochs=10000, lr=1e-2, nsub=3, length=5):\n",
    "        self.log_dir = './runs/'+datetime.now().strftime('%b.%d %H-%M-%S')\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.data = TrainData(r'C:\\workspace\\1_Michael\\Research\\NN_solveQuantumProblem\\DataSet\\rankone_R3_64_1.pt').getData()\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.nsub = nsub\n",
    "        self.length = length\n",
    "        self.model = Net(nsub,length).to(self.device)\n",
    "        self.criterion = CustomLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.95, patience=8,eps = 10e-32, verbose=True)\n",
    "        self.max_epoch = epochs\n",
    "\n",
    "    def run(self):\n",
    "        training_result_dir = self.log_dir + '/training_result'\n",
    "        os.makedirs(training_result_dir)\n",
    "        metrics = {'train_loss': []}\n",
    "        for self.epoch in range(self.max_epoch): # epochs\n",
    "            train_loss = self.train() # train 1 epoch\n",
    "            print('lr:',self.get_lr(self.optimizer))\n",
    "            print(f'Epoch {self.epoch:03d}:')\n",
    "            print('train loss:', train_loss)\n",
    "            metrics['train_loss'].append(train_loss)\n",
    "\n",
    "            if torch.tensor(metrics['train_loss']).argmin() == self.epoch:\n",
    "                torch.save(self.model.state_dict(), str(training_result_dir + '/model.pth'))\n",
    "        # fig, ax = plt.subplots(1, 1, figsize=(10, 10), dpi=100)\n",
    "        # ax.set_title('Loss')\n",
    "        # ax.plot(range(self.epoch + 1), metrics['train_loss'], label='Train')\n",
    "        # ax.plot(range(self.epoch + 1), metrics['valid_loss'], label='Valid')\n",
    "        # ax.legend()\n",
    "        # plt.show()\n",
    "        # fig.savefig(str(training_result_dir / 'metrics.jpg')) \n",
    "        # plt.close()\n",
    "    def process(self,output):\n",
    "        output = output.view(self.nsub, self.length)\n",
    "        for i in range(output.shape[0]):\n",
    "            a = torch.clone(output[i]).view(1,self.length)\n",
    "            A = torch.matmul(a.t(),a)\n",
    "            if i == 0:\n",
    "                kron = A\n",
    "            else:\n",
    "                kron = torch.kron(kron,A)\n",
    "\n",
    "        return kron\n",
    "    def train(self):\n",
    "        loss_steps = []\n",
    "        rho,ans,output_a = self.data\n",
    "        rho = rho.to(self.device)\n",
    "        ans = ans.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(rho)\n",
    "        out = self.process(out)\n",
    "        loss = self.criterion(out, ans)\n",
    "        loss.backward()\n",
    "        # print(loss.item())\n",
    "        self.optimizer.step()\n",
    "        loss_steps.append(loss.detach().item())\n",
    "\n",
    "        avg_loss = sum(loss_steps) / len(loss_steps)\n",
    "        self.scheduler.step(avg_loss) #加入scheduler\n",
    "        return avg_loss\n",
    "\n",
    "    def get_lr(self,optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "print(Trainer().run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
